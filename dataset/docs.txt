This November is Wikipedia Asian MonthJoin WAM contests and win postcards from Asia.
Computer science is the study of algorithmic processes, computational machines and computation itself.[1] As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.[2][3]
Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications. Algorithms and data structures have been called the heart of computer science.[4] Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems. Computer architecture describes construction of computer components and computer-operated equipment. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. A digital computer is capable of simulating various information processes.[5] The fundamental concern of computer science is determining what can and cannot be automated.[6] Computer scientists usually focus on academic research. The Turing Award is generally recognized as the highest distinction in computer sciences.
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. 
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[9] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[10] Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[11] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[12] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[12] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[13] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[14] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[15] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[16]

During the 1940s, with the development of new and more powerful computing machines such as the AtanasoffBerry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[17] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[18] Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[19] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[20][21] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[22] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.Although first proposed in 1956,[23] the term "computer science" appears in a 1959 article in Communications of the ACM,[24]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[25] justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[24]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[26] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[27] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[28] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACMturingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.[29] Three months later in the same journal, comptologist was suggested, followed next year by hypologist.[30] The term computics has also been suggested.[31] In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informtica (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh).[32] "In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain."[33]
A folkloric quotation, often attributed tobut almost certainly not first formulated byEdsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[20] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gdel, Alan Turing, John von Neumann, Rzsa Pter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[23]
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[34] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[35]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.

Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering.[36] Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[36] It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[36] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[36] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[36]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[36] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[36]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[37] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[38] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[39]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[40]
Computer science is no more about computers than astronomy is about telescopes.As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[41][42]
CSAB, formerly called Computing Sciences Accreditation Boardwhich is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[43]identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, humancomputer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[41]
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[20] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[44] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[45]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[46]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[47] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[48] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term architecture in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[49] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[50] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[51]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[52] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[citation needed]
Social computing is an area that is concerned with the intersection of social behavior and computational systems. Humancomputer interaction research develops theories, principles, and guidelines for user interface designers.
Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of softwareit doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[53]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[59]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[60][61] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[62]
Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students.[63] In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (1116-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.[64]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[65] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[66]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[67][68] and several others are following.[69]

Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content.  Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.
Computer graphics studies the 
anesthetic manipulation of visual and geometric information using computational techniques.  It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues.  Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.
Connected studies include:
Applications of computer graphics include:
There are several international conferences and journals where the most significant results in computer graphics are published. Among them are the SIGGRAPH and Eurographics conferences and the Association for Computing Machinery (ACM) Transactions on Graphics journal. The joint Eurographics and ACM SIGGRAPH symposium series features the major venues for the more specialized sub-fields: Symposium on Geometry Processing,[1] Symposium on Rendering, Symposium on Computer Animation,[2] and High Performance Graphics.[3]
As in the rest of computer science, conference publications in computer graphics are generally more significant than journal publications (and subsequently have lower acceptance rates).[4][5][6][7]
A broad classification of major subfields in computer graphics might be:
The subfield of geometry studies the representation of three-dimensional objects in a discrete digital setting.  Because the appearance of an object depends largely on its exterior, boundary representations are most commonly used.  Two dimensional surfaces are a good representation for most objects, though they may be non-manifold.  Since surfaces are not finite, discrete digital approximations are used. Polygonal meshes (and to a lesser extent subdivision surfaces) are by far the most common representation, although point-based representations have become more popular recently (see for instance the Symposium on Point-Based Graphics).[8] These representations are Lagrangian, meaning the spatial locations of the samples are independent.  Recently, Eulerian surface descriptions (i.e., where spatial samples are fixed) such as level sets have been developed into a useful representation for deforming surfaces which undergo many topological changes (with fluids being the most notable example).[9]
Geometry subfields include:
The subfield of animation studies descriptions for surfaces (and other phenomena) that move or deform over time.  Historically, most work in this field has focused on parametric and data-driven models, but recently physical simulation has become more popular as computers have become more powerful computationally.
Animation subfields include:
Rendering generates images from a model.  Rendering may simulate light transport to create realistic images or it may create images that have a particular artistic style in non-photorealistic rendering.  The two basic operations in realistic rendering are transport (how much light passes from one place to another) and scattering (how surfaces interact with light).  See Rendering (computer graphics) for more information.
Rendering subfields include:
Bitmap Design / Image Editing
Vector drawing
Architecture
Video editing
Sculpting, Animation, and 3D Modeling
Digital composition
Rendering
Other applications examples
Industrial labs doing "blue sky" graphics research include:
Major film studios notable for graphics research include:

In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages.[1] 
It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.
The field of formal semantics encompasses all of the following:
It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.
There are many approaches to formal semantics; these belong to three major classes:
Apart from the choice between denotational, operational, or axiomatic approaches, most variations in formal semantic systems arise from the choice of supporting mathematical formalism.
Some variations of formal semantics include the following:
For a variety of reasons, one might wish to describe the relationships between different formal semantics.  For example:
It is also possible to relate multiple semantics through abstractions via the theory of abstract interpretation.
Robert W. Floyd is credited with founding the field of programming language semantics in Floyd (1967).[3]

This November is Wikipedia Asian MonthJoin WAM contests and win postcards from Asia.The history of computer science began long before our modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science.[1] This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.[2]
The earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer.[3] The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.[4]:11 Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.[5]
In the 5th century BC in ancient India, the grammarian Pini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.[6]
The Antikythera mechanism is believed to be an early mechanical analog computer.[7]  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.[7]
Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Ab Rayhn al-Brn,[8] and the torquetum by Jabir ibn Aflah.[9] According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus.[10][11] Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Ban Ms brothers,[12] and Al-Jazari's programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer.[13] Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.[14]
When John Napier discovered logarithms for computational purposes in the early 17th century,[15] there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624.[16] Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria.[17] Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.[18]
In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.[19]
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.[20]
By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.[20]
Charles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control.  This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the Analytical Engine, which was the first true representation of what is the modern computer.[21]
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his Analytical Engine, the first mechanical computer.[22] During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers.[23] Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not.[24] While she was never able to see the results of her work, as the Analytical Engine was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.[25]
In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.[26] During 188081 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933.[27] The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow.[28] Consequently, these gates are sometimes called universal logic gates.[29]
Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with switching circuit theory in the 1930s. From 1934 to 1936, Akira Nakashima, Claude Shannon, and Viktor Shetakov published a series of papers showing that the two-valued Boolean algebra, can describe the operation of switching circuits.[30][31][32][33] This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.[33]
While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.[34]
Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women.[35][36][37][38] Some performed astronomical calculations for calendars, others ballistic tables for the military.[39]
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described "purely mechanical." The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.
The mathematical foundations of modern computer science began to be laid by Kurt Gdel with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by Gdel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.[40]
In 1936  Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a "purely mechanical" model for computing.[41] This became the ChurchTuring thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis states that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.[41]
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use.[42] These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.[43]
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:[42]
 I know that in or about 1943 or 44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936 Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the "father of the computer" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...The world's first electronic digital computer, the AtanasoffBerry computer, was built on the Iowa State campus from 1939 through 1942 by John V. Atanasoff, a professor of physics and mathematics, and Clifford Berry, an engineering graduate student.
In 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle.[44][45] Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer.  In 1946, he designed the first high-level programming language, Plankalkl.[46]
In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers.[42] The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.[42]
In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy. With an operating speed of 1MHz, the Pilot Model ACE was for some time the fastest computer in the world.[42][47] Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day.[42] Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.[42]
The first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II.[48]
While the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the "bug" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident'  along with the insect and the notation "First actual case of bug being found" (see software bug for details).[48]
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.[49]
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for "steersman." He published "Cybernetics" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.[50]
In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.[51]  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.[52]
Von Neumann's machine design uses a RISC (Reduced instruction set computing) architecture,[dubious   discuss] which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.)  With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations)[53] are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the "IR" (instruction register), "IBR" (instruction buffer register), "MQ" (multiplier quotient register), "MAR" (memory address register), and "MDR" (memory data register)."[52]  The architecture also uses a program counter ("PC") to keep track of where in the program the machine is.[52]
The term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research. The naming of artificial intelligence also led to the birth of a new field in computer science.[54] On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence's makeup.[55]
McCarthy and his colleagues' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results. They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program. The knowledge to produce a program that sophisticated was not there yet.[55]
The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process.[55] The way computers can understand is at a hardware level. This language is written in binary (1s and 0's). This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.[56]
Minsky's process determined how these artificial neural networks could be arranged to have similar qualities to the human brain. However, he could only produce partial results and needed to further the research into this idea.[55]
McCarthy and Shannon's idea behind this theory was to develop a way to use complex problems to determine and measure the machine's efficiency through mathematical theory and computations.[57] However, they were only to receive partial test results.[55]
The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter. This would allow for a machine to grow in intelligence and increase calculation speeds.[58] The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.[55]
The group thought that research in this category could be broken down into smaller groups. This would consist of sensory and other forms of information about artificial intelligence.[55] Abstractions in computer science can refer to mathematics and programing language.[59]
Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking.[60] They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do. If this machine could do this; they needed to think of how did the machine determine the outcome.[55]

This November is Wikipedia Asian MonthJoin WAM contests and win postcards from Asia.In computer science, a record (also called a structure,  struct, or compound data) is a basic data structure. Records in a database or spreadsheet are usually called "rows".[1][2][3][4]
A record is a collection of fields, possibly of different data types, typically in a fixed number and sequence.[5] The fields of a record may also be called members, particularly in object-oriented programming; fields may also be called elements, though this risks confusion with the elements of a collection.
For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field. A personnel record might contain a name, a salary, and a rank. A Circle record might contain a center and a radiusin this instance, the center itself might be represented as a point record containing x and y coordinates.
Records are distinguished from arrays by the fact that their number of fields is typically fixed, each field has a name, and that each field may have a different type.
A record type is a data type that describes such values and variables. Most modern computer languages allow the programmer to define new record types. The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed.  In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub.  Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.
Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks.  Records are a fundamental component of most data structures, especially linked data structures.  Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.
The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call. Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.
An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types. Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.
A record can be viewed as the computer analog of a mathematical tuple, although a tuple may or may not be considered a record, and vice versa, depending on conventions and the specific programming language.  In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.
A record may have zero or more keys.  A key is a field or set of fields in the record that serves as an identifier.  A unique key is often called the primary key, or simply the record key.  For example an employee file might contain employee number, name, department, and salary.  The employee number will be unique in the organization and would be the primary key.  Depending on the storage medium and file organization the employee number might be indexedthat is also stored in a separate file to make lookup faster.  The department code may not be unique; it may also be indexed, in which case it would be considered a secondary key, or alternate key.  If it is not indexed the entire employee file would have to be scanned to produce a listing of all employees in a specific department.  The salary field would not normally be considered usable as a key.  Indexing is one factor considered when designing a file.
The concept of a record can be traced to various types of tables and ledgers used in accounting since remote times.  The modern notion of records in computer science, with fields of well-defined type and size, was already implicit in 19th century mechanical calculators, such as Babbage's Analytical Engine.[6][7]
The original machine-readable medium used for data (as opposed to control) was punch card used for records in the 1890 United States Census: each punch card was a single record. Compare the journal entry from 1880 and the punch card from 1895. Records were well-established in the first half of the 20th century, when most data processing was done using punched cards. Typically each record of a data file would be recorded in one punched card, with specific columns assigned to specific fields. Generally, a record was the smallest unit that could be read in from external storage (e.g. card reader, tape or disk).
Most machine language implementations and early assembly languages did not have special syntax for records, but the concept was available (and extensively used) through the use of index registers, indirect addressing, and self-modifying code. Some early computers, such as the IBM 1620, had hardware support for delimiting records and fields, and special instructions for copying such records.
The concept of records and fields was central in some early file sorting and tabulating utilities, such as IBM's Report Program Generator (RPG).
COBOL was the first widespread programming language to support record types,[8] and its record definition facilities were quite sophisticated at the time. The language allows for the definition of nested records with alphanumeric, integer, and fractional fields of arbitrary size and precision, as well as fields that automatically format any value assigned to them (e.g., insertion of currency signs, decimal points, and digit group separators). Each file is associated with a record variable where data is read into or written from. COBOL also provides a MOVE CORRESPONDING statement that assigns corresponding fields of two records according to their names.
The early languages developed for numeric computing, such as FORTRAN (up to FORTRAN IV) and Algol 60, did not have support for record types; but later versions of those languages, such as FORTRAN 77 and Algol 68 did add them. The original Lisp programming language too was lacking records (except for the built-in cons cell), but its S-expressions provided an adequate surrogate. The Pascal programming language was one of the first languages to fully integrate record types with other basic types into a logically consistent type system. The PL/I programming language provided for COBOL-style records. The C programming language initially provided the record concept as a kind of template (struct) that could be laid on top of a memory area, rather than a true record data type.  The latter were provided eventually (by the typedef declaration), but the two concepts are still distinct in the language. Most languages designed after Pascal (such as Ada, Modula, and Java), also supported records.
The selection of a field from a record value yields a value.
Some languages may provide facilities that enumerate all fields of a record, or at least the fields that are references. This facility is needed to implement certain services such as debuggers, garbage collectors, and serialization. It requires some degree of type polymorphism.
In systems with record subtyping, operations on values of record type may also include:
In such settings, a specific record type implies that a specific set of fields are present, but values of that type may contain additional fields. A record with fields x, y, and z would thus belong to the type of records with fields x and y, as would a record with fields x, y, and r. The rationale is that passing an (x,y,z) record to a function that expects an (x,y) record as argument should work, since that function will find all the fields it requires within the record. Many ways of practically implementing records in programming languages would have trouble with allowing such variability, but the matter is a central characteristic of record types in more theoretical contexts.
Most languages allow assignment between records that have exactly the same record type (including same field types and names, in the same order). Depending on the language, however, two record data types defined separately may be regarded as distinct types even if they have exactly the same fields.
Some languages may also allow assignment between records whose fields have different names, matching each field value with the corresponding field variable by their positions within the record; so that, for example, a complex number with fields called real and imag can be assigned to a 2D point record variable with fields X and Y.  In this alternative, the two operands are still required to have the same sequence of field types.  Some languages may also require that corresponding types have the same size and encoding as well, so that the whole record can be assigned as an uninterpreted bit string.  Other languages may be more flexible in this regard, and require only that each value field can be legally assigned to the corresponding variable field; so that, for example, a short integer field can be assigned to a long integer field, or vice versa.
Other languages (such as COBOL) may match fields and values by their names, rather than positions.
These same possibilities apply to the comparison of two record values for equality.  Some languages may also allow order comparisons ('<'and '>'), using the lexicographic order based on the comparison of individual fields.[citation needed]
PL/I allows both of the preceding types of assignment, and also allows structure expressions, such as a = a+1; where "a" is a record, or structure in PL/I terminology.
In Algol 68, if Pts was an array of records, each with integer fields X and Y, one could write Y of Pts to obtain an array of integers, consisting of the Y fields of all the elements of Pts.  As a result, the statements Y of Pts[3]:= 7 and (Y of Pts)[3]:= 7 would have the same effect.
In the Pascal programming language, the command with R do S would execute the command sequence S as if all the fields of record R had been declared as variables.  So, instead of writing Pt.X:= 5; Pt.Y:= Pt.X + 3 one could write with Pt do begin X:= 5; Y:= X + 3 end.
The representation of records in memory varies depending on the programming languages. Usually the fields are stored in consecutive positions in memory, in the same order as they are declared in the record type. This may result in two or more fields stored into the same word of memory; indeed, this feature is often used in systems programming to access specific bits of a word. On the other hand, most compilers will add padding fields, mostly invisible to the programmer, in order to comply with alignment constraints imposed by the machinesay, that a floating point field must occupy a single word.
Some languages may implement a record as an array of addresses pointing to the fields (and, possibly, to their names and/or types). Objects in object-oriented languages are often implemented in rather complicated ways, especially in languages that allow multiple class inheritance.
A self-defining record is a type of record which contains information to identify the record type and to locate information within the record.  It may contain the offsets of elements; the elements can therefore be stored in any order or may be omitted.[9] Alternatively, various elements of the record, each including an element identifier, can simply follow one another in any order.
The following show examples of record definitions:


Computer Science & Engineering (CSE) is an academic program at many universities which comprises scientific and engineering aspects of computing. CSE is also a term often used in Europe to translate the name of engineering informatics academic programs. It is offered in both Undergraduate as well Postgraduate with specializations.
Academic programs vary between colleges. Undergraduate Courses usually include programming, algorithms and data structures, computer architecture, operating systems, computer networks, parallel computing, embedded systems, algorithms design, circuit analysis and electronics, digital logic and processor design, computer graphics, scientific computing, software engineering, database systems, digital signal processing, virtualization, computer simulations and games programming. CSE programs also include core subjects of theoretical computer science such as theory of computation, numerical methods, machine learning, programming theory and paradigms.[1] Modern academic programs also cover emerging computing fields like image processing, data science, robotics, bio-inspired computing, computational biology, autonomic computing and artificial intelligence.[2] Most of the above CSE areas require initial mathematical knowledge, hence the first year of study is dominated by mathematical courses, primarily discrete mathematics, mathematical analysis, linear algebra, Probability, and statistics, as well as the basics of Electrical and electronic engineering, physics - field theory, and electromagnetism.

In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions;[1] the remembered information is called the state of the system.
The set of states a system can occupy is known as its state space. In a discrete system, the state space is countable and often finite. The system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state. Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers.  
The output of a digital circuit or deterministic computer program at any time is completely determined by its current inputs and its state.[2]
Digital logic circuits can be divided into two types: combinational logic, whose output signals are dependent only on its present input signals, and sequential logic, whose outputs are a function of both the current inputs and the past history of inputs.[3] In sequential logic, information from past inputs is stored in electronic memory elements, such as flip-flops. The stored contents of these memory elements, at a given point in time, is collectively referred to as the circuit's state and contains all the information about the past to which the circuit has access.[4]
Since each binary memory element, such as a flip-flop, has only two possible states, one or zero, and there is a finite number of memory elements, a digital circuit has only a certain finite number of possible states. If N is the number of binary memory elements in the circuit, the maximum number of states a circuit can have is 2N.
Similarly, a computer program stores data in variables, which represent storage locations in the computer's memory. The contents of these memory locations, at any given point in the program's execution, is called the program's state.[5][6][7]
A more specialized definition of state is used for computer programs that operate serially or sequentially on streams of data, such as parsers, firewalls, communication protocols and encryption. Serial programs operate on the incoming data characters or packets sequentially, one at a time. In some of these programs, information about previous data characters or packets received is stored in variables and used to affect the processing of the current character or packet. This is called a stateful protocol and the data carried over from the previous processing cycle is called the state. In others, the program has no information about the previous data stream and starts fresh with each data input; this is called a stateless protocol.
Imperative programming is a programming paradigm (way of designing a programming language) that describes computation in terms of the program state, and of the statements which change the program state. In declarative programming languages, the program describes the desired results and doesn't specify changes to the state directly.
The output of a sequential circuit or computer program at any time is completely determined by its current inputs and current state. Since each binary memory element has only two possible states, 0 or 1, the total number of different states a circuit can assume is finite, and fixed by the number of memory elements. If there are N binary memory elements, a digital circuit can have at most 2N distinct states. The concept of state is formalized in an abstract mathematical model of computation called a finite state machine, used to design both sequential digital circuits and computer programs.
An example of an everyday device that has a state is a television set. To change the channel of a TV, the user usually presses a "channel up" or "channel down" button on the remote control, which sends a coded message to the set. In order to calculate the new channel that the user desires, the digital tuner in the television must have stored in it the number of the current channel it is on. It then adds one or subtracts one from this number to get the number for the new channel, and adjusts the TV to receive that channel. This new number is then stored as the current channel. Similarly, the television also stores a number that controls the level of volume produced by the speaker. Pressing the "volume up" or "volume down" buttons increments or decrements this number, setting a new level of volume. Both the current channel and current volume numbers are part of the TV's state. They are stored in non-volatile memory, which preserves the information when the TV is turned off, so when it is turned on again the TV will return to its previous station and volume level.
As another example, the state of a microprocessor is the contents of all the memory elements in it: the accumulators, storage registers, data caches, and flags. When computers such as laptops go into a hibernation mode to save energy by shutting down the processor, the state of the processor is stored on the computer's hard disk, so it can be restored when the computer comes out of hibernation, and the processor can take up operations where it left off.


The essence of abstraction is preserving information that is relevant in a given context, and forgetting information that is irrelevant in that context.
 John V. Guttag[1]
In software engineering and computer science, abstraction is:
Abstraction, in general, is a fundamental concept in computer science and software development.[4] The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design.[5] Models can also be considered types of abstractions per their generalization of aspects of reality.
Abstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects,[2] but is also related to other notions of abstraction used in other fields such as art.[3]
Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:
Computing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others.[citation needed] The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.
A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.
Within a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.
Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky  that they can never completely hide the details below;[10] however, this does not negate the usefulness of abstraction.
Some abstractions are designed to inter-operate with other abstractions  for example, a programming language may contain a foreign function interface for making calls to the lower-level language.
Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:
Analysts have developed various methods to formally specify software systems.  Some known methods include:
Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The UML specification language, for example, allows the definition of abstract classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.
Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:
To a human, this seems a fairly simple and obvious calculation ("one plus two is three, times five is fifteen"). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.
Without control abstraction, a programmer would need to specify all the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:
Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with a reduction of the complexity potential for side-effects.
In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.
In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:
These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.
Data abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation. The abstract properties are those that are visible to client code that makes use of the data typethe interface to the data typewhile the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.
For example, one could define an abstract data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.
Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a contract on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.
While much of data abstraction occurs through computer science and automation, there are times when this process is done manually and without programming intervention. One way this can be understood is through data abstraction within the process of conducting a systematic review of the literature. In this methodology, data is abstracted by one or several abstractors when conducting a meta-analysis, with errors reduced through dual data abstraction followed by independent checking, known as adjudication.[13]
In object-oriented programming theory, abstraction involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.
Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime. This would leave only a minimum of such bindings to change at run-time.
Common Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.
C++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.
Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code  all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.
Consider for example a sample Java fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an Animal class to represent both the state of the animal and its functions:
With the above definition, one could create objects of type Animal and call their methods like this:
In the above example, the class Animal is an abstraction used in place of an actual animal, LivingThing is a further abstraction (in this case a generalisation) of Animal.
If one requires a more differentiated hierarchy of animals  to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives  that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.
Such an abstraction could remove the need for the application coder to specify the type of food, so they could concentrate instead on the feeding schedule. The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.
Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysisactually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.
In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchangedthus it is entirely under the control of the programmer, and it is called an abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.
When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.
Abstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if one wishes to know what the result of the evaluation of a mathematical expression involving only integers +, -, , is worth modulo n, then one needs only perform all operations modulo n (a familiar form of this abstraction is casting out nines).
Abstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from themeven though the abstraction may simply yield a result of undecidability. For instance, students in a class may be abstracted by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don't know".
The level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.
Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don't know" to some questions).
Abstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.
Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
[14]
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.
Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:
Physical level: The lowest level of abstraction describes how a system actually stores data. The physical level describes complex low-level data structures in detail.
Logical level: The next higher level of abstraction describes what data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.
View level: The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.
The ability to provide a design of different levels of abstraction can
Systems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.
Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.

In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.
In the object-oriented programming paradigm, object can be a combination of variables, functions, and data structures; in particular in class-based variations of the paradigm it refers to a particular instance of a class.
In the relational model of database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).[1]
An important distinction in programming languages is the difference between an object-oriented language and an object-based language. A language is usually considered object-based if it includes the basic capabilities for an object: identity, properties, and attributes. A language is considered object-oriented if it is object-based and also has the capability of polymorphism, inheritance, encapsulation, and, possibly, composition. Polymorphism refers to the ability to overload the name of a function with multiple behaviors based on which object(s) are passed to it. Conventional message passing discriminates only on the first object and considers that to be "sending a message" to that object. However, some OOP languages such as Flavors and the Common Lisp Object System (CLOS) enable discriminating on more than the first parameter of the function.[2] Inheritance is the ability to subclass an object class, to create a new class that is a subclass of an existing one and inherits all the data constraints and behaviors of its parents but also adds new and/or changes one or more of them.[3][4]
Object-oriented programming is an approach to designing modular reusable software systems. The object-oriented approach is an evolution of good design practices that go back to the very beginning of computer programming. Object-orientation is simply the logical extension of older techniques such as structured programming and abstract data types. An object is an abstract data type with the addition of polymorphism and inheritance.
Rather than structure programs as code and data, an object-oriented system integrates the two using the concept of an "object". An object has state (data) and behavior (code). Objects can correspond to things found in the real world. So for example, a graphics program will have objects such as circle, square, menu. An online shopping system will have objects such as shopping cart, customer, product. The shopping system will support behaviors such as place order, make payment, and offer discount. The objects are designed as class hierarchies. So for example with the shopping system there might be high level classes such as electronics product, kitchen product, and book. There may be further refinements for example under electronic products: CD Player, DVD player, etc. These classes and subclasses correspond to sets and subsets in mathematical logic.[5][6]
An important concept for objects is the design pattern. A design pattern provides a reusable template to address a common problem. The following object descriptions are examples of some of the most common design patterns for objects.[7]
The object-oriented approach is not just a programming model. It can be used equally well as an interface definition language for distributed systems. The objects in a distributed computing model tend to be larger grained, longer lasting, and more service-oriented than programming objects.
A standard method to package distributed objects is via an Interface Definition Language (IDL). An IDL shields the client of all of the details of the distributed server object. Details such as which computer the object resides on, what programming language it uses, what operating system, and other platform-specific issues. The IDL is also usually part of a distributed environment that provides services such as transactions and persistence to all objects in a uniform manner. Two of the most popular standards for distributed objects are the Object Management Group's CORBA standard and Microsoft's DCOM.[8]
In addition to distributed objects, a number of other extensions to the basic concept of an object have been proposed to enable distributed computing:
Some of these extensions, such as distributed objects and protocol objects, are domain-specific terms for special types of "ordinary" objects used in a certain context (such as remote method invocation or protocol composition). Others, such as replicated objects and live distributed objects, are more non-standard, in that they abandon the usual case that an object resides in a single location at a time, and apply the concept to groups of entities (replicas) that might span across multiple locations, might have only weakly consistent state, and whose membership might dynamically change.
The Semantic Web is essentially a distributed-objects framework. Two key technologies in the Semantic Web are the Web Ontology Language (OWL) and the Resource Description Framework (RDF). RDF provides the capability to define basic objectsnames, properties, attributes, relationsthat are accessible via the Internet. OWL adds a richer object model, based on set theory, that provides additional modeling capabilities such as multiple inheritance.
OWL objects are not like standard large-grained distributed objects accessed via an Interface Definition Language. Such an approach would not be appropriate for the Internet because the Internet is constantly evolving and standardization on one set of interfaces is difficult to achieve. OWL objects tend to be similar to the kinds of objects used to define application domain models in programming languages such as Java and C++.
However, there are important distinctions between OWL objects and traditional object-oriented programming objects. Traditional objects get compiled into static hierarchies usually with single inheritance, but OWL objects are dynamic. An OWL object can change its structure at run time and can become an instance of new or different classes.
Another critical difference is the way the model treats information that is currently not in the system. Programming objects and most database systems use the "closed-world assumption". If a fact is not known to the system that fact is assumed to be false. Semantic Web objects use the open-world assumption, a statement is only considered false if there is actual relevant information that it is false, otherwise it is assumed to be unknown, neither true nor false.
OWL objects are actually most like objects in artificial intelligence frame languages such as KL-ONE and Loom.
The following table contrasts traditional objects from Object-Oriented programming languages such as Java or C++ with Semantic Web Objects:[10][11]

A macro (short for "macro instruction", from Greek combining form - "long, large"[1]) in computer science is a rule or pattern that specifies how a certain input should be mapped to a replacement output. Applying a macro to an input is macro expansion. The input and output may be a sequence of lexical tokens or characters, or a syntax tree. Character macros are supported in software applications to make it easy to invoke common command sequences. Token and tree macros are supported in some programming languages to enable code reuse or to extend the language, sometimes for domain-specific languages.
Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone.[2][3] (Thus, they are called "macros" because a "big" block of code can be expanded from a "small" sequence of characters.) Macros often allow positional or keyword parameters that dictate what the conditional assembler program generates and have been used to create entire programs or program suites according to such variables as operating system, platform or other factors. The term derives from "macro instruction", and such expansions were originally used in generating assembly language code.
Keyboard macros and mouse macros allow short sequences of keystrokes and mouse actions to transform into other, usually more time-consuming, sequences of keystrokes and mouse actions. In this way, frequently used or repetitive sequences of keystrokes and mouse movements can be automated. Separate programs for creating these macros are called macro recorders.
During the 1980s, macro programs originally SmartKey, then SuperKey, KeyWorks, Prokey were very popular, first as a means to automatically format screenplays, then for a variety of user input tasks. These programs were based on the TSR (terminate and stay resident) mode of operation and applied to all keyboard input, no matter in which context it occurred. They have to some extent fallen into obsolescence following the advent of mouse-driven user interfaces and the availability of keyboard and mouse macros in applications such as word processors and spreadsheets, making it possible to create application-sensitive keyboard macros.
Keyboard macros can be used in massively multiplayer online role-playing games (MMORPGs) to perform repetitive, but lucrative tasks, thus accumulating resources.  As this is done without human effort, it can skew the economy of the game. For this reason, use of macros is a violation of the TOS or EULA of most MMORPGs, and their administrators spend considerable effort to suppress them.[4]
Keyboard and mouse macros that are created using an application's built-in macro features are sometimes called application macros. They are created by carrying out the sequence once and letting the application record the actions. An underlying macro programming language, most commonly a scripting language, with direct access to the features of the application may also exist.
The programmers' text editor, Emacs, (short for "editing macros") follows this idea to a conclusion. In effect, most of the editor is made of macros. Emacs was originally devised as a set of macros in the editing language TECO; it was later ported to dialects of Lisp.
Another programmers' text editor, Vim (a descendant of vi), also has an implementation of keyboard macros. It can record into a register (macro) what a person types on the keyboard and it can be replayed or edited just like VBA macros for Microsoft Office. Vim also has a scripting language called Vimscript[5] to create macros.
Visual Basic for Applications (VBA) is a programming language included in Microsoft Office from Office 97 through Office 2019 (although it was available in some components of Office prior to Office 97). However, its function has evolved from and replaced the macro languages that were originally included in some of these applications.
XEDIT, running on the Conversational Monitor System (CMS) component of VM, supports macros written in EXEC, EXEC2 and REXX, and some CMS commands were actually wrappers around XEDIT macros. The Hessling Editor (THE), a partial clone of XEDIT, supports Rexx macros using Regina and Open Object REXX (oorexx). Many common applications, and some on PCs, use Rexx as a scripting language.
VBA has access to most Microsoft Windows system calls and executes when documents are opened. This makes it relatively easy to write computer viruses in VBA, commonly known as macro viruses. In the mid-to-late 1990s, this became one of the most common types of computer virus. However, during the late 1990s and to date, Microsoft has been patching and updating their programs. In addition, current anti-virus programs immediately counteract such attacks.
A parameterized macro is a macro that is able to insert given objects into its expansion. This gives the macro some of the power of a function.
As a simple example, in the C programming language, this is a typical macro that is not a parameterized macro:
This causes PI to always be replaced with 3.14159 wherever it occurs. An example of a parameterized macro, on the other hand, is this:
What this macro expands to depends on what argument x is passed to it. Here are some possible expansions:
Parameterized macros are a useful source-level mechanism for performing in-line expansion, but in languages such as C where they use simple textual substitution, they have a number of severe disadvantages over other mechanisms for performing in-line expansion, such as inline functions.
The parameterized macros used in languages such as Lisp, PL/I and Scheme, on the other hand, are much more powerful, able to make decisions about what code to produce based on their arguments; thus, they can effectively be used to perform run-time code generation.
Languages such as C and some assembly languages have rudimentary macro systems, implemented as preprocessors to the compiler or assembler. C preprocessor macros work by simple textual substitution at the token, rather than the character level. However, the macro facilities of more sophisticated assemblers, e.g., IBM High Level Assembler (HLASM) can't be implemented with a preprocessor; the code for assembling instructions and data is interspersed with the code for assembling macro invocations.
A classic use of macros is in the computer typesetting system TeX and its derivatives, where most of the functionality is based on macros.
MacroML is an experimental system that seeks to reconcile static typing and macro systems. Nemerle has typed syntax macros, and one productive way to think of these syntax macros is as a multi-stage computation.
Other examples:
Some major applications have been written as text macro invoked by other applications, e.g., by XEDIT in CMS.
Some languages, such as PHP, can be embedded in free-format text, or the source code of other languages. The mechanism by which the code fragments are recognised (for instance, being bracketed by <?php and ?>) is similar to a textual macro language, but they are much more powerful, fully featured languages.
Macros in the PL/I language are written in a subset of PL/I itself: the compiler executes "preprocessor statements" at compilation time, and the output of this execution forms part of the code that is compiled. The ability to use a familiar procedural language as the macro language gives power much greater than that of text substitution macros, at the expense of a larger and slower compiler.
Frame technology's frame macros have their own command syntax but can also contain text in any language. Each frame is both a generic component in a hierarchy of nested subassemblies, and a procedure for integrating itself with its subassembly frames (a recursive process that resolves integration conflicts in favor of higher level subassemblies). The outputs are custom documents, typically compilable source modules. Frame technology can avoid the proliferation of similar but subtly different components, an issue that has plagued software development since the invention of macros and subroutines.
Most assembly languages have less powerful procedural macro facilities, for example allowing a block of code to be repeated N times for loop unrolling; but these have a completely different syntax from the actual assembly language.
Macro systemssuch as the C preprocessor described earlierthat work at the level of lexical tokens cannot preserve the lexical structure reliably.
Syntactic macro systems work instead at the level of abstract syntax trees, and preserve the lexical structure of the original program. The most widely used implementations of syntactic macro systems are found in Lisp-like languages. These languages are especially suited for this style of macro due to their uniform, parenthesized syntax (known as S-expressions). In particular, uniform syntax makes it easier to determine the invocations of macros. Lisp macros transform the program structure itself, with the full language available to express such transformations. While syntactic macros are often found in Lisp-like languages, they are also available in other languages such as Prolog,[6] Erlang,[7] Dylan,[8] Scala,[9] Nemerle,[10] Rust,[11] Elixir,[12] Nim,[13] Haxe,[14] and Julia.[15] They are also available as third-party extensions to JavaScript,[16] C#[17] and Python.[18]
Before Lisp had macros, it had so-called FEXPRs, function-like operators whose inputs were not the values computed by the arguments but rather the syntactic forms of the arguments, and whose output were values to be used in the computation. In other words, FEXPRs were implemented at the same level as EVAL, and provided a window into the meta-evaluation layer. This was generally found to be a difficult model to reason about effectively.[19]
In 1963, Timothy Hart proposed adding macros to Lisp 1.5 in AI Memo 57: MACRO Definitions for LISP.[20]
An anaphoric macro is a type of programming macro that deliberately captures some form supplied to the macro which may be referred to by an anaphor (an expression referring to another). Anaphoric macros first appeared in Paul Graham's On Lisp and their name is a reference to linguistic anaphorathe use of words as a substitute for preceding words.
In the mid-eighties, a number of papers[21][22] introduced the notion of hygienic macro expansion (syntax-rules), a pattern-based system where the syntactic environments of the macro definition and the macro use are distinct, allowing macro definers and users not to worry about inadvertent variable capture (cf. referential transparency). Hygienic macros have been standardized for Scheme in the R5RS, R6RS, and R7RS standards. A number of competing implementations of hygienic macros exist such as syntax-rules, syntax-case, explicit renaming, and syntactic closures. Both syntax-rules and syntax-case have been standardized in the Scheme standards.
Recently, Racket has combined the notions of hygienic macros with a "tower of evaluators", so that the syntactic expansion time of one macro system is the ordinary runtime of another block of code,[23] and showed how to apply interleaved expansion and parsing in a non-parenthesized language.[24]
A number of languages other than Scheme either implement hygienic macros or implement partially hygienic systems. Examples include Scala, Rust, Elixir, Julia, Dylan, Nim, and Nemerle.
Felleisen conjectures[26] that these three categories make up the primary legitimate uses of macros in such a system. Others have proposed alternative uses of macros, such as anaphoric macros in macro systems that are unhygienic or allow selective unhygienic transformation.
The interaction of macros and other language features has been a productive area of research. For example, components and modules are useful for large-scale programming, but the interaction of macros and these other constructs must be defined for their use together. Module and component-systems that can interact with macros have been proposed for Scheme and other languages with macros. For example, the Racket language extends the notion of a macro system to a syntactic tower, where macros can be written in languages including macros, using hygiene to ensure that syntactic layers are distinct and allowing modules to export macros to other modules.
Macros are normally used to map a short string (macro invocation) to a longer sequence of instructions. Another, less common, use of macros is to do the reverse: to map a sequence of instructions to a macro string. This was the approach taken by the STAGE2 Mobile Programming System, which used a rudimentary macro compiler (called SIMCMP) to map the specific instruction set of a given computer to counterpart machine-independent macros. Applications (notably compilers) written in these machine-independent macros can then be run without change on any computer equipped with the rudimentary macro compiler. The first application run in such a context is a more sophisticated and powerful macro compiler, written in the machine-independent macro language. This macro compiler is applied to itself, in a bootstrap fashion, to produce a compiled and much more efficient version of itself. The advantage of this approach is that complex applications can be ported from one computer to a very different computer with very little effort (for each target machine architecture, just the writing of the rudimentary macro compiler).[27][28] The advent of modern programming languages, notably C, for which compilers are available on virtually all computers, has rendered such an approach superfluous. This was, however, one of the first instances (if not the first) of compiler bootstrapping.
While macro instructions can be defined by a programmer for any set of native assembler program instructions, typically macros are associated with macro libraries delivered with the operating system allowing access to operating system functions such as
In older operating systems such as those used on IBM mainframes, full operating system functionality was only available to assembler language programs, not to high level language programs (unless assembly language subroutines were used, of course), as the standard macro instructions did not always have counterparts in routines available to high-level languages.
In the mid-1950s, when assembly language programming was commonly used to write programs for digital computers, the use of macro instructions was initiated for two main purposes: to reduce the amount of program coding that had to be written by generating several assembly language statements from one macro instruction and to enforce program writing standards, e.g. specifying input/output commands in standard ways.[31] Macro instructions were effectively a middle step between assembly language programming and the high-level programming languages that followed, such as FORTRAN and COBOL. Two of the earliest programming installations to develop "macro languages" for the IBM 705 computer were at Dow Chemical Corp. in Delaware and the Air Material Command, Ballistics Missile Logistics Office in California. A macro instruction written in the format of the target assembly language would be processed by a macro compiler, which was a pre-processor to the assembler, to generate one or more assembly language instructions to be processed next by the assembler program that would translate the assembly language instructions into machine language instructions.[32]
By the late 1950s the macro language was followed by the Macro Assemblers. This was a combination of both where one program served both functions, that of a macro pre-processor and an assembler in the same package.[32][failed verification]
In 1959, Douglas E. Eastwood and Douglas McIlroy of Bell Labs introduced conditional and recursive macros into the popular SAP assembler,[33] creating what is known as Macro SAP.[34] McIlroy's 1960 paper was seminal in the area of extending any (including high-level) programming languages through macro processors.[35][33]
Macro Assemblers allowed assembly language programmers to implement their own macro-language and allowed limited portability of code between two machines running the same CPU but different operating systems, for example, early versions of MSDOS and CPM-86. The macro library would need to be written for each target machine but not the overall assembly language program. Note that more powerful macro assemblers allowed use of conditional assembly constructs in macro instructions that could generate different code on different machines or different operating systems, reducing the need for multiple libraries.[citation needed]
In the 1980s and early 1990s, desktop PCs were only running at a few MHz and assembly language routines were commonly used to speed up programs written in C, Fortran, Pascal and others. These languages, at the time, used different calling conventions. Macros could be used to interface routines written in assembly language to the front end of applications written in almost any language. Again, the basic assembly language code remained the same, only the macro libraries needed to be written for each target language.[citation needed]
In modern operating systems such as Unix and its derivatives, operating system access is provided through subroutines, usually provided by dynamic libraries. High-level languages such as C offer comprehensive access to operating system functions, obviating the need for assembler language programs for such functionality.[citation needed]


Biology is the  scientific study of life.[1][2][3] It is a natural science with a broad scope but has several unifying themes that tie it together as a single, coherent field.[1][2][3] For instance, all organisms are made up of  cells that process hereditary information encoded in genes, which can be transmitted to future generations. Another major theme is evolution, which explains the unity and diversity of life.[1][2][3] Energy processing is also important to life as it allows organisms to move, grow, and reproduce.[1][2][3] Finally, all organisms are able to regulate their own internal environments.[1][2][3][4][5]
Biologists are able to study life at multiple levels of organization.[1] From the molecular biology of a cell to the anatomy and physiology of plants and animals, and evolution of populations.[1][6] Hence, there are multiple subdisciplines within biology, each defined by the nature of their research questions and the tools that they use.[7][8][9] Like other scientists, biologists use the scientific method to make observations, pose questions, generate hypotheses, perform experiments, and form conclusions about the world around them.[1]
Life on Earth, which emerged more than 3.7 billion years ago,[10] is immensely diverse. Biologists have sought to study and classify the various forms of life, from prokaryotic organisms such as archaea and bacteria to eukaryotic organisms such as protists, fungi, plants, and animals. These various organisms contribute to the biodiversity of an ecosystem, where they play specialized roles in the  cycling of nutrients and energy through their biophysical environment.
Biology derives from the Ancient Greek words of  romanized bos meaning 'life' and -; romanized -loga meaning 'branch of study' or 'to speak'.[11][12] Those combined make the Greek word  romanized biologa meaning 'biology'. Despite this, the term  as a whole didn't exist in Ancient Greek. The first to borrow it was the English and French (biologie). Historically there was another term for biology in English, lifelore; it is rarely used today.
The Latin-language form of the term first appeared in 1736 when Swedish scientist Carl Linnaeus (Carl von Linn) used biologi in his Bibliotheca Botanica. It was used again in 1766 in a work entitled Philosophiae naturalis sive physicae: tomus III, continens geologian, biologian, phytologian generalis, by Michael Christoph Hanov, a disciple of Christian Wolff. The first German use, Biologie, was in a 1771 translation of Linnaeus' work. In 1797, Theodor Georg August Roose used the term in the preface of a book, Grundzge der Lehre van der Lebenskraft. Karl Friedrich Burdach used the term in 1800 in a more restricted sense of the study of human beings from a morphological, physiological and psychological perspective (Propdeutik zum Studien der gesammten Heilkunst). The term came into its modern usage with the six-volume treatise Biologie, oder Philosophie der lebenden Natur (180222) by Gottfried Reinhold Treviranus, who announced:[13]
Many other terms used in biology to describe plants, animals, diseases, and drugs have been derived from Greek and Latin due to the historical contributions of the Ancient Greek and Roman civilizations as well as the continued use of these two languages in European universities during the Middle Ages and at the beginning of the Renaissance.[14]
The earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE.[15][16] Their contributions later entered and shaped Greek natural philosophy of classical antiquity.[15][16][17][18] Ancient Greek philosophers such as Aristotle (384322 BCE) contributed extensively to the development of biological knowledge. His works such as History of Animals were especially important because they revealed his naturalist leanings, and later more empirical works that focused on biological causation and the diversity of life. Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botany that survived as the most important contribution of antiquity to the plant sciences, even into the Middle Ages.[19]
Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781869), Al-Dnawar (828896), who wrote on botany,[20] and Rhazes (865925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought, especially in upholding a fixed hierarchy of life.
Biology began to quickly develop and grow with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop the basic techniques of microscopic dissection and staining.[21]
Advances in microscopy also had a profound impact on biological thinking. In the early 19th century, a number of biologists pointed to the central importance of the cell. Then, in 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells. Thanks to the work of Robert Remak and Rudolf Virchow, however, by the 1860s most biologists accepted all three tenets of what came to be known as cell theory.[22][23]
Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735 (variations of which have been in use ever since), and in the 1750s introduced scientific names for all his species.[24] Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleableeven suggesting the possibility of common descent. Although he was opposed to evolution, Buffon is a key figure in the history of evolutionary thought; his work influenced the evolutionary theories of both Lamarck and Darwin.[25]
Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who was the first to present a coherent theory of evolution.[27] He posited that evolution was the result of environmental stress on properties of animals, meaning that the more frequently and rigorously an organ was used, the more complex and efficient it would become, thus adapting the animal to its environment. Lamarck believed that these acquired traits could then be passed on to the animal's offspring, who would further develop and perfect them.[28] However, it was the British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, who forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions.[29][30] Darwin's theory of evolution by natural selection quickly spread through the scientific community and soon became a central axiom of the rapidly developing science of biology.
The basis for modern genetics began with the work of Gregor Mendel, who presented his paper, "Versuche ber Pflanzenhybriden" ("Experiments on Plant Hybridization"), in 1865,[31] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[32] However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics.[33] In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s to the present times, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. Finally, the Human Genome Project was launched in 1990 with the goal of mapping the general human genome. This project was essentially completed in 2003,[34] with further analysis still being published. The Human Genome Project was the first step in a globalized effort to incorporate accumulated knowledge of biology into a functional, molecular definition of the human body and the bodies of other organisms.
All organisms are made up of matter and all matter is made up of elements.[35] Oxygen, carbon, hydrogen, and nitrogen are the four elements that account for 96% of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting the remaining 3.7%.[35] Different elements can combine to form  compounds such as water, which is fundamental to life.[35] Life on Earth began from water and remained there for about three billions years prior to migrating onto land.[36] Matter can exist in different states as a solid, liquid, or gas.
The smallest unit of an element is an atom, which is composed of an atomic nucleus and one or more electrons moving around the nucleus, as described by the Bohr model.[37] The nucleus is made of one or more protons and a number of neutrons. Protons have a positive electric charge, neutrons are electrically neutral, and electrons have a negative electric charge.[38] Atoms with equal numbers of protons and electrons are electrically neutral. The atom of each specific element contains a unique number of protons, which is known as its atomic number, and the sum of its protons and neutrons is an atom's mass number. The masses of individual protons, neutrons, and electrons can be measured in grams or Daltons (Da), with the mass of each proton or neutron rounded to 1 Da.[38] Although all atoms of a specific element have the same number of protons, they may differ in the number of neutrons, thereby existing as isotopes.[35] Carbon, for example, can exist as a stable isotope (carbon-12 or carbon-13) or as a radioactive isotope (carbon-14), the latter of which can be used in radiometric dating (specifically radiocarbon dating) to determine the age of organic materials.[35]
Individual atoms can be held together by chemical bonds to form molecules and ionic compounds.[35] Common types of chemical bonds include ionic bonds, covalent bonds, and hydrogen bonds. Ionic bonding involves the electrostatic attraction between oppositely charged ions, or between two atoms with sharply different electronegativities,[39] and is the primary interaction occurring in ionic compounds. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions) whereas those that lose electrons make positively charged ions (called cations). 
Unlike ionic bonds, a covalent bond involves the sharing of electron pairs between atoms. These electron pairs and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding.[40]
A hydrogen bond is primarily an electrostatic force of attraction between a hydrogen atom which is covalently bound to a more electronegative atom or group such as oxygen. A ubiquitous example of a hydrogen bond is found between water molecules. In a discrete water molecule, there are two hydrogen atoms and one oxygen atom. Two molecules of water can form a hydrogen bond between them. When more molecules are present, as is the case with liquid water, more bonds are possible because the oxygen of one water molecule has two lone pairs of electrons, each of which can form a hydrogen bond with a hydrogen on another water molecule.
Life arose from the Earth's first ocean, which was formed approximately 3.8 billion years ago.[38] Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such as sodium and chloride ions or other small molecules to form an aqueous solution. Once dissolved in water, these solutes are more likely to come in contact with one another and therefore take part in chemical reactions that sustain life.[38]
In terms of its molecular structure, water is a small polar molecule with a bent shape formed by the polar covalent bonds of two hydrogen (H) atoms to one oxygen (O) atom (H2O).[38] Because the OH bonds are polar, the oxygen atom has a slight negative charge and the two hydrogen atoms have a slight positive charge.[38] This polar property of water allows it to attract other water molecules via hydrogen bonds, which makes water cohesive.[38] Surface tension results from the cohesive force due to the attraction between molecules at the surface of the liquid.[38] Water is also adhesive as it is able to adhere to the surface of any polar or charged non-water molecules.[38]
Water is denser as a liquid than it is as a solid (or ice).[38] This unique property of water allows ice to float above liquid water such as ponds, lakes, and oceans, thereby insulating the liquid below from the cold air above.[38] The lower density of ice compared to liquid water is due to the lower number of water molecules that form the crystal lattice structure of ice, which leaves a large amount of space between water molecules.[38] In contrast, there is no crystal lattice structure in liquid water, which allows more water molecules to occupy the same amount of volume.[38]
Water also has the capacity to absorb energy, giving it a higher specific heat capacity than other solvents such as ethanol.[38] Thus, a large amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into gas (or water vapor).[38]
As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before reforming into a water molecule again.[38] In pure water, the number of hydrogen ions balances (or equals) the number of hydroxyl ions, resulting in a pH that is neutral. If hydrogen ions were to exceed hydroxyl ions, then the pH of the solution would be acidic. Conversely, a solution's pH would turn basic if hydroxyl ions were to exceed hydrogen ions.
Organic compounds are molecules that contain carbon bonded to another element such as hydrogen.[38] With the exception of water, nearly all the molecules that make up each organism contain carbon.[38][41] Carbon has six electrons, two of which are located in its first shell, leaving four electrons in its valence shell. Thus, carbon can form covalent bonds with up to four other atoms, making it the most versatile atom on Earth as it is able to form diverse, large, and complex molecules.[38][41] For example, a single carbon atom can form four single covalent bonds such as in methane, two double covalent bonds such as in carbon dioxide (CO2), or a triple covalent bond such as in carbon monoxide (CO). Moreover, carbon can form very long chains of interconnecting carboncarbon bonds such as octane or ring-like structures such as glucose.  
The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound.[38] Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups.[38] There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group.[38]
In 1953, Stanley Miller and Harold Urey conducted a classic experiment (otherwise known as the Miller-Urey experiment), which showed that organic compounds could be synthesized abiotically within a closed system that mimicked the conditions of early Earth, leading them to conclude that complex organic molecules could have arisen spontaneously in early Earth, most likely near volcanoes, and could have part of the early stages of abiogenesis (or origin of life).[42][38]
Macromolecules are large molecules made up of smaller molecular subunits that are joined together.[43] Small molecules such as sugars, amino acids, and nucleotides can act as single repeating units called monomers to form chain-like molecules called polymers via a chemical process called  condensation.[44] For example, amino acids can form  polypeptides whereas nucleotides can form strands of nucleic acid. Polymers make up three of the four macromolecules (polysaccharides, lipids, proteins, and nucleic acids) that are found in all organisms. Each of these macromolecules plays a specialized role within any given cell.
Carbohydrates (or sugar) are molecules with the molecular formula (CH2O)n, with n being the number of carbon-hydrate groups.[45] They include monosaccharides (monomer), oligosaccharides (small polymers), and polysaccharides (large polymers). Monosaccharides can be linked together by glyosidic linkages, a type of covalent bond.[45] When two monosaccharides such as glucose and fructose are linked together, they can form a disaccharide such as sucrose.[45] When many monosaccharides are linked together, they can form an oligosaccharide or a polysaccharide, depending on the number of monosaccharides. Polysaccharides can vary in function. Monosaccharides such as glucose can be a source of energy and some polysaccharides can serve as storage material that can be  hydrolyzed to provide cells with sugar. 
Lipids are the only class of macromolecules that are not made up of polymers. The most biologically important lipids are steroids, phospholipids, and fats.[44] These lipids are organic compounds that are largely nonpolar and hydrophobic.[46] Steroids are organic compounds that consist of four fused rings.[46] Phospholipids consist of glycerol that is linked to a phosphate group and two hydrocarbon chains (or fatty acids).[46] The glycerol and phosphate group together constitute the polar and hydrophilic (or head) region of the molecule whereas the fatty acids make up the nonpolar and hydrophobic (or tail) region.[46] Thus, when in water, phospholipids tend to form a phospholipid bilayer whereby the hydrophobic heads face outwards to interact with water molecules. Conversely, the hydrophobic tails face inwards towards other hydrophobic tails to avoid contact with water.[46]
Proteins are the most diverse of the macromolecules, which include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid, which has a central carbon atom that is covalently bonded to a hydrogen atom, an amino group, a carboxyl group, and a side chain (or R-group, "R" for residue).[43] There are twenty amino acids that make up the building blocks of proteins, with each amino acid having its own unique side chain.[43] The polarity and charge of the side chains affect the solubility of amino acids. An amino acid with a side chain that is polar and electrically charged is soluble as it is hydrophilic whereas an amino acid with a side chain that lacks a charged or an electronegative atom is hydrophobic and therefore tends to coalesce rather than dissolve in water.[43] Proteins have four distinct levels of organization (primary, secondary, tertiary, and quartenary). The primary structure consists of a unique sequence of amino acids that are covalently linked together by peptide bonds.[43] The side chains of the individual amino acids can then interact with each other, giving rise to the secondary structure of a protein.[43] The two common types of secondary structures are alpha helices and beta sheets.[43] The folding of alpha helices and beta sheets gives a protein its three-dimensional or tertiary structure. Finally, multiple tertiary structures can combine to form the quaternary structure of a protein.  
Nucleic acids are polymers made up of monomers called nucleotides.[47] Their function is to store, transmit, and express hereditary information.[44] Nucleotides consist of a phosphate group, a five-carbon sugar, and a nitrogenous base. Ribonucleotides, which contain ribose as the sugar, are the monomers of  ribonucleic acid (RNA). In contrast, deoxyribonucleotides contain deoxyribose as the sugar and are constitute the monomers of  deoxyribonucleic acid (DNA). RNA and DNA also differ with respect to one of their bases.[47] There are two types of bases: purines and pyrimidines.[47] The purines include guanine (G) and adenine (A) whereas the pyrimidines consist of cytosine (T), uracil (U), and thymine (T). Uracil is used in RNA whereas thymine is used in DNA. Taken together, when the different sugar and bases are take into consideration, there are eight distinct nucleotides that can form two types of nucleic acids: DNA (A, G, C, and T) and RNA (A, G, C, and U).[47]
Cell theory states that cells are the fundamental units of life, that all living things are composed of one or more cells, and that all cells arise from preexisting cells through cell division.[48] Most cells are very small, with diameters ranging from 1 to 100micrometers and are therefore only visible under a  light or electron microscope.[49] There are generally two types of cells: eukaryotic cells, which contain a nucleus, and prokaryotic cells, which do not. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg.
Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space.[50] A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions.[51] Cell membranes also contains membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell.[52] Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton. 
Within the cytoplasm of a cell, there are many biomolecules such as proteins and nucleic acids.[53] In addition to biomolecules, eukaryotic cells have specialized structures called organelles that have their own lipid bilayers or are spatially units.[54] These organelles include the cell nucleus, which contains most of the cell's DNA, or  mitochondria, which generates adenosine triphosphate (ATP) to power cellular processes. Other organelles such as endoplasmic reticulum and Golgi apparatus play a role in the synthesis and packaging of proteins, respectively. Biomolecules such as proteins can be engulfed by lysosomes, another specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural support as well as being involved in reproduction and breakdown of plant seeds.[54] Eukaryotic cells also have cytoskeleton that is made up of microtubules, intermediate filaments, and microfilaments, all of which provide support for the cell and are involved in the movement of the cell and its organelles.[54] In terms of their structural composition, the microtubules are made up of tubulin (e.g., -tubulin and -tubulin whereas intermediate filaments are made up of fibrous proteins.[54] Microfilaments are made up of actin molecules that interact with other strands of proteins.[54]
All cells require energy to sustain cellular processes. Energy is the capacity to do work, which, in thermodynamics, can be calculated using Gibbs free energy. According to the first law of thermodynamics, energy is conserved, i.e., cannot be created or destroyed. Hence, chemical reactions in a cell do not create new energy but are involved instead in the transformation and transfer of energy.[55] Nevertheless, all energy transfers lead to some loss of usable energy, which increases entropy (or state of disorder) as stated by the second law of thermodynamics. As a result, an organism requires continuous input of energy to maintain a low state of entropy. In cells, energy can be transferred as electrons during redox (reductionoxidation) reactions, stored in covalent bonds, and generated by the movement of ions (e.g., hydrogen, sodium, potassium) across a membrane.
Metabolism is the set of life-sustaining chemical reactions in organisms. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to building blocks for proteins, lipids, nucleic acids, and some carbohydrates; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. Metabolic reactions may be categorized as catabolic  the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic  the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy.
The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts  they allow a reaction to proceed more rapidly without being consumed by it  by reducing the amount of activation energy needed to convert reactants into  products. Enzymes also allow the regulation of the rate of a  metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells.
Cellular respiration is a set of metabolic reactions and processes that take place in the cells of organisms to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products.[56] The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy because weak high-energy bonds, in particular in molecular oxygen,[57] are replaced by stronger bonds in the products. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions.
Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation.[58] Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two  pyruvates, with two net molecules of ATP being produced at the same time.[58] Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-Coa enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force.[58] Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor.
If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis.  In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis.  This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen.
Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.[59][60][61] In most cases, oxygen is also released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.[62]
Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation.[58] Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the  thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration.[58]
During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase.[58] The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle.[63]
Cell communication (or signaling) is the ability of cells to receive, process, and transmit signals with its environment and with itself.[64][65] Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell.[66][65] There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones.[66] In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affect them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a  synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an  inotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction
The cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning of its cytoplasm into two daughter cells in a process called cell division.[67] In eukaryotes (i.e., animal, plant, fungal, and protist cells), there are two distinct types of cell division: mitosis and meiosis.[68] Mitosis is part of the cell cycle, in which replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of mitosis all together define the mitotic phase of an animal cell cyclethe division of the mother cell into two  genetically identical daughter cells.[69] The cell cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. In contrast to mitosis, meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions.[70] Homologous chromosomes are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor.
Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission takes in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and "Z-ring" formation)[71] The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids.
Genetics is the scientific study of inheritance.[72][73][74] Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring.[32] It was formulated by Gregor Mendel, based on his work with pea plants in the mid-nineteenth century. Mendel established several principles of inheritance. The first is that genetic characteristics, which are now called alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on his law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele.[75] Exceptions to this rule include penetrance and expressivity.[32] Mendel noted that during gamete formation, the alleles for each gene segregate from each other so that each gamete carries only one allele for each gene, which is stated by his law of segregation. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, Mendel formulated the law of independent assortment, which states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are  sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype.[76] A Punnett square can be used to predict the results of a test cross. The  chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with  fruit flies, which established the sex linkage between eye color and sex in these insects.[77] In humans and other mammals (e.g., dogs), it is not feasible or practical to conduct test cross experiments. Instead, pedigrees, which are genetic representations of family trees,[78] are used instead to trace the inheritance of a specific trait or disease through multiple generations.[79]
A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that influences the form or function of an organism in specific ways. DNA is a molecule composed of two polynucleotide chains that coil around each other to form a double helix, which was first described by James Watson and Francis Crick in 1953.[80] It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. A chromosome is an organized structure consisting of DNA and histones. The set of chromosomes in a cell and any other hereditary information found in the mitochondria, chloroplasts, or other locations is collectively known as a cell's genome. In eukaryotes, genomic DNA is localized in the cell nucleus, or with small amounts in mitochondria and chloroplasts.[81] In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid.[82] The genetic information in a genome is held within genes, and the complete assemblage of this information in an organism is called its genotype.[83] Genes encode the information needed by cells for the synthesis of proteins, which in turn play a central role in influencing the final phenotype of the organism.
The two polynucleotide strands that make up DNA run in opposite directions to each other and are thus antiparallel. Each strand is composed of nucleotides,[84][85] with each nucleotide containing one of four nitrogenous  bases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. It is the sequence of these four bases along the backbone that encodes genetic information. Bases of the two polynucleotide strands are bound together by hydrogen bonds, according to base pairing rules (A with T and C with G), to make double-stranded DNA. The bases are divided into two groups: pyrimidines and purines. In DNA, the pyrimidines are thymine and cytosine whereas the purines are adenine and guanine. 
There are grooves that run along the entire length of the double helix due to the uneven spacing of the DNA strands relative to each other.[80] Both grooves differ in size, with the major groove being larger and therefore more accessible to the binding of proteins than the minor groove.[80] The outer edges of the bases are exposed to these grooves and are therefore accessible for additional hydrogen bonding.[80] Because each groove can have two possible base-pair configurations (G-C and A-T), there are four possible base-pair configurations within the entire double helix, each of which is chemically distinct from another.[80] As a result, protein molecules are able to recognize and bind to specific base-pair sequences, which is the basis of specific DNA-protein interactions. 
DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA.[80] The process begins with the unwounding of the double helix at an origin of replication, which separates the two strands, thereby making them available as two templates. This is then followed by the binding of the enzyme primase to the template to synthesize a starter RNA (or DNA in some viruses) strand called a primer from the 5 to 3 location.[80] Once the primer is completed, the primase is released from the template, followed by the binding of the enzyme DNA polymerase to the same template to synthesize new DNA. 
DNA replication is not perfect as the DNA polymerase sometimes insert bases that are not complementary to the template (e.g., putting in A in the strand opposite to G in the template strand).[80] In eukaryotes, the initial error or mutation rate is about 1 in 100,000.[80] Proofreading and mismatch repair are the two mechanisms that repair these errors, which reduces the mutation rate to 10-10, particularly before and after a cell cycle.[80]
Mutations are heritable changes in DNA.[80] They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes).[80] Mutations can appear as a change in single base or at a larger scale involving chromosomal mutations such as deletions, inversions, or translocations.[80]
In multicellular organisms, mutations can occur in somatic or germline cells.[80] In somatic cells, the mutations are passed on to daughter cells during mitosis.[80] In a germline cell such as a sperm or an egg, the mutation will appear in an organism at fertilization.[80] Mutations can lead to several types of phenotypic effects such as silent, loss-of-function, gain-of-function, and conditional mutations.[80]
Some mutations can be beneficial, as they are a source of genetic variation for evolution.[80] Others can be harmful if they were to result in a loss of function of genes needed for survival.[80] Mutagens such as carcinogens are typically avoided as a matter of public health policy goals.[80] One example is the banning of chlorofluorocarbons (CFC) by the Montreal Protocol, as CFCs tend to deplete the ozone layer, resulting in more ultraviolet radiation from the sun passing through the Earth's upper atmosphere, thereby causing somatic mutations that can lead to skin cancer.[80] Similarly, smoking bans have been enforced throughout the world in an effort to reduce the incidence of lung cancer.[80]
Gene expression is the molecular process by which a genotype gives rise to a phenotype, i.e., observable trait. The genetic information stored in DNA represents the genotype, whereas the phenotype results from the synthesis of proteins that control an organism's structure and development, or that act as enzymes catalyzing specific metabolic pathways. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958.[86][87][88] According to the Central Dogma, genetic information flows from DNA to RNA to protein. Hence, there are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein).[89] These processes are used by all lifeeukaryotes (including multicellular organisms), prokaryotes (bacteria and archaea), and are exploited by virusesto generate the macromolecular machinery for life. 
During transcription, messenger RNA (mRNA) strands are created using DNA strands as a template, which is initiated when RNA polymerase binds to a DNA sequence called a promoter, which instructs the RNA to begin transcription of one of the two DNA strands.[90] The DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U).[91] In eukaryotes, a large part of DNA (e.g., >98% in humans) contain non-coding called introns, which do not serve as patterns for protein sequences. The coding regions or exons are interspersed along with the introns in the primary transcript (or pre-mRNA).[90] Before translation, the pre-mRNA undergoes further processing whereby the introns are removed (or spliced out), leaving only the spliced exons in the mature mRNA strand.[90]
The translation of mRNA to protein occurs in ribosomes, whereby the transcribed mRNA strand specifies the sequence of amino acids within proteins using the genetic code. Gene products are often proteins, but in non-protein-coding genes such as transfer RNA (tRNA) and small nuclear RNA (snRNA), the product is a functional non-coding RNA.[92][93]
The regulation of gene expression (or gene regulation) by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein.[94]
The ability of gene transcription to be regulated allows for the conservation of energy as cells will only make proteins when needed.[94] Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter.[94] A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans).[94][95] It was first identified in Escherichia colia prokaryotic cell that can be found in the intestines of humans and other animalsin the 1960s by Franois Jacob and Jacques Monod.[94] They studied the prokaryotic cell's lac operon, which is part of three genes (lacZ, lacY, and lacA) that encode three lactose-metabolizing enzymes (-galactosidase, -galactoside permease, and -galactoside transacetylase).[94] In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. In contrast, negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. When a repressor binds to a repressible operon (e.g., trp operon), it does so only in the presence of a corepressor. Repressors can be inhibited by compounds called inducers (e.g., allolactose), which exert their effects by binding to a repressor to prevent it from binding to an operator, thereby allowing transcription to occur.[94] Specific genes that can be activated by inducers are called inducible genes (e.g., lacZ or lacA in E. coli), which are in contrast to constitutive genes that are almost always active.[94] In contrast to both, structural genes encode proteins that are not involved in gene regulation.[94]
In prokaryotic cells, transcription is regulated by proteins called sigma factors, which bind to RNA polymerase and direct it to specific promoters.[94] Similarly, transcription factors in eukaryotic cells can also coordinate the expression of a group of genes, even if the genes themselves are located on different chromosomes.[94] Coordination of these genes can occur as long as they share the same regulatory DNA sequence that bind to the same transcription factors.[94] Promoters in eukaryotic cells are more diverse but tend to contain a core sequence that RNA polymerase can bind to, with the most common sequence being the TATA box, which contains multiple repeating A and T bases.[94] Specifically, RNA polymerase II is the RNA polymerase that binds to a promoter to initiate transcription of protein-coding genes in eukaryotes, but only in the presence of multiple general transcription factors, which are distinct from the transcription factors that have regulatory effects, i.e., activators and repressors.[94] In eukaryotic cells, DNA sequences that bind with activators are called enhances whereas those sequences that bind with repressors are called silencers.[94] Transcription factors such as nuclear factor of activated T-cells (NFAT) are able to identify specific nucleotide sequence based on the base sequence (e.g., CGAGGAAAATTG for NFAT) of the binding site, which determines the arrangement of the chemical groups within that sequence that allows for specific DNA-protein interactions.[94] The expression of transcription factors is what underlies cellular differentiation in a developing embryo.[94]
In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells.[94]
Post-transcriptional control of mRNA can involve the alternative splicing of primary mRNA transcripts, resulting in a single gene giving rise to different mature mRNAs that encode a family of different proteins.[94][96] A well-studied example is the Sxl gene in Drosophila, which determines the sex in these animals. The gene itself contains four exons and alternative splicing of its pre-mRNA transcript can generate two active forms of the Sxl protein in female flies and one in inactive form of the protein in males.[94] Another example is the human immunodeficiency virus (HIV), which has a single pre-mRNA transcript that can generate up to nine proteins as a result of alternative splicing.[94] In humans, eighty percent of all 21,000 genes are alternatively spliced.[94] Given that both chimpanzees and humans have a similar number of genes, it is thought that alternative splicing might have contributed to the latter's complexity due to the greater number of alternative splicing in the human brain than in the brain of chimpanzees.[94]
Translation can be regulated in three known ways, one of which involves the binding of tiny RNA molecules called microRNA (miRNA) to a target mRNA transcript, which inhibits its translation and causes it to degrade.[94] Translation can also be inhibited by the modification of the 5 cap by substituting the modified guanosine triphosphate (GTP) at the 5 end of an mRNA for an unmodified GTP molecule.[94] Finally, translational repressor proteins can bind to mRNAs and prevent them from attaching to a ribosome, thereby blocking translation.[94]
Once translated, the stability of proteins can be regulated by being targeted for degradation.[94] A common example is when an enzyme attaches a regulatory protein called ubiquitin to the lysine residue of a targeted protein.[94] Other ubiquitins then attached to the primary ubiquitin to form a polyubiquitinated protein, which then enters a much larger protein complex called proteasome.[94] Once the polyubiquitinated protein enters the proteasome, the polyubiquitin detaches from the target protein, which is unfolded by the proteasome in an ATP-dependent manner, allowing it to be hydrolyzed by three proteases.[94]
A genome is an organism's complete set of DNA, including all of its genes.[97] Sequencing and analysis of genomes can be done using high throughput DNA sequencing and bioinformatics to assemble and analyze the function and structure of entire genomes.[98][99][100] The genomes of prokaryotes are small, compact, and diverse. In contrast, the genomes of eukaryotes are larger and more complex such as having more regulatory sequences and much of its genome are made up of non-coding DNA sequences for functional RNA (rRNA, tRNA, and mRNA) or regulatory sequences. The genomes of various model organisms such as arabidopsis, fruit fly, mice, nematodes, and yeast have been sequenced. The Human Genome Project was a major undertaking by the international scientific community to sequence the entire human genome, which was completed in 2003.[101] The sequencing of the human genome has yielded practical applications such as DNA fingerprinting, which can be used for paternity testing and forensics. In medicine, sequencing of the entire human genome has allowed for the identification of mutations that cause  tumors as well as genes that cause a specific genetic disorder.[101] The sequencing of genomes from various organisms has led to the emergence of comparative genomics, which aims to draw comparisons of genes from the genomes of those different organisms.[101]
Many genes encode more than one protein, with posttranslational modifications increasing the diversity of proteins within a cell. An organism's proteome is its entire set of proteins expressed by its genome and proteomics seeks to study the complete set of proteins produced by an organism.[101] Because many proteins are enzymes, their activities tend to affects the concentrations of substrates and products. Thus, as the proteome changes, so do the amount of small molecules or metabolites.[101] The complete set of small molecules in a cell or organism is called a metabolome and metabolomics is the study of the metabolome in relation to the physiological activity of a cell or organism.[101]
Biotechnology is the use of cells or organisms to develop products for humans.[102] One commonly used technology with wide applications is the creation of recombinant DNA, which is a DNA molecule assembled from two or more sources in a laboratory. Before the advent of polymerase chain reaction, biologists would manipulate DNA by cutting it into smaller fragments using restriction enzymes. They would then purify and analyze the fragments using gel electrophoresis and then later recombine the fragments into a novel DNA sequence using DNA ligase.[102] The recombinant DNA is then cloned by inserting it into a host cell, a process known as transformation if the host cells were bacteria such as E. coli,  or transfection if the host cells were eukaryotic cells like yeast, plant, or animal cells. Once the host cell or organism has received and integrated the recombinant DNA, it is described as transgenic.[102]
A recombinant DNA can be inserted in one of two ways. A common method is to simply insert the DNA into a host chromosome, with the site of insertion being random.[102] Another approach would be to insert the recombinant DNA as part of another DNA sequence called a vector, which then integrates into the host chromosome or has its own origin of DNA replication, thereby allowing to replicate independently of the host chromosome.[102] Plasmids from bacterial cells such as E. coli are typically used as vectors due to their relatively small size (e.g. 2000-6000 base pairs in E. coli), presence of restriction enzymes, genes that are resistant to antibiotics, and the presence of an origin of replication.[102] A gene coding for a selectable marker such as antibiotic resistance is also incorporated into the vector.[102] Inclusion of this market allows for the selection of only those host cells that contained the recombinant DNA while discarding those that do not.[102] Moreover, the marker also serves as a reporter gene that once expressed, can be easily detected and measured.[102]
Once the recombinant DNA is inside individual bacterial cells, those cells are then plated and allowed to grow into a colony that contains millions of transgenic cells that carry the same recombinant DNA.[103] These transgenic cells then produce large quantities of the transgene product such as human insulin, which was the first medicine to be made using recombinant DNA technology.[102]
DNA fragments used in molecular cloning procedures can also be used to create a genomic library.[102]
Other biotechnology tools include DNA microarrays, expression vectors, synthetic genomics, and CRISPR gene editing.[102][104] Other approaches such as pharming can produce large quantities of medically useful products through the use of genetically modified organisms.[102] Many of these other tools also have wide applications such as creating medically useful proteins, or improving  plant cultivation and animal husbandry.[102]
Development is the process by which a multicellular organism (plant or animal) goes through a series of a changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle.[106] There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells from less specialized cells such as stem cells.[107][108] Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell.[109] Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics.  With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself.[110] Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or development of body form, is the result of spatial differences in gene expression.[106] Specially, the organization of differentiated tissues into specific structures such as arms or wings, which is known as  pattern formation, is governed by morphogens, signaling molecules that move from one group of cells to surrounding cells, creating a morphogen gradient as described by the French flag model. Apoptosis, or programmed cell death, also occurs during morphogenesis, such as the death of cells between digits in human embryonic development, which frees up individual fingers and toes. Expression of transcription factor genes can determine organ placement in a plant and a cascade of transcription factors themselves can establish body segmentation in a fruit fly.[106]
A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva.[111] Variations in the toolkit may have produced a large part of the morphological evolution of animals. The toolkit can drive evolution in two ways. A toolkit gene can be expressed in a different pattern, as when the beak of Darwin's large ground-finch was enlarged by the BMP gene,[112] or when snakes lost their legs as Distal-less (Dlx) genes became under-expressed or not expressed at all in the places where other reptiles continued to form their limbs.[113] Or, a toolkit gene can acquire a new function, as seen in the many functions of that same gene, distal-less, which controls such diverse structures as the mandible in vertebrates,[114][115] legs and antennae in the fruit fly,[116] and eyespot pattern in butterfly wings.[117] Given that small changes in toolbox genes can cause significant changes in body structures, they have often enabled convergent or parallel evolution.
A central organizing concept in biology is that life changes and develops through evolution, which is the change in heritable characteristics of populations over successive generations.[118][119] Evolution is now used to explain the great variations of life on Earth. The term evolution was introduced into the scientific lexicon by Jean-Baptiste de Lamarck in 1809.[120][121] He proposed that evolution occurred as a result of inheritance of acquired characteristics, which was unconvincing but there were no alternative explanations at the time.[120] Charles Darwin, an English naturalist, had returned to England in 1836 from his five-year travels on the HMS Beagle where he studied rocks and collected plants and animals from various parts of the world such as the Galpagos Islands.[120] He had also read Principles of Geology by Charles Lyell and An Essay on the Principle of Population by Thomas Malthus and was influenced by them.[122] Based on his observations and readings, Darwin began to formulate his theory of evolution by natural selection to explain the diversity of plants and animals in different parts of the world.[120][122] Alfred Russel Wallace, another English naturalist who had studied plants and animals in the Malay Archipelago, also came to the same idea, but later and independently of Darwin.[120] Both Darwin and Wallace jointly presented their essay and manuscript, respectively, at the Linnaean Society of London in 1858, giving them both credit for their discovery of evolution by natural selection.[120][123][124][125][126] Darwin would later publish his book On the Origin of Species in 1859, which explained in detail how the process of evolution by natural selection works.[120]
To explain natural selection, Darwin drew an analogy with humans modifying animals through artificial selection, whereby animals were selectively bred for specific traits, which has given rise to individuals that no longer resemble their wild ancestors.[122] Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits. He came to this conclusion based on two observations and two inferences.[122] First, members of any population tend to vary with respect to their heritable traits. Second, all species tend to produce more offspring than can be supported by their respective environments, resulting in many individuals not surviving and reproducing.[122] Based on these observations, Darwin inferred that those individuals who possessed heritable traits that are better adapted to their environments are more likely to survive and produce more offspring than other individuals.[122] He further inferred that the unequal or differential survival and reproduction of certain individuals over others will lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment.[122][127][128] Thus, taken together, natural selection is the differential survival and reproduction of individuals in subsequent generations due to differences in or more heritable traits.[129][122][120]
Darwin was not aware of Mendel's work of inheritance and so the exact mechanism of inheritance that underlie natural selection was not well-understood[130] until the early 20th century when the modern synthesis reconciled Darwinian evolution with classical genetics, which established a neo-Darwinian perspective of evolution by natural  selection.[129] This perspective holds that evolution occurs when there are changes in the allele frequencies within a population of interbreeding organisms. In the absence of any evolutionary process acting on a large random mating population, the allele frequencies will remain constant across generations as described by the HardyWeinberg principle.[131]
Another process that drives evolution is genetic drift, which is the random fluctuations of allele frequencies within a population from one generation to the next.[132] When selective forces are absent or relatively weak, allele frequencies are equally likely to drift upward or downward at each successive generation because the alleles are subject to sampling error.[133] This drift halts when an allele eventually becomes fixed, either by disappearing from the population or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone.
A species is a group of organisms that mate with one another and speciation is the process by which one lineage splits into two lineages as a result of having evolved independently from each other.[134] For speciation to occur, there has to be reproductive isolation.[134] Reproductive isolation can result from incompatibilities between genes as described by BatesonDobzhanskyMuller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation.[134] In contrast, sympatric speciation occurs in the absence of physical barriers. 
 Pre-zygotic isolation such as mechanical, temporal,  behavioral, habitat, and gametic isolations can prevent different species from  hybridizing.[134] Similarly, post-zygotic isolations can result in hybridization being selected against due to the lower viability of hybrids or hybrid infertility (e.g., mule). Hybrid zones can emerge if there were to be incomplete reproductive isolation between two closely related species.
A phylogeny is an evolutionary history of a specific group of organisms or their genes.[135] It can be represented using a phylogenetic tree, which is a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a node (or split) on the phylogenetic tree. The more splits there are over time, the more branches there will be on the tree, with the common ancestor of all the organisms in that tree being represented by the root of that tree. Phylogenetic trees may portray the evolutionary history of all life forms, a major evolutionary group (e.g., insects), or an even smaller group of closely related species. Within a tree, any group of species designated by a name is a taxon (e.g., humans, primates, mammals, or vertebrates) and a taxon that consists of all its evolutionary descendants is a clade, otherwise known as a monophyletic taxon.[135] Closely related species are referred to as sister species and closely related clades are sister clades. In contrast to a monophyletic group, a polyphyletic group does not include its common ancestor whereas a paraphyletic group does not include all the descendants of a common ancestor.[135]
Phylogenetic trees are the basis for comparing and grouping different species.[135] Different species that share a feature inherited from a common ancestor are described as having  homologous features (or synapomorphy).[136][137][135] Homologous features may be any heritable traits such as DNA sequence, protein structures, anatomical features, and behavior patterns. A vertebral column is an example of a homologous feature shared by all vertebrate animals. Traits that have a similar form or function but were not derived from a common ancestor are described as  analogous features. Phylogenies can be reconstructed for a group of organisms of primary interests, which are called the ingroup. A species or group that is closely related to the ingroup but is phylogenetically outside of it is called the outgroup, which serves a reference point in the tree. The root of the tree is located between the ingroup and the outgroup.[135] When phylogenetic trees are reconstructed, multiple trees with different evolutionary histories can be generated. Based on the principle of Parsimony (or Occam's razor), the tree that is favored is the one with the fewest evolutionary changes needed to be assumed over all traits in all groups. Computational algorithms can be used to determine how a tree might have evolved given the evidence.[135]
Phylogeny provides the basis of biological classification, which is based on Linnaean taxonomy that was developed by Carl Linnaeus in the 18th century.[135] This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order,  family, genus, and species.[135] All organisms can be classified as belonging to one of  three domains: Archaea (originally Archaebacteria); bacteria (originally eubacteria), or eukarya (includes the protist, fungi, plant, and animal kingdoms).[138] A binomial nomenclature is used to classify different species. Based on this system, each species is given two names, one for its genus and another for its species.[135] For example, humans are Homo sapiens, with Homo being the genus and sapiens being the species. By convention, the scientific names of organisms are italicized, with only the first letter of the genus capitalized.[139][140]
The history of life on Earth traces the processes by which organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago.[141][142] The dating of the Earth's history can be done using several geological methods such as stratigraphy, radiometric dating, and paleomagnetic dating.[143] Based on these methods, geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years.[143] Each eon can be divided into eras, with the Phanerozoic eon that began 542 million years ago being subdivided into Paleozoic, Mesozoic, and Cenozoic eras.[143] These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary) and each period into epochs.[143]
The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor.[144] Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes.[145][10][146][147]  Microbal mats of coexisting bacteria and archaea were the dominant form of life in the early Archean epoch and many of the major steps in early evolution are thought to have taken place in this environment.[148] The earliest evidence of eukaryotes dates from 1.85 billion years ago,[149][150] and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions.[151]
Algae-like multicellular land plants are dated back even to about 1 billion years ago,[152] although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago.[153] Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event.[154]
Ediacara biota appear during the Ediacaran period,[155] while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion.[156] During the Permian period, synapsids, including the ancestors of mammals, dominated the land,[157] but most of this group became extinct in the PermianTriassic extinction event 252 million years ago.[158] During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates;[159] one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods.[160] After the CretaceousPaleogene extinction event 66 million years ago killed off the non-avian dinosaurs,[161] mammals increased rapidly in size and diversity.[162] Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.[163]
Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste,[164] and the deep biosphere of the earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.[165]
Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use.[166] Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi.[167] Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes,[168] including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.
The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.
Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin.[169] Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.[170]
Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern day eukaryotic cells.[171] The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals.[171] Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals.[171] While it is likely that protists share a common ancestor (the last eukaryotic common ancestor),[172] protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience.[171][173] Most protists are unicellular, which are also known as microbial eukaryotes.[171]
The alveolates are mostly photosynthetic unicellular protists that possess sacs called alveoli (hence their name alveolates) that are located beneath their cell membrane, providing support for the cell surface.[171] Alveolates comprise several groups such as dinoflagellates, apicomplexans, and ciliates. Dinoflagellates are photosynthetic and can be found in the ocean where they play a role as primary producers of organic matter.[171] Apicomplexans are parasitic alveolates that possess an apical complex, which is a group of organelles located in the apical end of the cell.[171] This complex allows apicomplexans to invade their hosts' tissues. Ciliates are alveolates that possess numerous hair-like structure called cilia. A defining characteristic of ciliates is the presence of two types of nuclei in each ciliate cell. A commonly studied ciliate is the paramecium.[171]
The excavates are groups of protists that began to diversify approximately 1.5 billion years ago shortly after the origin of the eukaryotes.[171] Some excavates do not possess mitochondria, which are thought to have been lost over the course of evolution as these protists still possess nuclear genes that are associated with mitochondria.[171] The excavates comprise several groups such as diplomonads, parabasalids, heteroloboseans, euglenids, and kinetoplastids.[171]
Stramenopiles, most of which can be characterized by the presence of tubular hairs on the longer of their two flagella, include diatoms and brown algae.[171] Diatoms are primary producers and contribute about one-fifth of all photosynthetic carbon fixation, making them a major component of phytoplankton.[171]
Rhizarians are mostly unicellular and aquatic protists that typically contain long, thin pseudopods.[171] The rhizarians comprise three main groups: cercozoans, foraminiferans, and radiolarians.[171]
Amoebozoans are protists with a body form characterized by the presence lobe-shaped pseudopods, which help them to move.[171] They include groups such as loboseans and slime molds (e.g., plasmodial slime mold and cellular slime molds).[171]
Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. A shared derived trait (or synapomorphy) of Plantae is the primary endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts.[174] The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related.[174] Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae.[174] Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts.[174]
Land plants (embryophytes) first appeared in terrestrial environments approximately 450 to 500 million years ago.[174] A synapomorphy of land plants is an embryo that develops under the protection of tissues of its parent plant.[174] Land plants comprise ten major clades, seven of which constitute a single clade known as vascular plants (or tracheophytes) as they all have tracheids, which are fluid-conducting cells, and a well-developed system that transports materials throughout their bodies.[174] In contrast, the other three clades are nonvascular plants as they do not have tracheids.[174] They also do not constitute a single clade.[174]
Nonvascular plants include liverworts, mosses, and hornworts. They tend to be found in areas where water is readily available.[174] Most live on soil or even on vascular plants themselves. Some can grow on bare rock, tree trunks that are dead or have fallen, and even buildings.[174] Most nonvascular plants are terrestrial, with a few living in freshwater environments and none living in the oceans.[174]
The seven clades (or divisions) that make up vascular plants include horsetails and ferns, which together can be grouped as a single clade called monilophytes.[174] Seed plants (or spermatophyte) comprise the other five divisions, four of which are grouped as gymnosperms and one is angiosperms. Gymnosperms includes conifers, cycads, Ginkgo, and gnetophytes. Gymnosperm seeds develop either on the surface of scales or leaves, which are often modified to form cones, or solitary as in yew, Torreya, Ginkgo.[175] Angiosperms are the most diverse group of land plants, with 64 orders, 416 families, approximately 13,000 known genera and 300,000 known species.[176] Like gymnosperms, angiosperms are seed-producing plants. They are distinguished from gymnosperms by having characteristics such as flowers, endosperm within their seeds, and production of fruits that contain the seeds.
Fungi are eukaryotic organisms that digest foods outside of their bodies.[177] They do so through a process called absorptive heterotrophy whereby they would first secrete digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes as they are able to take in nutrients from dead organic matter and are hence, the principal decomposers in ecological systems.[177] Some fungi are parasites by absorbing nutrients from living hosts while others are mutualists.[177] Fungi, along with two other lineages, choanoflagellates and animals, can be grouped as opisthokonts. A synapomorphy that distinguishes fungi from other two opisthokonts is the presence of chitin in their cell walls.[177]
Most fungi are multicellular but some are unicellular such as yeasts, which live in liquid or moist environments and are able to absorb nutrients directly into their cell surfaces.[177] Multicellular fungi, on the other hand, have a body called mycelium, which is composed of a mass of individual tubular filaments called hyphae that allows for nutrient absorption to occur.[177]
Fungi can be divided into six major groups based on their life cycles: microsporidia, chytrids, zygospore fungi (Zygomycota), arbuscular mycorrhizal fungi (Glomeromycota), sac fungi (Ascomycota), and club fungi (Basidiomycota).[177]
The fungus kingdom encompasses an enormous diversity of taxa with varied ecologies, life cycle strategies, and morphologies ranging from unicellular aquatic chytrids to large mushrooms. However, little is known of the true biodiversity of Kingdom Fungi, which has been estimated at 2.2million to 3.8million species.[178] Of these, only about 148,000 have been described,[179] with over 8,000 species known to be detrimental to plants and at least 300 that can be pathogenic to humans.[180]
Animals are multicellular eukaryotic organisms that form the kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been describedof which around 1 million are insectsbut it has been estimated there are over 7 million animal species in total. They have complex interactions with each other and their environments, forming intricate food webs. 
Animals can be distinguished into two groups based on their developmental characteristics.[181] For instance, embryos of diploblastic animals such as ctenophores, placeozoans, and cnidarians have two cell layers (ectoderm and endoderm) whereas the embryos of triploblastic animals have three tissue layers (ectoderm, mesoderm, and endoderm), which is a synapomorphy of these animals.[181] Triploblastic animals can be further divided into two major clades based on based on the pattern of gastrulation, whereby a cavity called a blastopore is formed from the indentation of a blastula. In protostomes, the blastopore gives rise to the mouth, which is then followed by the formation of the anus.[181] In deuterostomes, the blastopore gives rise to the anus, followed by the formation of the mouth.[181]
Animals can also be differentiated based on their body plan, specifically with respect to four key features: symmetry, body cavity, segmentation, and appendages.[181] The bodies of most animals are symmetrical, with symmetry being either radial or bilateral.[181] Triploblastic animals can be divided into three types based on their body cavity: acoelomate, pseudocoelomate, and coelomate.[181] Segmentation can be observed in the bodies of many animals, which allows for specialization of different parts of the body as well as allowing the animal to change the shape of its body to control its movements.[181] Finally, animals can be distinguished based on the type and location of their appendages such as antennae for sensing the environment or claws for capturing prey.[181]
Sponges, the members of the phylum Porifera, are a basal Metazoa (animal) clade as a sister of the diploblasts.[182][183][184][185][186] They are multicellular organisms that have bodies full of pores and channels allowing water to circulate through them, consisting of jelly-like mesohyl sandwiched between two thin layers of cells.
The majority (~97%) of animal species are invertebrates,[187] which are animals that do not have a vertebral column (or backbone or spine), derived from the notochord. This includes all animals apart from the subphylum Vertebrata. Familiar examples of invertebrates include sponges, cnidarians (hydras, jellyfishes, sea anemones, and corals), mollusks (chitons, snail, bivalves, squids, and octopuses), annelids (earthworms and leeches), and arthropods (insects, arachnids, crustaceans, and myriapods). Many invertebrate taxa have a greater number and variety of species than the entire subphylum of Vertebrata.[188]
In contrast, vertebrates comprise all species of animals within the subphylum Vertebrata, which are chordates with vertebral columns. These animals have four key features, which are an anterior skull with a brain, a rigid internal skeleton supported by a vertebral column that encloses a spinal cord, internal organs suspended in a coelom, and a well-developed circulatory system driven by a single large heart.[181] Vertebrates represent the overwhelming majority of the phylum Chordata, with currently about 69,963 species described.[189] Vertebrates comprise different major groups that include jawless fishes (not including hagfishes), jawed vertebrates such as cartilaginous fishes (sharks, rays, and ratfish), bony fishes, tetrapods such as amphibians, reptiles, birds, and mammals.[181]
The two remaining groups of jawless fishes that have survived beyond the Devonian period are hagfishes and lamprey, which are collectively known as cyclostomes (for circled mouths).[181] Both groups of animals have elongated eel-like bodies with no paired fins.[181] However, because hagfishes have a weak circulatory system with three accessory hearts, a partial skull with no cerebellum, no jaws or stomach, and no jointed vertebrae, some biologists do not classify them as vertebrates but instead as a sister group of vertebrates.[181] In contrast, lampreys have a complete skull and a distinct vertebrae that is cartilaginous.[181]
Mammals have four key features that distinguish them from other animals such as sweat glands, mammary glands, hair, and a four-chambered heart.[181] Small and medium-sized mammals used to co-exist with large dinosaurs in much of the Mesozoic era but soon radiated following the mass extinction of dinosaurs at the end of the Cretaceous period.[181] There are approximately 57,000 mammal species, which can be divided into two primary groups: prototherians and therians. Prototherians do not possess nipples on their mammary but instead secrete milk onto their skin, allowing their offspring to lap if off their furs.[181] They also lack a placenta, lays eggs, and have sprawling legs. Currently, there only five known species of prototherians (platypus and four species of echidnas).[181] The therian clade is viviparous and can be further divided into two groups: marsupials and eutherians.[181] Marsupial females have a ventral pouch to carry and feed their offspring. Eutherians form the majority of mammals and include major groups such as rodents, bats, even-toed ungulates and cetaceans, shrews and moles, primates, carnivores, rabbits, African insectivores, spiny insectivores, armadillos, treeshrews, odd-toed ungulates, long-nosed insectivores, anteaters and sloths, pangolins, hyraxes, sirenians, elephants, colugos, and aardvark.[181]
Viruses are submicroscopic infectious agents that replicate inside the cells of organisms.[190] Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea.[191][192] More than 6,000 virus species have been described in detail.[193] Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.[194][195]
When infected, a host cell is forced to rapidly produce thousands of identical copies of the original virus. When not inside an infected cell or in the process of infecting a cell, viruses exist in the form of independent particles, or virions, consisting of the genetic material (DNA or RNA), a protein coat called capsid, and in some cases an outside envelope of lipids. The shapes of these virus particles range from simple helical and icosahedral forms to more complex structures. Most virus species have virions too small to be seen with an optical microscope, as they are one-hundredth the size of most bacteria.
The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmidspieces of DNA that can move between cellswhile others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction.[196] Because viruses possess some but not all characteristics of life, they have been described as "organisms at the edge of life",[197] and as self-replicators.[198]
Viruses can spread in many ways. One transmission pathway is through disease-bearing organisms known as vectors: for example, viruses are often transmitted from plant to plant by insects that feed on plant sap, such as aphids; and viruses in animals can be carried by blood-sucking insects. Influenza viruses are spread by coughing and sneezing. Norovirus and rotavirus, common causes of viral gastroenteritis, are transmitted by the faecaloral route, passed by hand-to-mouth contact or in food or water. Viral infections in animals provoke an immune response that usually eliminates the infecting virus. Immune responses can also be produced by vaccines, which confer an artificially acquired immunity to the specific viral infection.
The plant body is made up of  organs that can be organized into two major organ systems: a  root system and a  shoot system.[199] The root system anchors the plants into place. The roots themselves absorb water and minerals and store photosynthetic products. The shoot system is composed of stem, leaves, and flowers. The stems hold and orient the leaves to the sun, which allow the leaves to conduct photosynthesis. The flowers are shoots that have been modified for reproduction. Shoots are composed of phytomers, which are functional units that consist of a node carrying one or more leaves, internode, and one or more buds. 
A plant body has two basic patterns (apicalbasal and radial axes) that been established during  embryogenesis.[199] Cells and tissues are arranged along the apical-basal axis from root to shoot whereas the three tissue systems (dermal, ground, and vascular) that make up a plant's body are arranged concentrically around its radial axis.[199] The dermal tissue system forms the epidermis (or outer covering) of a plant, which is usually a single cell layer that consists of cells that have differentiated into three specialized structures: stomata for gas exchange in leaves, trichomes (or leaf hair) for protection against insects and solar radiation, and root hairs for increased surface areas and absorption of water and nutrients. The ground tissue makes up virtually all the tissue that lies between the dermal and vascular tissues in the shoots and roots. It consists of three cell types: Parenchyma, collenchyma, and sclerenchyma cells. Finally, the vascular tissues are made up of two constituent tissues: xylem and phloem. The xylem is made up of two conducting cells called tracheids and vessel elements whereas the phloem is characterized by the presence of sieve tube elements and companion cells.[199]
Like all other organisms, plants are primarily made up of water and other molecules containing  elements that are essential to life.[200] The absence of specific nutrients (or essential elements), many of which have been identified in hydroponic experiments, can disrupt  plant growth and reproduction. The majority of plants are able to obtain these nutrients from solutions that surrounds their roots in the soil.[200] Continuous leaching and  harvesting of crops can deplete the soil of its nutrients, which can be restored with the use of fertilizers. Carnivorous plants such as Venus flytraps are able to obtain nutrients by digesting other arthropods whereas parasitic plants such as mistletoes can parasitize other plants for water and nutrients. 
Plants need water to conduct photosynthesis, transport solutes between organs, cool their leaves by evaporation, and maintain internal pressures that support their bodies.[200] Water is able to  diffuse in and out of plant cells by osmosis. The direction of water movement across a semipermeable membrane is determined by the water potential across that membrane.[200] Water is able to diffuse across a root cell's membrane through aquaporins whereas solutes are transported across by the membrane by ion channels and pumps. In vascular plants, water and solutes are able to enter the xylem, a vascular tissue, by way of an apoplast and symplast. Once in the xylem, the water and minerals are distributed upward by transpiration from the soil to the aerial parts of the plant.[174][200] In contrast, the phloem, another vascular tissue, distributes carbohydrates (e.g., sucrose) and other solutes such as hormones by  translocation from a source (e.g., mature leaf or root) in which they were produced to a sink (e.g., root, flower, or developing fruit) in which they will be used and stored.[200] Sources and sinks can switch roles, depending on the amount of carbohydrates accumulated or mobilized for the nourishment of other organs.
Plant development is regulated by environmental cues and the plant's own receptors, hormones, and genome.[201] Morever, they have several characteristics that allow them to obtain resources for growth and reproduction such as meristems, post-embryonic organ formation, and differential growth.
Development begins with a seed, which is an embryonic plant enclosed in a protective outer covering. Most plant seeds are usually dormant, a condition in which the seed's normal activity is suspended.[201] Seed dormancy may last may last weeks, months, years, and even centuries. Dormancy is broken once conditions are favorable for growth, and the seed will begin to sprout, a process called germination. Imbibition is the first step in germination, whereby water is absorbed by the seed. Once water is absorbed, the seed undergoes metabolic changes whereby enzymes are activated and RNA and proteins are synthesized. Once the seed germinates, it obtains carbohydrates, amino acids, and small lipids that serve as building blocks for its development. These monomers are obtained from the hydrolysis of starch, proteins, and lipids that are stored in either the cotyledons or endosperm. Germination is completed once embryonic roots called radicle have emerged from the seed coat. At this point, the developing plant is called a seedling and its growth is regulated by its own photoreceptor proteins and hormones.[201]
Unlike animals in which growth is determinate, i.e., ceases when the adult state is reached, plant growth is indeterminate as it is an open-ended process that could potentially be lifelong.[199] Plants grow in two ways:  primary and  secondary. In primary growth, the shoots and roots are formed and lengthened. The apical meristem produces the primary plant body, which can be found in all  seed plants. During secondary growth, the thickness of the plant increases as the  lateral meristem produces the secondary plant body, which can be found in woody eudicots such as trees and shrubs.  Monocots do not go through secondary growth.[199] The plant body is generated by a hierarchy of meristems. The apical meristems in the root and shoot systems give rise to primary meristems (protoderm, ground meristem, and procambium), which in turn, give rise to the three tissue systems (dermal, ground, and vascular).
Most angiosperms (or flowering plants) engage in sexual reproduction.[202] Their flowers are organs that facilitate reproduction, usually by providing a mechanism for the union of sperm with eggs. Flowers may facilitate two types of pollination: self-pollination and cross-pollination. Self-pollination occurs when the pollen from the anther is deposited on the stigma of the same flower, or another flower on the same plant. Cross-pollination is the transfer of pollen from the anther of one flower to the stigma of another flower on a different individual of the same species. Self-pollination happened in flowers where the stamen and carpel mature at the same time, and are positioned so that the pollen can land on the flower's stigma. This pollination does not require an investment from the plant to provide nectar and pollen as food for pollinators.[203]
Like animals, plants produce hormones in one part of its body to signal cells in another part to respond. The ripening of fruit and loss of leaves in the winter are controlled in part by the production of the gas ethylene by the plant. Stress from water loss, changes in air chemistry, or crowding by other plants can lead to changes in the way a plant functions. These changes may be affected by genetic, chemical, and physical factors.
To function and survive, plants produce a wide array of chemical compounds not found in other organisms. Because they cannot move, plants must also defend themselves chemically from herbivores, pathogens and competition from other plants. They do this by producing toxins and foul-tasting or smelling chemicals. Other compounds defend plants against disease, permit survival during drought, and prepare plants for dormancy, while other compounds are used to attract pollinators or herbivores to spread ripe seeds.
Many plant organs contain different types of  photoreceptor proteins, each of which reacts very specifically to certain wavelengths of light.[204] The photoreceptor proteins relay information such as whether it is day or night, duration of the day, intensity of light available, and the source of light. Shoots generally grow towards light, while roots grow away from it, responses known as phototropism and skototropism, respectively. They are brought about by light-sensitive pigments like phototropins and phytochromes and the plant hormone auxin.[205] Many flowering plants bloom at the appropriate time because of light-sensitive compounds that respond to the length of the night, a phenomenon known as photoperiodism.
In addition to light, plants can respond to other types of stimuli. For instance, plants can sense the direction of gravity to orient themselves correctly. They can respond to mechanical stimulation.[206]
The cells in each animal body are bathed in interstitial fluid, which make up the cell's environment. This fluid and all its characteristics (e.g., temperature, ionic composition) can be described as the animal's internal environment, which is in contrast to the external environment that encompasses the animal's outside world.[207] Animals can be classified as either regulators or conformers. Animals such as mammals and birds are regulators as they are able to maintain a constant internal environment such as body temperature despite their environments changing. These animals are also described as homeotherms as they exhibit thermoregulation by keeping their internal body temperature constant. In contrast, animals such as fishes and frogs are conformers as they adapt their internal environment (e.g., body temperature) to match their external environments. These animals are also described as poikilotherms or ectotherms as they allow their body temperatures to match their external environments. In terms of energy, regulation is more costly than conformity as an animal expands more energy to maintain a constant internal environment such as increasing its basal metabolic rate, which is the rate of energy consumption.[207] Similarly, homeothermy is more costly than poikilothermy. Homeostasis is the stability of an animal's internal environment, which is maintained by  negative feedback loops.[207][208]
The body size of terrestrial animals vary across different species but their use of energy does not scale linearly according to their size.[207] Mice, for example, are able to consume three times more food than rabbits in proportion to their weights as the basal metabolic rate per unit weight in mice is greater than in rabbits.[207] Physical activity can also increase an animal's metabolic rate. When an animal runs, its metabolic rate increases linearly with speed.[207] However, the relationship is non-linear in animals that swim or fly. When a fish swims faster, it encounters greater water resistance and so its metabolic rates increases exponential.[207] Alternatively, the relationship of flight speeds and metabolic rates is U-shaped in birds.[207] At low flight speeds, a bird must maintain a high metabolic rates to remain airborne. As it speeds up its flight, its metabolic rate decreases with the aid of air rapidly flows over its wings. However, as it increases in its speed even further, its high metabolic rates rises again due to the increased effort associated with rapid flight speeds. Basal metabolic rates can be measured based on an animal's rate of heat production.
An animal's body fluids have three properties: osmotic pressure,  ionic composition, and volume.[209] Osmotic pressures determine the direction of the diffusion of water (or osmosis), which moves from a region where osmotic pressure (total solute concentration) is low to a region where osmotic pressure (total solute concentration) is high. Aquatic animals are diverse with respect to their body fluid compositions and their environments. For example, most invertebrate animals in the ocean have body fluids that are  isosmotic with seawater. In contrast, ocean  bony fishes have body fluids that are  hyposmotic to seawater. Finally, freshwater animals have body fluids that are hyperosmotic to fresh water. Typical ions that can be found in an animal's body fluids are sodium, potassium, calcium, and chloride. The volume of body fluids can be regulated by excretion. Vertebrate animals have kidneys, which are excretory organs made up of tiny tubular structures called nephrons, which make urine from blood plasma. The kidneys' primary function is to regulate the composition and volume of blood plasma by selectively removing material from the blood plasma itself. The ability of  xeric animals such as kangaroo rats to minimize water loss by producing urine that is 10-20 times concentrated than their blood plasma allows them to adapt in desert environments that receive very little precipitation.[209]
Animals are heterotrophs as they feed on other organisms to obtain energy and organic compounds.[210] They are able to obtain food in three major ways such as targeting visible food objects, collecting tiny food particles, or depending on microbes for critical food needs. The amount of energy stored in food can be quantified based on the amount of heat (measured in calories or  kilojoules) emitted when the food is burnt in the presence of oxygen. If an animal were to consume food that contains an excess amount of chemical energy, it will store most of that energy in the form of lipids for future use and some of that energy as glycogen for more immediate use (e.g., meeting the brain's energy needs).[210] The molecules in food are chemical building blocks that are needed for growth and development. These molecules include nutrients such as carbohydrates, fats, and proteins. Vitamins and  minerals (e.g., calcium, magnesium, sodium, and phosphorus) are also essential. The digestive system, which typically consist of a tubular tract that extends from the mouth to the anus, is involved in the breakdown (or digestion) of food into small molecules as it travels down  peristaltically through the gut lumen shortly after it has been  ingested. These small food molecules are then  absorbed into the blood from the lumen, where they are then distributed to the rest of the body as building blocks (e.g., amino acids) or sources of energy (e.g., glucose).[210]
In addition to their digestive tracts, vertebrate animals have accessory glands such as a liver and pancreas as part of their digestive systems.[210] The processing of food in these animals begins in the foregut, which includes the mouth, esophagus, and stomach. Mechanical digestion of food starts in the mouth with the esophagus serving as a passageway for food to reach the stomach, where it is stored and disintegrated (by the stomach's acid) for further processing. Upon leaving the stomach, food enters into the midgut, which is the first part of the intestine (or small intestine in mammals) and is the principal site of digestion and absorption. Food that does not get absorbed are stored as indigestible waste (or feces) in the hindgut, which is the second part of the intestine (or large intestine in mammals). The hindgut then completes the reabsorption of needed water and salt prior to eliminating the feces from the rectum.[210]
The respiratory system consists of specific organs and structures used for gas exchange in animals. The anatomy and physiology that make this happen varies greatly, depending on the size of the organism, the environment in which it lives and its evolutionary history. In land animals the respiratory surface is internalized as linings of the lungs.[211] Gas exchange in the lungs occurs in millions of small air sacs; in mammals and reptiles these are called alveoli, and in birds they are known as atria. These microscopic air sacs have a very rich blood supply, thus bringing the air into close contact with the blood.[212] These air sacs communicate with the external environment via a system of airways, or hollow tubes, of which the largest is the trachea, which branches in the middle of the chest into the two main bronchi. These enter the lungs where they branch into progressively narrower secondary and tertiary bronchi that branch into numerous smaller tubes, the bronchioles. In birds the bronchioles are termed parabronchi. It is the bronchioles, or parabronchi that generally open into the microscopic alveoli in mammals and atria in birds. Air has to be pumped from the environment into the alveoli or atria by the process of breathing, which involves the muscles of respiration.
A circulatory system usually consists of a muscular pump such as a heart, a fluid (blood), and system of blood vessels that deliver it.[213][214] Its principal function is to transport blood and other substances to and from cell (biology)s and tissues. There are two types of circulatory systems: open and closed. In open circulatory systems, blood exits blood vessels as it circulates throughout the body whereas in closed circulatory system, blood is contained within the blood vessels as it circulates. Open circulatory systems can be observed in invertebrate animals such as arthropods (e.g., insects, spiders, and lobsters) whereas closed circulatory systems can be found in vertebrate animals such as fishes, amphibians, and mammals. Circulation in animals occur between two types of tissues: systemic tissues and breathing (or pulmonary) organs.[213] Systemic tissues are all the tissues and organs that make up an animal's body other than its breathing organs. Systemic tissues take up oxygen but adds carbon dioxide to the blood whereas a breathing organs takes up carbon dioxide but add oxygen to the blood.[215] In birds and mammals, the systemic and pulmonary systems are connected in series. 
In the circulatory system, blood is important because it is the means by which oxygen, carbon dioxide, nutrients, hormones, agents of immune system, heat, wastes, and other commodities are transported.[213] In annelids such as earthworms and leeches, blood is propelled by peristaltic waves of contractions of the heart muscles that make up the blood vessels. Other animals such as crustaceans (e.g., crayfish and lobsters), have more than one heart to propel blood throughout their bodies. Vertebrate hearts are multichambered and are able to pump blood when their  ventricles contract at each cardiac cycle, which propels blood through the blood vessels.[213] Although vertebrate hearts are myogenic, their rate of contraction (or heart rate) can be modulated by neural input from the body's autonomic nervous system.
In vertebrates, the muscular system consists of skeletal, smooth and cardiac muscles. It permits movement of the body, maintains posture and circulates blood throughout the body.[216] Together with the skeletal system, it forms the musculoskeletal system, which is responsible for the movement of vertebrate animals.[217] Skeletal muscle contractions are neurogenic as they require synaptic input from motor neurons. A single motor neuron is able to innervate multiple muscle fibers, thereby causing the fibers to contract at the same time. Once innervated, the protein filaments within each skeletal muscle fiber slide past each other to produce a contraction, which is explained by the sliding filament theory. The contraction produced can be described as a twitch, summation, or tetanus, depending on the frequency of action potentials. Unlike skeletal muscles, contractions of smooth and cardiac muscles are myogenic as they are initiated by the smooth or heart muscle cells themselves instead of a motor neuron. Nevertheless, the strength of their contractions can be modulated by input from the autonomic nervous system. The mechanisms of contraction are similar in all three muscle tissues.
In invertebrates such as earthworms and leeches, circular and longitudinal muscles cells form the body wall of these animals and are responsible for their movement.[218] In an earthworm that is moving through a soil, for example, contractions of circular and longitudinal muscles occur reciprocally while the coelomic fluid serves as a hydroskeleton by maintaining turgidity of the earthworm.[219] Other animals such as mollusks, and nematodes, possess obliquely striated muscles, which contain bands of thick and thin filaments that are arranged helically rather than transversely, like in vertebrate skeletal or cardiac muscles.[220] Advanced insects such as wasps, flies, bees, and beetles possess asynchronous muscles that constitute the flight muscles in these animals.[220] These flight muscles are often called fibrillar muscles because they contain myofibrils that are thick and conspicuous.[221]
Most multicellular animals have nervous systems[223] that allow them to sense from and respond to their environments. A nervous system is a network of cells that processes sensory information and generates behaviors. At the cellular level, the nervous system is defined by the presence of neurons, which are cells specialized to handle information.[224] They can transmit or receive information at sites of contacts called synapses.[224] More specifically, neurons can conduct nerve impulses (or action potentials) that travel along their thin fibers called axons, which can then be transmitted directly to a neighboring cell through electrical synapses or cause chemicals called neurotransmitters to be released at chemical synapses. According to the sodium theory, these action potentials can be generated by the increased permeability of the neuron's cell membrane to sodium ions.[225] Cells such as neurons or muscle cells may be excited or inhibited upon receiving a signal from another neuron. The connections between neurons can form neural pathways, neural circuits, and larger networks that generate an organism's perception of the world and determine its behavior. Along with neurons, the nervous system contains other specialized cells called glia or glial cells, which provide structural and metabolic support.
In vertebrates, the nervous system comprises the central nervous system (CNS), which includes the brain and spinal cord, and the peripheral nervous system (PNS), which consists of nerves that connect the CNS to every other part of the body. Nerves that transmit signals from the CNS are called motor nerves or efferent nerves, while those nerves that transmit information from the body to the CNS are called sensory nerves or afferent nerves. Spinal nerves are mixed nerves that serve both functions. The PNS is divided into three separate subsystems, the somatic, autonomic, and enteric nervous systems. Somatic nerves mediate voluntary movement. The autonomic nervous system is further subdivided into the sympathetic and the parasympathetic nervous systems. The sympathetic nervous system is activated in cases of emergencies to mobilize energy, while the parasympathetic nervous system is activated when organisms are in a relaxed state. The enteric nervous system functions to control the gastrointestinal system. Both autonomic and enteric nervous systems function involuntarily. Nerves that exit directly from the brain are called cranial nerves while those exiting from the spinal cord are called spinal nerves.
Many animals have sense organs that can detect their environment. These sense organs contain sensory receptors, which are sensory neurons that convert stimuli into electrical signals.[226] Mechanoreceptors, for example, which can be found in skin, muscle, and hearing organs, generate action potentials in response to changes in pressures.[226][227] Photoreceptor cells such as rods and cones, which are part of the vertebrate retina, can respond to specific  wavelengths of light.[226][227] Chemoreceptors detect chemicals in the mouth (taste) or in the air (smell).[227]
Hormones are signaling molecules transported in the blood to distant organs to regulate their function.[228][229] Hormones are secreted by internal glands that are part of an animal's endocrine system. In vertebrates, the hypothalamus is the neural control center for all endocrine systems. In humans specifically, the major endocrine glands are the thyroid gland and the adrenal glands. Many other organs that are part of other body systems have secondary endocrine functions, including bone, kidneys, liver, heart and gonads. For example, kidneys secrete the endocrine hormone erythropoietin. Hormones can be amino acid complexes, steroids, eicosanoids, leukotrienes, or prostaglandins.[230] The endocrine system can be contrasted to both exocrine glands, which secrete hormones to the outside of the body, and paracrine signaling between cells over a relatively short distance. Endocrine glands have no ducts, are vascular, and commonly have intracellular vacuoles or granules that store their hormones. In contrast, exocrine glands, such as salivary glands, sweat glands, and glands within the gastrointestinal tract, tend to be much less vascular and have ducts or a hollow lumen.

Animals can reproduce in one of two ways: asexual and sexual. Nearly all animals engage in some form of sexual reproduction.[231] They produce haploid gametes by meiosis. The smaller, motile gametes are spermatozoa and the larger, non-motile gametes are ova.[232] These fuse to form zygotes,[233] which develop via mitosis into a hollow sphere, called a blastula. In sponges, blastula larvae swim to a new location, attach to the seabed, and develop into a new sponge.[234] In most other groups, the blastula  undergoes more complicated rearrangement.[235] It first invaginates to form a gastrula with a digestive chamber and two separate germ layers, an external ectoderm and an internal endoderm.[236] In most cases, a third germ layer, the mesoderm, also develops between them.[237] These germ layers then differentiate to form tissues and organs.[238] Some animals are capable of asexual reproduction, which often results in a genetic clone of the parent. This may take place through fragmentation; budding, such as in Hydra and other cnidarians; or parthenogenesis, where fertile eggs are produced without mating, such as in aphids.[239][240]Animal development begins with the formation of a zygote that results from the fusion of a sperm and egg during fertilization.[241] The zygote undergoes a rapid multiple rounds of mitotic cell period of cell divisions called cleavage, which forms a ball of similar cells called a blastula. Gastrulation occurs, whereby morphogenetic movements convert the cell mass into a three germ layers that comprise the ectoderm, mesoderm and endoderm. 
The end of gastrulation signals the beginning of organogenesis, whereby the three germ layers form the internal organs of the organism.[242] The cells of each of the three germ layers undergo differentiation, a process where less-specialized cells become more-specialized through the expression of a specific set of genes. Cellular differentiation is influenced by extracellular signals such as growth factors that are exchanged to adjacent cells, which is called juxtracrine signaling, or to neighboring cells over short distances, which is called paracrine signaling.[243][244] Intracellular signals consist of a cell signaling itself (autocrine signaling), also play a role in organ formation. These signaling pathways allows for cell rearrangement and ensures that organs form at specific sites within the organism.[242][245]
The immune system is a network of biological processes that detects and responds to a wide variety of pathogens. Many species have two major subsystems of the immune system. The innate immune system provides a preconfigured response to broad groups of situations and stimuli. The adaptive immune system provides a tailored response to each stimulus by learning to recognize molecules it has previously encountered. Both use molecules and cells to perform their functions.
Nearly all organisms have some kind of immune system. Bacteria have a rudimentary immune system in the form of enzymes that protect against virus infections. Other basic immune mechanisms evolved in ancient plants and animals and remain in their modern descendants. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt to recognize pathogens more efficiently. Adaptive (or acquired) immunity creates an immunological memory leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.
Behaviors play a central a role in animals' interaction with each other and with their environment.[246] They are able to use their muscles to approach one another, vocalize, seek shelter, and migrate. An animal's nervous system activates and coordinates its behaviors. Fixed action patterns, for instance, are genetically determined and stereotyped behaviors that occur without learning.[246][247] These behaviors are under the control of the nervous system and can be quite elaborate.[246] Examples include the pecking of kelp gull chicks at the red dot on their mother's beak. Other behaviors that have emerged as a result of natural selection include foraging, mating, and altruism.[248] In addition to evolved behavior, animals have evolved the ability to learn by modifying their behaviors as a result of early individual experiences.[246]
Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment.[249] The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem.[250][251][252] These biotic and abiotic components are linked together through nutrient cycles and energy flows.[253] Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals play an important role in the movement of matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.[254]
The Earth's physical environment is shaped by solar energy and topography.[252] The amount of solar energy input varies in space and time due to the spherical shape of the Earth and its axial tilt. Variation in solar energy input drives weather and climate patterns. Weather is the day-to-day temperature and precipitation activity, whereas climate is the long-term average of weather, typically averaged over a period of 30 years.[255][256] Variation in topography also produces environmental heterogeneity. On the windward side of a mountain, for example, air rises and cools, with water changing from gaseous to liquid or solid form, resulting in precipitation such as rain or snow.[252] As a result, wet environments allow for lush vegetation to grow. In contrast, conditions tend to be dry on the leeward side of a mountain due to the lack of precipitation as air descends and warms, and moisture remains as water vapor in the atmosphere. Temperature and precipitation are the main factors that shape terrestrial biomes.
A population is the number of organisms of the same species that occupy an area and reproduce from generation to generation.[257][258][259][260][261] Its abundance can be measured using population density, which is the number of individuals per unit area (e.g., land or tree) or volume (e.g., sea or air).[257] Given that it is usually impractical to count every individual within a large population to determine its size, population size can be estimated by multiplying population density by the area or volume. Population growth during short-term intervals can be determined using the population growth rate equation, which takes into consideration birth, death, and immigration rates. In the longer term, the exponential growth of a population tends to slow down as it reaches its carrying capacity, which can be modeled using the logistic equation.[258] The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available.[262] The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability resources and the cost of maintaining them. In human populations, new  technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the famous of which was by  Thomas Malthus in the 18th century.[257]
A community is a group of populations of two or more different species occupying the same geographical area at the same time. A biological interaction is the effect that a pair of organisms living together in a community have on each other. They can be either of the same species (intraspecific interactions), or of different species (interspecific interactions). These effects may be short-term, like pollination and predation, or long-term; both often strongly influence the evolution of the species involved. A long-term interaction is called a symbiosis. Symbioses range from mutualism, beneficial to both partners, to competition, harmful to both partners.[264]
Every species participates as a consumer, resource, or both in consumerresource interactions, which form the core of food chains or food webs.[265] There are different trophic levels within any food web, with the lowest level being the primary producers (or autotrophs) such as plants and algae that convert energy and inorganic material into organic compounds, which can then be used by the rest of the community.[62][266][267] At the next level are the heterotrophs, which are the species that obtain energy by breaking apart organic compounds from other organisms.[265] Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous heterotrophs are able to consume at multiple levels. Finally, there are decomposers that feed on the waste products or dead bodies of organisms.[265]
On average, the total amount of energy incorporated into the biomass of a trophic level per unit of time is about one-tenth of the energy of the trophic level that it consumes. Waste and dead material used by decomposers as well as heat lost from metabolism make up the other ninety percent of energy that is not consumed by the next trophic level.[268]
In the global ecosystem (or biosphere), matter exist as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations.[270] For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific  elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water. In some cycles there are reservoirs where a substance remains or is sequestered for a long period of time.
Climate change includes both global warming driven by human-induced emissions of greenhouse gases and the resulting large-scale shifts in weather patterns. Though there have been previous periods of climatic change, since the mid-20th century humans have had an unprecedented impact on Earth's climate system and caused change on a global scale.[271] The largest driver of warming is the emission of greenhouse gases, of which more than 90% are carbon dioxide and methane.[272] Fossil fuel burning (coal, oil, and natural gas) for energy consumption is the main source of these emissions, with additional contributions from agriculture, deforestation, and manufacturing.[273] Temperature rise is accelerated or tempered by climate feedbacks, such as loss of sunlight-reflecting snow and ice cover, increased water vapor (a greenhouse gas itself), and changes to land and ocean carbon sinks.
Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions.[274][275][276] It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity.[277][278][279][280] The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years,[281] which has contributed to poverty, starvation, and will reset the course of evolution on this planet.[282][283] Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend.
Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.[284][277][278][279]
Journal links



The cell (from Latin  cellula'small room'[1]) is the basic structural and functional unit of life. Every cell consists of a cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids.[2]
Most plant and animal cells are only visible under a light microscope, with dimensions between 1 and 100micrometres.[3] Electron microscopy gives a much higher resolution showing greatly detailed cell structure. Organisms can be classified as unicellular (consisting of a single cell such as bacteria) or multicellular (including plants and animals).[4] Most unicellular organisms are classed as microorganisms. The number of cells in plants and animals varies from species to species; it has been approximated that the human body contains roughly 40 trillion (41013) cells.[a][5] The brain accounts for around 80 billion of these cells.[6]
Cell biology is the study of cells, which were discovered by Robert Hooke in 1665, who named them for their resemblance to cells inhabited by Christian monks in a monastery.[7][8] Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, and that all cells come from pre-existing cells.[9] Cells emerged on Earth about 4 billion years ago.[10][11][12][13]
Cells are of two types: eukaryotic, which contain a nucleus, and prokaryotic cells, which do not have a nucleus, but a nucleoid region is still present. Prokaryotes are single-celled organisms, while eukaryotes can be either single-celled or multicellular.[14]
Prokaryotes include bacteria and archaea, two of the three domains of life. Prokaryotic cells were the first form of life on Earth, characterized by having vital biological processes including cell signaling. They are simpler and smaller than eukaryotic cells, and lack a nucleus, and other membrane-bound organelles. The DNA of a prokaryotic cell consists of a single circular chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most prokaryotes are the smallest of all organisms ranging from 0.5 to 2.0m in diameter.[15]
A prokaryotic cell has three regions:
Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound organelles (compartments) in which specific activities take place. Most important among these is a cell nucleus,[4] an organelle that houses the cell's DNA. This nucleus gives the eukaryote its name, which means "true kernel (nucleus)". Some of the other differences are:
All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, the cytoplasm takes up most of the cell's volume. All cells (except red blood cells which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin) possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary cellular components, then briefly describes their function.
The cell membrane, or plasma membrane, is a selectively permeable[21] biological membrane that surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a macromolecular structure called the porosome the universal secretory portal in cells and a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell.[4] The membrane is semi-permeable, and selectively permeable, in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.
The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microtubules, intermediate filaments and microfilaments. In the cytoskeleton of a neuron the intermediate filaments are known as neurofilaments. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments.[4] The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis.[22] The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits vary among the cell types in different tissues. But some of the subunit protein of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), neurofilament proteins (NFL, NFM).
Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence.[4] RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.
Prokaryotic genetic material is organized in a simple circular bacterial chromosome in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different,[4] linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).
A human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from the nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes,[4] it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.
Foreign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.
Organelles are parts of the cell which are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function).[4] Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.
There are several types of organelles in a cell. Some (such as the nucleus and golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.
Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the semipermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.
Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of cellulose, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.
A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as Bacillus anthracis or hyaluronic acid as in streptococci.
Capsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.[24]:87
Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. A different type of flagellum is found in archaea and a different type is found in eukaryotes.
A fimbria (plural fimbriae also known as a pilus, plural pili) is a short, thin, hair-like filament found on the surface of bacteria. Fimbriae are formed of a protein called pilin (antigenic) and are responsible for the attachment of bacteria to specific receptors on human cells (cell adhesion). There are special types of pili involved in bacterial conjugation.
Cell division involves a single cell (called a mother cell) dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.
DNA replication, or the process of duplicating a cell's genome,[4] always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.
In meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II.[25] Replication, like all cellular activities, requires specialized proteins for carrying out the job.[4]
In general, cells of all organisms contain enzyme systems that scan their DNA for damages and carry out repair processes when damages are detected.[26] Diverse repair processes have evolved in organisms ranging from bacteria to humans. The widespread prevalence of these repair processes indicates the importance of maintaining cellular DNA in an undamaged state in order to avoid cell death or errors of replication due to damages that could lead to mutation. E. coli bacteria are a well-studied example of a cellular organism with diverse well-defined DNA repair processes. These include: (1) nucleotide excision repair, (2) DNA mismatch repair, (3) non-homologous end joining of double-strand breaks, (4) recombinational repair and (5) light-dependent repair (photoreactivation).
Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.
Complex sugars consumed by the organism can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP),[4] a molecule that possesses readily available energy, through two different pathways.
Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.
Transcription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.
Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.
In multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins.[27] The process is divided into three steps  protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.[28][29]
In August 2020, scientists described one way cells  in particular cells of a slime mold and mouse pancreatic cancerderived cells  are able to navigate efficiently through a body and identify the best routes through complex mazes: generating gradients after breaking down diffused chemoattractants which enable them to sense upcoming maze junctions before reaching them, including around corners.[30][31][32]
Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.[33]
In complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but of different cell type due to the differential expression of the genes they contain.
Most distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cellcell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).
Multicellularity has evolved independently at least 25 times,[34] including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, Magnetoglobus multicellularis or Methanosarcina. However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants.[35] It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae.[36] Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships.
The first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago.[34] Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.[37]
The evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.[34]
The origin of cells has to do with the origin of life, which began the history of life on Earth.
There are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see MillerUrey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.[38]
Cells emerged at least 3.5 billion years ago.[10][11][12] The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.[39]
The eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing proteobacteria and cyanobacteria, respectively, which were endosymbiosed by an ancestral archaean prokaryote.
There is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.


In biology, a kingdom (Latin: regnum, plural regna) is the second highest taxonomic rank, just below domain. Kingdoms are divided into smaller groups called phyla. Traditionally, some textbooks from the United States and Canada used a system of six kingdoms (Animalia, Plantae, Fungi, Protista, Archaea/Archaebacteria, and Bacteria/Eubacteria) while textbooks in Great Britain, India, Greece, Brazil and other countries use five kingdoms only (Animalia, Plantae, Fungi, Protista and Monera). Some recent classifications based on modern cladistics have explicitly abandoned the term kingdom, noting that the traditional kingdoms are not monophyletic, meaning that they do not consist of all the descendants of a common ancestor. The informal terms flora (plants), fauna (animals), and, in the 21st century, funga are also used.
When Carl Linnaeus introduced the rank-based system of nomenclature into biology in 1735, the highest rank was given the name "kingdom" and was followed by four other main or principal ranks: class, order, genus and species.[1] Later two further main ranks were introduced, making the sequence kingdom, phylum or division, class, order, family, genus and species.[2]  In 1990, the rank of domain was introduced above kingdom.[3]
Prefixes can be added so  subkingdom (subregnum) and infrakingdom (also known as infraregnum) are the two ranks immediately below kingdom. Superkingdom may be considered as an equivalent of domain or empire or as an independent rank between kingdom and domain or subdomain. In some classification systems the additional rank branch (Latin: ramus) can be inserted between subkingdom and infrakingdom, e.g., Protostomia and Deuterostomia in the classification of Cavalier-Smith.[4]

The classification of living things into animals and plants is an ancient one. Aristotle (384322 BC) classified animal species in his History of Animals, while his pupil Theophrastus (c. 371c. 287 BC) wrote a parallel work, the Historia Plantarum, on plants.[5]
Carl Linnaeus (17071778) laid the foundations for modern biological nomenclature, now regulated by the Nomenclature Codes, in 1735. He distinguished two kingdoms of living things: Regnum Animale ('animal kingdom') and Regnum Vegetabile ('vegetable kingdom', for plants). Linnaeus also included minerals in his classification system, placing them in a third kingdom, Regnum Lapideum.
Regnum Animale (animals)
Regnum Vegetabile ('vegetables'/plants)
Regnum Lapideum (minerals)

In 1674, Antonie van Leeuwenhoek, often called the "father of microscopy", sent the Royal Society of London a copy of his first observations of microscopic single-celled organisms. Until then, the existence of such microscopic organisms was entirely unknown. Despite this, Linnaeus did not include any microscopic creatures in his original taxonomy.
At first, microscopic organisms were classified within the animal and plant kingdoms. However, by the mid19th century, it had become clear to many that "the existing dichotomy of the plant and animal kingdoms [had become] rapidly blurred at its boundaries and outmoded".[6]
In 1860 John Hogg proposed the Protoctista, a third kingdom of life composed of all the lower creatures, or the primary organic beings"; he retained Regnum Lapideum as a fourth kingdom of minerals.[6] In 1866, Ernst Haeckel also proposed a third kingdom of life, the Protista, for "neutral organisms" or "the kingdom of primitive forms", which were neither animal nor plant; he did not include the Regnum Lapideum in his scheme.[6] Haeckel revised the content of this kingdom a number of times before settling on a division based on whether organisms were unicellular (Protista) or multicellular (animals and plants).[6]
Kingdom Protista or Protoctista
Kingdom Plantae
Kingdom Animalia
Regnum Lapideum (minerals)
The development of microscopy revealed important distinctions between those organisms whose cells do not have a distinct nucleus (prokaryotes) and organisms whose cells do have a distinct nucleus (eukaryotes). In 1937 douard Chatton introduced the terms "prokaryote" and "eukaryote" to differentiate these organisms.[7]
In 1938, Herbert F. Copeland proposed a four-kingdom classification by creating the novel Kingdom Monera of prokaryotic organisms; as a revised phylum Monera of the Protista, it included organisms now classified as Bacteria and Archaea. Ernst Haeckel, in his 1904 book The Wonders of Life, had placed the blue-green algae (or Phycochromacea) in Monera; this would gradually gain acceptance, and the blue-green algae would become classified as bacteria in the phylum Cyanobacteria.[6][7]
In the 1960s, Roger Stanier and C. B. van Niel promoted and popularized douard Chatton's earlier work, particularly in their paper of 1962, "The Concept of a Bacterium"; this created, for the first time, a rank above kingdoma superkingdom or empirewith the two-empire system of prokaryotes and eukaryotes.[7] The two-empire system would later be expanded to the three-domain system of Archaea, Bacteria, and Eukaryota.[8]
Kingdom Monera
Kingdom Protista or Protoctista
Kingdom Plantae
Kingdom Animalia

The differences between fungi and other organisms regarded as plants had long been recognised by some; Haeckel had moved the fungi out of Plantae into Protista after his original classification,[6] but was largely ignored in this separation by scientists of his time. Robert Whittaker recognized an additional kingdom for the Fungi. The resulting five-kingdom system, proposed in 1969 by Whittaker, has become a popular standard and with some refinement is still used in many works and forms the basis for new multi-kingdom systems. It is based mainly upon differences in nutrition; his Plantae were mostly multicellular autotrophs, his Animalia multicellular heterotrophs, and his Fungi multicellular saprotrophs.
The remaining two kingdoms, Protista and Monera, included unicellular and simple cellular colonies.[9] The five kingdom system may be combined with the two empire system. In the Whittaker system, Plantae included some algae. In other systems, such as Lynn Margulis's system of five kingdoms, the plants included just the land plants (Embryophyta), and Protoctista has a broader definition.[10]
Following publication of Whittaker's system, the five-kingdom model began to be commonly used in high school biology textbooks.[11] But despite the development from two kingdoms to five among most scientists, some authors as late as 1975 continued to employ a traditional two-kingdom system of animals and plants, dividing the plant kingdom into subkingdoms Prokaryota (bacteria and cyanobacteria), Mycota (fungi and supposed relatives), and Chlorota (algae and land plants).[12]
Kingdom Monera
Kingdom Protista or Protoctista
Kingdom Plantae
Kingdom Fungi
Kingdom Animalia
In 1977, Carl Woese and colleagues proposed the fundamental subdivision of the prokaryotes into the Eubacteria (later called the Bacteria) and Archaebacteria (later called the Archaea), based on ribosomal RNA structure;[13] this would later lead to the proposal of three "domains" of life, of Bacteria, Archaea, and Eukaryota.[3] Combined with the five-kingdom model, this created a six-kingdom model, where the kingdom Monera is replaced by the kingdoms Bacteria and Archaea.[14] This six-kingdom model is commonly used in recent US high school biology textbooks, but has received criticism for compromising the current scientific consensus.[11] But the division of prokaryotes into two kingdoms remains in use with the recent seven kingdoms scheme of Thomas Cavalier-Smith, although it primarily differs in that Protista is replaced by Protozoa and Chromista.[15]
Kingdom Eubacteria (Bacteria)
Kingdom Archaebacteria (Archaea)
Kingdom Protista or Protoctista
Kingdom Plantae
Kingdom Fungi
Kingdom Animalia
Thomas Cavalier-Smith supported the consensus at that time, that the difference between Eubacteria and Archaebacteria was so great (particularly considering the genetic distance of ribosomal genes) that the prokaryotes needed to be separated into two different kingdoms. He then divided Eubacteria into two subkingdoms: Negibacteria (Gram negative bacteria) and Posibacteria (Gram positive bacteria). Technological advances in electron microscopy allowed the separation of the Chromista from the Plantae kingdom. Indeed, the chloroplast of the chromists is located in the lumen of the endoplasmic reticulum instead of in the cytosol. Moreover, only chromists contain chlorophyll c. Since then, many non-photosynthetic phyla of protists, thought to have secondarily lost their chloroplasts, were integrated into the kingdom Chromista.
Finally, some protists lacking mitochondria were discovered.[16] As mitochondria were known to be the result of the endosymbiosis of a proteobacterium, it was thought that these amitochondriate eukaryotes were primitively so, marking an important step in eukaryogenesis. As a result, these amitochondriate protists were separated from the protist kingdom, giving rise to the, at the same time, superkingdom and kingdom Archezoa. This superkingdom was opposed to the Metakaryota superkingdom, grouping together the five other eukaryotic kingdoms (Animalia, Protozoa, Fungi, Plantae and Chromista). This was known as the Archezoa hypothesis, which has since been abandoned;[17] later schemes did not include the ArchezoaMetakaryota divide.[4][15]
Kingdom Eubacteria
Kingdom Archaebacteria
Kingdom Archezoa
Kingdom Protozoa
Kingdom Chromista
Kingdom Plantae
Kingdom Fungi
Kingdom Animalia
 No longer recognized by taxonomists.
In 1998, Cavalier-Smith published a six-kingdom model,[4] which has been revised in subsequent papers. The version published in 2009 is shown below.[18][a][19] Cavalier-Smith no longer accepted the importance of the fundamental EubacteriaArchaebacteria divide put forward by Woese and others and supported by recent research.[20] The kingdom Bacteria (sole kingdom of empire Prokaryota) was subdivided into two sub-kingdoms according to their membrane topologies: Unibacteria and Negibacteria. Unibacteria was divided into phyla Archaebacteria and Posibacteria; the bimembranous-unimembranous transition was thought to be far more fundamental than the long branch of genetic distance of Archaebacteria, viewed as having no particular biological significance.
Cavalier-Smith does not accept the requirement for taxa to be monophyletic ("holophyletic" in his terminology) to be valid. He defines Prokaryota, Bacteria, Negibacteria, Unibacteria, and Posibacteria as valid paraphyla (therefore "monophyletic" in the sense he uses this term) taxa, marking important innovations of biological significance (in regard of the concept of biological niche).
In the same way, his paraphyletic kingdom Protozoa includes the ancestors of Animalia, Fungi, Plantae, and Chromista. The advances of phylogenetic studies allowed Cavalier-Smith to realize that all the phyla thought to be archezoans (i.e. primitively amitochondriate eukaryotes) had in fact secondarily lost their mitochondria, typically by transforming them into new organelles: Hydrogenosomes. This means that all living eukaryotes are in fact metakaryotes, according to the significance of the term given by Cavalier-Smith. Some of the members of the defunct kingdom Archezoa, like the phylum Microsporidia, were reclassified into kingdom Fungi. Others were reclassified in kingdom Protozoa, like Metamonada which is now part of infrakingdom Excavata.
Because Cavalier-Smith allows paraphyly, the diagram below is an organization chart, not an ancestor chart, and does not represent an evolutionary tree.

Kingdom Bacteria  includes Archaebacteria as part of a subkingdom
Kingdom Protozoa  e.g. Amoebozoa, Choanozoa, Excavata
Kingdom Chromista  e.g. Alveolata, cryptophytes, Heterokonta (Brown Algae, Diatoms etc.), Haptophyta, Rhizaria
Kingdom Plantae  e.g. glaucophytes, red and green algae, land plants
Kingdom Fungi
Kingdom Animalia
Cavalier-Smith and his collaborators revised their classification in 2015. In this scheme they reintroduced the division of prokaryotes into two kingdoms, Bacteria (=Eubacteria) and Archaea (=Archaebacteria). This is based on the consensus in the Taxonomic Outline of Bacteria and Archaea (TOBA) and the Catalogue of Life.[15]
Kingdom Bacteria
Kingdom Archaea
Kingdom Protozoa  e.g. Amoebozoa, Choanozoa, Excavata
Kingdom Chromista  e.g. Alveolata, cryptophytes, Heterokonta (Brown Algae, Diatoms etc.), Haptophyta, Rhizaria
Kingdom Plantae  e.g. glaucophytes, red and green algae, land plants
Kingdom Fungi
Kingdom Animalia

The kingdom-level classification of life is still widely employed as a useful way of grouping organisms, notwithstanding some problems with this approach:

While the concept of kingdoms continues to be used by some taxonomists, there has been a movement away from traditional kingdoms, as they are no longer seen as providing a cladistic classification, where there is emphasis in arranging organisms into natural groups.[39]
From around the mid-1970s onwards, there was an increasing emphasis on comparisons of genes at the molecular level (initially ribosomal RNA genes) as the primary factor in classification; genetic similarity was stressed over outward appearances and behavior. Taxonomic ranks, including kingdoms, were to be groups of organisms with a common ancestor, whether monophyletic (all descendants of a common ancestor) or paraphyletic (only some descendants of a common ancestor).[citation needed]
Based on such RNA studies, Carl Woese thought life could be divided into three large divisions and referred to them as the "three primary kingdom" model or "urkingdom" model.[13] In 1990, the name "domain" was proposed for the highest rank.[3] This term represents a synonym for the category of dominion (lat. dominium), introduced by Moore in 1974.[40] Unlike Moore, Woese et al. (1990) did not suggest a Latin term for this category, which represents a further argument supporting the accurately introduced term dominion.[41]
Woese divided the prokaryotes (previously classified as the Kingdom Monera) into two groups, called Eubacteria and Archaebacteria, stressing that there was as much genetic difference between these two groups as between either of them and all eukaryotes.
Domain Bacteria/Eubacteria
Domain Archaea/Archaebacteria
Domain Eukarya/Eukaryota
According to genetic data, although eukaryote groups such as plants, fungi, and animals may look different, they are more closely related to each other than they are to either the Eubacteria or Archaea. It was also found that the eukaryotes are more closely related to the Archaea than they are to the Eubacteria. Although the primacy of the Eubacteria-Archaea divide has been questioned, it has been upheld by subsequent research.[20] There is no consensus on how many kingdoms exist in the classification scheme proposed by Woese.

In 2004, a review article by Simpson and Roger noted that the Protista were "a grab-bag for all eukaryotes that are not animals, plants or fungi". They held that only monophyletic groups should be accepted as formal ranks in a classification and that  while this approach had been impractical previously (necessitating "literally dozens of eukaryotic 'kingdoms'")  it had now become possible to divide the eukaryotes into "just a few major groups that are probably all monophyletic".[39]
On this basis, the diagram opposite (redrawn from their article) showed the real "kingdoms" (their quotation marks) of the eukaryotes.[39] A classification which followed this approach was produced in 2005 for the International Society of Protistologists, by a committee which "worked in collaboration with specialists from many societies". It divided the eukaryotes into the same six "supergroups".[42] The published classification deliberately did not use formal taxonomic ranks, including that of "kingdom".
Bacteria
Archaea
Excavata  Various flagellate protozoa
Amoebozoa  most lobose amoeboids and slime moulds
Opisthokonta  animals, fungi, choanoflagellates, etc.
Rhizaria  Foraminifera, Radiolaria, and various other amoeboid protozoa
Chromalveolata  Stramenopiles (Brown Algae, Diatoms etc.), Haptophyta, Cryptophyta (or cryptomonads), and Alveolata
Archaeplastida (or Primoplantae)  Land plants, green algae, red algae, and glaucophytes
In this system the multicellular animals (Metazoa) are descended from the same ancestor as both the unicellular choanoflagellates and the fungi which form the Opisthokonta.[42] Plants are thought to be more distantly related to animals and fungi.
However, in the same year as the International Society of Protistologists' classification was published (2005), doubts were being expressed as to whether some of these supergroups were monophyletic, particularly the Chromalveolata,[43] and a review in 2006 noted the lack of evidence for several of the six proposed supergroups.[44]
As of 2010[update], there is widespread agreement that the Rhizaria belong with the Stramenopiles and the Alveolata, in a clade dubbed the SAR supergroup,[45] so that Rhizaria is not one of the main eukaryote groups.[18][46][47][48][49] Beyond this, there does not appear to be a consensus. Rogozin et al. in 2009 noted that "The deep phylogeny of eukaryotes is an extremely difficult and controversial problem."[50] As of December2010[update], there appears to be a consensus that the six supergroup model proposed in 2005 does not reflect the true phylogeny of the eukaryotes and hence how they should be classified, although there is no agreement as to the model which should replace it.[46][47][51]
The International Committee on Taxonomy of Viruses uses the taxonomic rank "kingdom" for the classification of viruses (with the suffix -virae); but this is beneath the top level classifications of realm and subrealm.[52]
There is ongoing debate as to whether viruses can be included in the tree of life. The ten arguments against include the fact that they are obligate intracellular parasites that lack metabolism and are not capable of replication outside of a host cell.[53][54] Another argument is that their placement in the tree would be problematic, since it is suspected that viruses have arisen multiple times[citation needed], and they have a penchant for harvesting nucleotide sequences from their hosts.
On the other hand, arguments favor their inclusion.[55]
One comes from the discovery of unusually large and complex viruses, such as Mimivirus, that possess typical cellular genes.[56]
See also: Virus classification

Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems.[1] The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, ecology, and evolution.[2]
Computational biology is different from biological computing, which is a subfield of computer engineering using bioengineering and biology to build computers.
Computational biology, which includes many aspects of bioinformatics and much more, is the science of using biological data to develop algorithms or models in order to understand biological systems and relationships. 
Until recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.[3]
Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared among researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information.[3]
Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology.[4]
The terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, computational evolutionary biology is a subfield of it.[5]
Computational biology has been used to help sequence the human genome, create accurate models of the human brain, and assist in modeling biological systems.[3]
Computational anatomy is a discipline focusing on the study of anatomical shape and form at the visible or gross anatomical 



50

100



{\displaystyle 50-100\mu }

 
scale of morphology. It involves the development and application of computational, mathematical and data-analytical methods for modeling and simulation of biological structures. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D.
The original formulation of computational anatomy is as a generative model of shape and form from exemplars acted upon via transformations.[6] The diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow from one anatomical configuration in 






R



3




{\displaystyle {\mathbb {R} }^{3}}

 to another. It relates with shape statistics and morphometrics, with the distinction that diffeomorphisms are used to map coordinate systems, whose study is known as diffeomorphometry.
Computational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that maintain their state and functions against external and internal perturbations,[7] which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy.[7]
A useful modelling approach is to use Petri nets via tools such as esyN.[8]
Computational methods in ecology have seen increasing interest. Until recent decades, theoretical ecology has largely dealt with analytic models that were largely detached from the statistical models used by empirical ecologists. However, computational methods have aided in developing ecological theory via simulation of ecological systems, in addition to increasing application of methods from computational statistics in ecological analyses.
Computational biology has assisted the field of evolutionary biology in many capacities. This includes:
Computational genomics is a field within genomics which studies the genomes of cells and organisms. It is sometimes referred to as Computational and Statistical Genetics and encompasses much of Bioinformatics. The Human Genome Project is one example of computational genomics. This project looks to sequence the entire human genome into a set of data. Once fully implemented, this could allow for doctors to analyze the genome of an individual patient.[10]  This opens the possibility of personalized medicine, prescribing treatments based on an individual's pre-existing genetic patterns. This project has created many similar programs. Researchers are looking to sequence the genomes of animals, plants, bacteria, and all other types of life.[11]
One of the main ways that genomes are compared is by sequence homology. Homology is the study of biological structures and nucleotide sequences in different organisms that come from a common ancestor. Research suggests that between 80 and 90% of genes in newly sequenced prokaryotic genomes can be identified this way.[11]
This field is still in development. An untouched project in the development of computational genomics is the analysis of intergenic regions. Studies show that roughly 97% of the human genome consists of these regions.[11] Researchers in computational genomics are working on understanding the functions of non-coding regions of the human genome through the development of computational and statistical methods and via large consortia projects such as ENCODE (The Encyclopedia of DNA Elements) and the Roadmap Epigenomics Project.
Computational neuropsychiatry is the emerging field that uses mathematical and computer-assisted modeling of brain mechanisms involved in mental disorders. It was already demonstrated by several initiatives that computational modeling is an important contribution to understand neuronal circuits that could generate mental functions and dysfunctions.[12][13][14]
Computational neuroscience is the study of brain function in terms of the information processing properties of the structures that make up the nervous system. It is a subset of the field of neuroscience, and looks to analyze brain data to create practical applications.[15] It looks to model the brain in order to examine specific aspects of the neurological system. Various types of models of the brain include:
It is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations.
Computational oncology, sometimes also called cancer computational biology, is a field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data. Research in this field has led to the use of high-throughput measurement. High throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.[17][18]
Computational pharmacology (from a computational biology perspective) is the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data.[19] The pharmaceutical industry requires a shift in methods to analyze drug data. Pharmacologists were able to use Microsoft Excel to compare chemical and genomic data related to the effectiveness of drugs. However, the industry has reached what is referred to as the Excel barricade. This arises from the limited number of cells accessible on a spreadsheet. This development led to the need for computational pharmacology. Scientists and researchers develop computational methods to analyze these massive data sets. This allows for an efficient comparison between the notable data points and allows for more accurate drugs to be developed.[20]
Analysts project that if major medications fail due to patents, that computational biology will be necessary to replace current drugs on the market. Doctoral students in computational biology are being encouraged to pursue careers in industry rather than take Post-Doctoral positions. This is a direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs.[20]
Computational Biologists use a wide range of software. These range from command line programs to graphical and web-based programs.
Open source software provides a platform to develop computational biological methods. Specifically, open source means that every person and/or entity can access and benefit from software developed in research. PLOS cites four main reasons for the use of open source software including:
Standard modeling and scientific softwares used at the industrial scale also provide an excellent bedrock for rapid development in Computational Biology. These softwares similarly provide insulation from code errors by virtue of their active development by a large number of concerted experts in that field employed specifically for algorithm development. This similarly expedites in-house computational biology development by eliminating the need to code functionality from the ground up, yet ensures compatibility and ease of learning via unified syntax guidelines and documentation practices. The libraries used therein can be cross-validated as producing the same results as open-source projects (or better). Because the software is closed-source, there is little scrutiny from the public, and the lack of verifiable scientific data anywhere in the process is less apparent, allowing researchers to uphold the facade of ever-expanding knowledge within the sociocultural framework of the scientific research aesthetic.[22]
Common proprietary scientific computing software used for mathematics, statistics, data analysis, and numerical methods include Mathematica (the Wolfram Language), Matlab, SAS, etc.
There are several large conferences that are concerned with computational biology. Some notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB) and Research in Computational Molecular Biology (RECOMB).
There are numerous journals dedicated to computational biology. Some notable examples include Journal of Computational Biology and PLOS Computational Biology. The PLOS computational biology journal is a peer-reviewed open access journal that has many notable research projects in the field of computational biology. They provide reviews on software, tutorials for open source software, and display information on upcoming computational biology conferences.[citation needed]
Computational biology, bioinformatics and mathematical biology are all interdisciplinary approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science. The NIH describes computational/mathematical biology as the use of computational/mathematical approaches to address theoretical and experimental questions in biology and, by contrast, bioinformatics as the application of information science to understand complex life-sciences data.[1]
Specifically, the NIH defines
Computational biology: The development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.[1]Bioinformatics: Research, development, or application of computational tools and approaches for expanding the use of biological, medical, behavioral or health data, including those to acquire, store, organize, archive, analyze, or visualize such data.[1]While each field is distinct, there may be significant overlap at their interface.[1]

Molecular biology /mlkjlr/ is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.[1][2][3] Molecular biology was first described as an approach focused on the underpinnings of biological phenomena - uncovering the structures of biological molecules as well as their interactions, and how these interactions explain observations of classical biology.[4]
Molecular biology is not simply the study of biological molecules and their interactions; rather, it is also collection of techniques developed since the field's genesis which have enabled scientists to learn about molecular processes.[5] One notable technique which has revolutionized the field is the polymerase chain reaction (PCR), which was developed in 1983.[5] PCR is a reaction which amplifies small quantities of DNA, and it's used in many applications across scientific disciplines, as will be discussed later.[6][7]
The central dogma of molecular biology describes the process in which DNA is transcribed into RNA, which is then translated into protein.[2][8]
Molecular biology also plays a critical role in the understanding of structures, functions, and internal controls within individual cells, all of which can be used to efficiently target new drugs, diagnose disease, and better understand cell physiology.[9] Some clinical research and medical therapies arising from molecular biology are covered under gene therapy whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine.
While molecular biology was established as an official branch of science in the 1930s, the term wasn't coined until 1938 by Warren Weaver. At the time, Weaver was the director of Natural Sciences for the Rockefeller Foundation and believed that biology was about to undergo significant change due to recent advancements in technology such as X-ray crystallography.[10][11]
Molecular biology arose as an attempt to answer the questions regarding the mechanisms of genetic inheritance and the structure of a gene. In 1953, James Watson and Francis Crick published the double helical structure of DNA[12] courtesy of the X-ray crystallography work done by Rosalind Franklin and Maurice Wilkins. Watson and Crick described the structure of DNA and the interactions within the molecule. This publication jump-started research into molecular biology and increased interest in the subject.[13]
In 1961, it was demonstrated that when a gene encodes a protein, three sequential bases of a genes DNA specify each successive amino acid of the protein.[14]  Thus the genetic code is a triplet code, where each triplet (called a codon) specifies a particular amino acid.  Furthermore, it was shown that the codons do not overlap with each other in the DNA sequence encoding a protein, and that each sequence is read from a fixed starting point.
During 1962-1964, through the use of conditional lethal mutants of a bacterial virus[15], fundamental advances were made in our understanding of the functions and interactions of the proteins employed in the machinery of DNA replication, DNA repair,  DNA recombination, and in the assembly of molecular structures.
The following list describes a viewpoint on the interdisciplinary relationships between molecular biology and other related fields.[16]
While researchers practice techniques specific to molecular biology, it is common to combine these with methods from genetics and biochemistry. Much of molecular biology is quantitative, and recently a significant amount of work has been done using computer science techniques such as bioinformatics and computational biology. Molecular genetics, the study of gene structure and function, has been among the most prominent sub-fields of molecular biology since the early 2000s. Other branches of biology are informed by molecular biology, by either directly studying the interactions of molecules in their own right such as in cell biology and developmental biology, or indirectly, where molecular techniques are used to infer historical attributes of populations or species, as in fields in evolutionary biology such as population genetics and phylogenetics. There is also a long tradition of studying biomolecules "from the ground up", or molecularly, in biophysics.[19]
Molecular cloning is a recombinant DNA technology which was first developed in the 1960's.[20] In this technique, DNA coding for a protein of interest is cloned using polymerase chain reaction (PCR), and/or restriction enzymes into a plasmid (expression vector). A vector has 3 distinctive features: an origin of replication, a multiple cloning site (MCS), and a selective marker (usually antibiotic resistance). Upstream of the multiple cloning site are the promoter regions and the transcription start site, which regulate the expression of cloned gene.
This plasmid can be inserted into either bacterial or animal cells. Introducing DNA into bacterial cells can be done by transformation via uptake of naked DNA, conjugation via cell-cell contact or by transduction via viral vector. Introducing DNA into eukaryotic cells, such as animal cells, by physical or chemical means is called transfection. Several different transfection techniques are available, such as calcium phosphate transfection, electroporation, microinjection and liposome transfection. The plasmid may be integrated into the genome, resulting in a stable transfection, or may remain independent of the genome, called transient transfection.[21][22]
DNA coding for a protein of interest is now inside a cell, and the protein can now be expressed. A variety of systems, such as inducible promoters and specific cell-signaling factors, are available to help express the protein of interest at high levels. Large quantities of a protein can then be extracted from the bacterial or eukaryotic cell. The protein can be tested for enzymatic activity under a variety of situations, the protein may be crystallized so its tertiary structure can be studied, or, in the pharmaceutical industry, the activity of new drugs against the protein can be studied.[23]
Polymerase chain reaction (PCR) is an extremely versatile technique for copying DNA. In brief, PCR allows a specific DNA sequence to be copied or modified in predetermined ways. The reaction is extremely powerful and under perfect conditions could amplify one DNA molecule to become 1.07 billion molecules in less than two hours. PCR has many applications, including the study of gene expression, the detection of pathogenic microorganisms, the detection of genetic mutations, and the introduction of mutations to DNA.[24] The PCR technique can be used to introduce restriction enzyme sites to ends of DNA molecules, or to mutate particular bases of DNA, the latter is a method referred to as site-directed mutagenesis. PCR can also be used to determine whether a particular DNA fragment is found in a cDNA library. PCR has many variations, like reverse transcription PCR (RT-PCR) for amplification of RNA, and, more recently, quantitative PCR which allow for quantitative measurement of DNA or RNA molecules.[25][26]
Gel electrophoresis is one of the principal tools of molecular biology. The basic principle is that DNA, RNA, and proteins can all be separated by means of an electric field and size. In agarose gel electrophoresis, DNA and RNA can be separated on the basis of size by running the DNA through an electrically charged agarose gel. Proteins can be separated on the basis of size by using an SDS-PAGE gel, or on the basis of size and their electric charge by using what is known as a 2D gel electrophoresis.[27]
The Bradford Assay is a molecular biology technique which enables the fast, accurate quantitation of protein molecules utilizing the unique properties of a dye called Coomassie Brilliant Blue G-250.[28] Coomassie Blue undergoes a visible color shift from reddish-brown to bright blue upon binding to protein.[28] In its unstable, cationic state, Coomassie Blue has a background wavelength of 465nm and gives off a reddish-brown color.[29] When Coomassie Blue binds to protein in an acidic solution, the background wavelength shifts to 595nm and the dye gives off a bright blue color.[29] Proteins in the assay bind Coomassie blue in about 2 minutes, and the protein-dye complex is stable for about an hour, although it's recommended that absorbance readings are taken within 5 to 20 minutes of reaction initiation.[28] The concentration of protein in the Bradford assay can then be measured using a visible light spectrophotometer, and therefore does not require extensive equipment.[29]
This method was developed in 1975 by Marion M. Bradford, and has enabled significantly faster, more accurate protein quantitation compared to previous methods: the Lowry procedure and the biuret assay.[28] Unlike the previous methods, the Bradford assay is not susceptible to interference by several non-protein molecules, including ethanol, sodium chloride, and magnesium chloride.[28] However, it is susceptible to influence by strong alkaline buffering agents, such as sodium dodecyl sulfate (SDS).[28]
The terms northern, western and eastern blotting are derived from what initially was a molecular biology joke that played on the term Southern blotting, after the technique described by Edwin Southern for the hybridisation of blotted DNA. Patricia Thomas, developer of the RNA blot which then became known as the northern blot, actually didn't use the term.[30]
Named after its inventor, biologist Edwin Southern, the Southern blot is a method for probing for the presence of a specific DNA sequence within a DNA sample. DNA samples before or after restriction enzyme (restriction endonuclease) digestion are separated by gel electrophoresis and then transferred to a membrane by blotting via capillary action. The membrane is then exposed to a labeled DNA probe that has a complement base sequence to the sequence on the DNA of interest.[31] Southern blotting is less commonly used in laboratory science due to the capacity of other techniques, such as PCR, to detect specific DNA sequences from DNA samples. These blots are still used for some applications, however, such as measuring transgene copy number in transgenic mice or in the engineering of gene knockout embryonic stem cell lines.[19]
The northern blot is used to study the presence of specific RNA molecules as relative comparison among a set of different samples of RNA.  It is essentially a combination of denaturing RNA gel electrophoresis, and a blot.  In this process RNA is separated based on size and is then transferred to a membrane that is then probed with a labeled complement of a sequence of interest. The results may be visualized through a variety of ways depending on the label used; however, most result in the revelation of bands representing the sizes of the RNA detected in sample.  The intensity of these bands is related to the amount of the target RNA in the samples analyzed.  The procedure is commonly used to study when and how much gene expression is occurring by measuring how much of that RNA is present in different samples, assuming that no post-transcriptional regulation occurs and that the levels of mRNA reflect proportional levels of the corresponding protein being produced.  It is one of the most basic tools for determining at what time, and under what conditions, certain genes are expressed in living tissues.[32][33]
In western blotting, proteins are first separated by size, in a thin gel sandwiched between two glass plates in a technique known as SDS-PAGE. The proteins in the gel are then transferred to a polyvinylidene fluoride (PVDF), nitrocellulose, nylon, or other support membrane. This membrane can then be probed with solutions of antibodies. Antibodies that specifically bind to the protein of interest can then be visualized by a variety of techniques, including colored products, chemiluminescence, or autoradiography. Often, the antibodies are labeled with enzymes. When a chemiluminescent substrate is exposed to the enzyme it allows detection. Using western blotting techniques allows not only detection but also quantitative analysis. Analogous methods to western blotting can be used to directly stain specific proteins in live cells or tissue sections.[34][35]
The eastern blotting technique is used to detect post-translational modification of proteins. Proteins blotted on to the PVDF or nitrocellulose membrane are probed for modifications using specific substrates.[36]
A DNA microarray is a collection of spots attached to a solid support such as a microscope slide where each spot contains one or more single-stranded DNA oligonucleotide fragments. Arrays make it possible to put down large quantities of very small (100 micrometre diameter) spots on a single slide. Each spot has a DNA fragment molecule that is complementary to a single DNA sequence. A variation of this technique allows the gene expression of an organism at a particular stage in development to be qualified (expression profiling). In this technique the RNA in a tissue is isolated and converted to labeled complementary DNA (cDNA). This cDNA is then hybridized to the fragments on the array and visualization of the hybridization can be done. Since multiple arrays can be made with exactly the same position of fragments, they are particularly useful for comparing the gene expression of two different tissues, such as a healthy and cancerous tissue. Also, one can measure what genes are expressed and how that expression changes with time or with other factors.
There are many different ways to fabricate microarrays; the most common are silicon chips, microscope slides with spots of ~100 micrometre diameter, custom arrays, and arrays with larger spots on porous membranes (macroarrays). There can be anywhere from 100 spots to more than 10,000 on a given array. Arrays can also be made with molecules other than DNA.[37][38][39][40]
Allele-specific oligonucleotide (ASO) is a technique that allows detection of single base mutations without the need for PCR or gel electrophoresis. Short (2025 nucleotides in length), labeled probes are exposed to the non-fragmented target DNA, hybridization occurs with high specificity due to the short length of the probes and even a single base change will hinder hybridization. The target DNA is then washed and the labeled probes that didn't hybridize are removed. The target DNA is then analyzed for the presence of the probe via radioactivity or fluorescence. In this experiment, as in most molecular biology techniques, a control must be used to ensure successful experimentation.[41][42]
In molecular biology, procedures and technologies are continually being developed and older technologies abandoned. For example, before the advent of DNA gel electrophoresis (agarose or polyacrylamide), the size of DNA molecules was typically determined by rate sedimentation in sucrose gradients, a slow and labor-intensive technique requiring expensive instrumentation; prior to sucrose gradients, viscometry was used. Aside from their historical interest, it is often worth knowing about older technology, as it is occasionally useful to solve another new problem for which the newer technique is inappropriate.[43]


In biological taxonomy, a domain (/dmen/ or /domen/) (Latin: regio[1]), also dominion,[2] superkingdom, realm, or empire,[3] is the highest taxonomic rank of organisms in the three-domain system of taxonomy devised by Carl Woese et al. in 1990.[1]
According to this system, the tree of life consists of three domains: Archaea, Bacteria, and Eukarya.[1] The first two are all prokaryotic microorganisms, or mostly single-celled organisms whose cells have a distorted or non-membrane bound nucleus. All life that has a cell nucleus and eukaryotic membrane-bound organelles is included in Eukarya.
Non-cellular life is not included in this system. Alternatives to the three-domain system include the earlier two-empire system (with the empires Prokaryota and Eukaryota), and the eocyte hypothesis (with two domains of Bacteria and Archaea, and Eukarya included within Archaea).
The term "domain" was proposed by Carl Woese, Otto Kandler and Mark Wheelis (1990) in a three-domain system. This term represents a synonym for the category of dominion (Lat. dominium), introduced by Moore in 1974.[2]
Each of these three domains contains unique rRNA. This forms the basis of the three-domain system. While the presence of a nuclear membrane differentiates the Eukarya from the Archaea and Bacteria, both of which lack a nuclear membrane, distinct biochemical and RNA markers differentiate the Archaea and Bacteria from each other.[1]
Archaea are  prokaryota cells, typically characterized by membrane lipids that are branched hydrocarbon chains attached to glycerol by ether linkages. The presence of these ether linkages in Archaea adds to their ability to withstand extreme temperatures and highly acidic conditions, but many archaea live in mild environments. Halophiles, organisms that thrive in highly salty environments, and hyperthermophiles, organisms that thrive in extremely hot environments, are examples of Archaea.[1]
Archaea evolved many cell sizes, but all are relatively small. Their size ranges from 0.1m to 15m diameter and up to 200m long. They are about the size of bacteria, or similar in size to the mitochondria found in eukaryotic cells. Members of the genus Thermoplasma are the smallest of the Archaea.[1]
Even though bacteria are prokaryotic cells just like Archaea, their membranes are made of phospholipid bilayers. Cyanobacteria and mycoplasmas are two examples of bacteria. They characteristically do not have ether linkages like Archaea, and they are grouped into a different categoryand hence a different domain. There is a great deal of diversity in this domain. Confounded by that diversity and horizontal gene transfer, it is next to impossible to determine how many species of bacteria exist on the planet, or to organize them in a tree-structure, without cross-connections between branches.[1]
Members of the domain Eukaryacalled eukaryoteshave membrane-bound organelles (including a nucleus containing genetic material) and are represented by five kingdoms: Plantae, Protozoa, Animalia, Chromista, and Fungi.[1]
The three-domain system does not include any form of non-cellular life. Stefan Luketa proposed a five-domain system in 2012, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains.[6]
See also: Virus classification
Alternative classifications of life include:


Cell biology (also cellular biology or cytology) is a branch of biology that studies the structure, function and behavior of cells.[1][2] Cell biology encompasses both prokaryotic and eukaryotic cells and has many subtopics which may include the study of cell metabolism, cell communication, cell cycle, biochemistry, and cell composition. The study of cells is performed using several microscopy techniques, cell culture, and cell fractionation. These have allowed for and are currently being used for discoveries and research pertaining to how cells function, ultimately giving insight into understanding larger organisms. Knowing the components of cells and how cells work is fundamental to all biological sciences while also being essential for research in biomedical fields such as cancer, and other diseases. Research in cell biology is interconnected to other fields such as genetics, molecular genetics, biochemistry, molecular biology, medical microbiology, immunology, and cytochemistry.
Cells were first seen in 17th century Europe with the invention of the compound microscope. In 1665, Robert Hooke termed the building block of all living organisms as "cells" after looking at a piece of cork and observing a cell-like structure,[3][4] however, the cells were dead and gave no indication to the actual overall components of a cell. A few years later, in 1674, Anton Van Leeuwenhoek was the first to analyze live cells in his examination of algae. All of this preceded the cell theory which states that all living things are made up of cells and that cells are the functional and structural unit of organisms. This was ultimately concluded by plant scientist, Matthias Schleiden[4] and animal scientist Theodor Schwann in 1838, who viewed live cells in plant and animal tissue, respectively.[5] 19 years later, Rudolf Virchow further contributed to the cell theory, adding that all cells come from the division of pre-existing cells.[5] Although widely accepted, there have been many studies that question the validity of the cell theory. Viruses, for example, lack common characteristics of a living cell, such as membranes, cell organelles, and the ability to reproduce by themselves.[6] The cell was first discovered and named by Robert Hooke in 1665 who had remarked that it looked strangely similar to the cells or small rooms which monks inhabited, thus deriving the name. However what Hooke actually saw was the dead cell walls of plant cells (cork) as it appeared under the microscope. Hookes description of these cells was published in Micrography.[7]Scientists have struggled to decide whether viruses are alive or not and whether they are in agreement with the cell theory.
Modern-day cell biology research looks at different ways to culture and manipulate cells outside of a living body to further research in human anatomy and physiology, and to derive medications. The techniques by which cells are studied have evolved. Due to advancements in microscopy, techniques and technology have allowed for scientists to hold a better understanding of the structure and function of cells. Many techniques commonly used to study cell biology are listed below:[8]
There are two fundamental classifications of cells: prokaryotic and eukaryotic. Prokaryotic cells are distinguished from eukaryotic cells by the absence of a cell nucleus or other membrane-bound organelle.[12] Prokaryotic cells are much smaller than eukaryotic cells, making them the smallest form of life.[13] Prokaryotic cells include Bacteria and Archaea, and lack an enclosed cell nucleus. They both reproduce through binary fission. Bacteria, the most prominent type, have several different shapes which include mainly spherical, and rod-shaped. Bacteria can be classed as either gram positive or gram negative depending on the cell wall composition. Gram positive bacteria have a thick peptidoglycan layer, and no outer lipid membrane  Bacterial structural features include a flagellum that helps the cell to move,[14] ribosomes for the translation of RNA to protein,[14] and a nucleoid that holds all the genetic material in a circular structure.[14] There are many process that occur in prokaryotic cells that allow them to survive. For instance, in a process termed conjugation, fertility factor allows the bacteria to possess a pilus which allows it to transmit DNA to another bacteria which lacks the F factor, permitting the transmittance of resistance allowing it to survive in certain environments.[15]
Eukaryotic cells are composed of the following organelles:
Eukaryotic cells may also be composed of the following molecular components:
Cell metabolism is necessary for the production of energy for the cell and therefore its survival and includes many pathways. For cellular respiration, once glucose is available, glycolysis occurs within the cytosol of the cell to produce pyruvate. Pyruvate undergoes decarboxylation using the multi-enzyme complex to form acetyl coA which can readily be used in the TCA cycle to produce NADH and FADH2. These products are involved in the electron transport chain to ultimately form a proton gradient across the inner mitochondrial membrane. This gradient can then drive the production of ATP and H2O during oxidative phosphorylation.[28] Metabolism in plant cells includes photosynthesis which is simply the exact opposite of respiration as it ultimately produces molecules of glucose.
Cell signaling or cell communication is important for cell regulation and for cells to process information from the environment and respond accordingly. Signaling can occur through direct cell contact or endocrine, paracrine, and autocrine signaling. Direct cell-cell contact is when a receptor on a cell binds a molecule that is attached to the membrane of another cell. Endocrine signaling occurs through molecules secreted into the bloodstream. Paracrine signaling uses molecules diffusing between two cells to communicate. Autocrine is a cell sending a signal to itself by secreting a molecule that binds to a receptor on its surface. Forms of communication can be through:
The growth process of the cell does not refer to the size of the cell, but the density of the number of cells present in the organism at a given time. Cell growth pertains to the increase in the number of cells present in an organism as it grows and develops; as the organism gets larger so does the number of cells present. Cells are the foundation of all organisms and are the fundamental unit of life. The growth and development of cells are essential for the maintenance of the host and survival of the organism. For this process, the cell goes through the steps of the cell cycle and development which involves cell growth, DNA replication, cell division, regeneration, and cell death. The cell cycle is divided into four distinct phases: G1, S, G2, and M. The G phase  which is the cell growth phase  makes up approximately 95% of the cycle. The proliferation of cells is instigated by progenitors. All cells start out in an identical form and can essentially become any type of cells. Cell signaling such as induction can influence nearby cells to determinate the type of cell it will become. Moreover, this allows cells of the same type to aggregate and form tissues, then organs, and ultimately systems. The G1, G2, and S phase (DNA replication, damage and repair) are considered to be the interphase portion of the cycle, while the M phase (mitosis) is the cell division portion of the cycle. Mitosis is composed of many stages which include, prophase, metaphase, anaphase, telophase, and cytokinesis, respectively. The ultimate result of mitosis is the formation of two identical daughter cells.
The cell cycle is regulated by a series of signaling factors and complexes such as cyclins, cyclin-dependent kinase, and p53. When the cell has completed its growth process and if it is found to be damaged or altered, it undergoes cell death, either by apoptosis or necrosis, to eliminate the threat it can cause to the organism's survival.[30]
The ancestry of each present day cell presumably traces back, in an unbroken lineage for over 3 billion years to the origin of life. It is not actually cells that are immortal but multi-generational cell lineages.[31]  The immortality of a cell lineage depends on the maintenance of cell division potential.  This potential may be lost in any particular lineage because of cell damage, terminal differentiation as occurs in nerve cells, or programmed cell death (apoptosis) during development.  Maintenance of cell division potential over successive generations depends on the avoidance and the accurate repair of cellular damage, particularly DNA damage.  In sexual organisms, continuity of the germline depends on the effectiveness of processes for avoiding DNA damage and repairing those DNA damages that do occur.  Sexual processes in eukaryotes, as well as in prokaryotes, provide an opportunity for effective repair of DNA damages in the germ line by homologous recombination.[31][32]
The cell cycle is a four-stage process that a cell goes through as it develops and divides. It includes Gap 1 (G1), synthesis (S), Gap 2 (G2), and mitosis (M).The cell either restarts the cycle from G1 or leaves the cycle through G0 after completing the cycle. The cell can progress from G0 through terminal differentiation.
The interphase refers to the phases of the cell cycle that occur between one mitosis and the next, and includes G1, S, and G2.
The size of the cell grows.
The contents of cells are replicated.
Replication of DNA
The cell replicates each of the 46 chromosomes (23 pairs).
The cell multiplies.
In preparation for cell division, organelles and proteins form.
After mitosis, cytokinesis occurs (cell separation)
Formation of two daughter cells that are identical
These cells leave G1 and enter G0, a resting stage. A cell in G0 is doing its job without actively preparing to divide. [33]
The scientific branch that studies and diagnoses diseases on the cellular level is called cytopathology. Cytopathology is generally used on samples of free cells or tissue fragments, in contrast to the pathology branch of histopathology, which studies whole tissues. Cytopathology is commonly used to investigate diseases involving a wide range of body sites, often to aid in the diagnosis of cancer but also in the diagnosis of some infectious diseases and other inflammatory conditions. For example, a common application of cytopathology is the Pap smear, a screening test used to detect cervical cancer, and precancerous cervical lesions that may lead to cervical cancer.[34]
Czech anatomist Jan Evangelista Purkyn is best known for his 1837 discovery of Purkinje cells.
Theodor Schwann discoverer of the Schwann cell.
Yoshinori Ohsumi Nobel Prize winner for work on autophagy.

Systems biology is the computational and mathematical analysis and modeling of complex biological systems. It is a biology-based interdisciplinary field of study that focuses on complex interactions within biological systems, using a holistic approach (holism instead of the more traditional reductionism) to biological research.[1]
Particularly from the year 2000 onwards, the concept has been used widely in biology in a variety of contexts. The Human Genome Project is an example of applied systems thinking in biology which has led to new, collaborative ways of working on problems in the biological field of genetics.[2] One of the aims of systems biology is to model and discover emergent properties, properties of cells, tissues and organisms functioning as a system whose theoretical description is only possible using techniques of systems biology.[1][3] These typically involve metabolic networks or cell signaling networks.[1][4]
Systems biology can be considered from a number of different aspects.
As a field of study, particularly, the study of the interactions between the components of biological systems, and how these interactions give rise to the function and behavior of that system (for example, the enzymes and metabolites in a metabolic pathway or the heart beats).[5][6][7]
As a paradigm, systems biology is usually defined in antithesis to the so-called reductionist paradigm (biological organisation), although it is consistent with the scientific method. The distinction between the two paradigms is referred to in these quotations: "the reductionist approach has successfully identified most of the components and many of the interactions but, unfortunately, offers no convincing concepts or methods to understand how system properties emerge... the pluralism of causes and effects in biological networks is better addressed by observing, through quantitative measures, multiple components simultaneously and by rigorous data integration with mathematical models." (Sauer et al.)[8] "Systems biology... is about putting together rather than taking apart, integration rather than reduction. It requires that we develop ways of thinking about integration that are as rigorous as our reductionist programmes, but different.... It means changing our philosophy, in the full sense of the term." (Denis Noble)[7]
As a series of operational protocols used for performing research, namely a cycle composed of theory, analytic or computational modelling to propose specific testable hypotheses about a biological system, experimental validation, and then using the newly acquired quantitative description of cells or cell processes to refine the computational model or theory.[9] Since the objective is a model of the interactions in a system, the experimental techniques that most suit systems biology are those that are system-wide and attempt to be as complete as possible. Therefore, transcriptomics, metabolomics, proteomics and high-throughput techniques are used to collect quantitative data for the construction and validation of models.[10]
As the application of dynamical systems theory to molecular biology. Indeed, the focus on the dynamics of the studied systems is the main conceptual difference between systems biology and bioinformatics.[11]
As a socioscientific phenomenon defined by the strategy of pursuing integration of complex data about the interactions in biological systems from diverse experimental sources using interdisciplinary tools and personnel.[12]
Systems biology was begun as a new field of science around 2000, when the Institute for Systems Biology was established in Seattle in an effort to lure "computational" type people who it was felt were not attracted to the academic settings of the university. The institute did not have a clear definition of what the field actually was: roughly bringing together people from diverse fields to use computers to holistically study biology in new ways.[13] A Department of Systems Biology at Harvard Medical School was launched in 2003.[14]  In 2006 it was predicted that the buzz generated by the "very fashionable" new concept would cause all the major universities to need a systems biology department, thus that there would be careers available for graduates with a modicum of ability in computer programming and biology.[13] In 2006 the National Science Foundation put forward a challenge to build a mathematical model of the whole cell.[citation needed] In 2012 the first whole-cell model of Mycoplasma genitalium was achieved by the Karr Laboratory at the Mount Sinai School of Medicine in New York. The whole-cell model is able to predict viability of M. genitalium cells in response to genetic mutations.[15]
An earlier precursor of systems biology, as a distinct discipline, may have been by systems theorist Mihajlo Mesarovic in 1966 with an international symposium at the Case Institute of Technology in Cleveland, Ohio, titled Systems Theory and Biology. Mesarovic predicted that perhaps in the future there would be such as thing as "systems biology".[16][17]
According to Robert Rosen in the 1960s, holistic biology had become pass by the early 20th century, as more empirical science dominated by molecular chemistry had become popular.[17] Echoing him forty years later in 2006 Kling writes that the success of molecular biology throughout the 20th century had suppressed holistic computational methods.[13] By 2011 the National Institutes of Health had made grant money available to support over ten systems biology centers in the United States,[18] but by 2012 Hunter writes that systems biology had not lived up to the hype, having promised more than it achieved, which had caused it to become a somewhat minor field with few practical applications. Nonetheless, proponents hoped that it might once prove more useful in the future.[19]
An important milestone in the development of systems biology has become the international project Physiome.[citation needed]
According to the interpretation of systems biology as using large data sets using interdisciplinary tools, a typical application is metabolomics, which is the complete set of all the metabolic products, metabolites, in the system at the organism, cell, or tissue level.[21]
Items that may be a computer database include: phenomics, organismal variation in phenotype as it changes during its life span; genomics, organismal deoxyribonucleic acid (DNA) sequence, including intra-organismal cell specific variation. (i.e., telomere length variation); epigenomics/epigenetics, organismal and corresponding cell specific transcriptomic regulating factors not empirically coded in the genomic sequence. (i.e., DNA methylation, Histone acetylation and deacetylation, etc.); transcriptomics, organismal, tissue or whole cell gene expression measurements by DNA microarrays or serial analysis of gene expression; interferomics, organismal, tissue, or cell-level transcript correcting factors (i.e., RNA interference), proteomics, organismal, tissue, or cell level measurements of proteins and peptides via two-dimensional gel electrophoresis, mass spectrometry or multi-dimensional protein identification techniques (advanced HPLC systems coupled with mass spectrometry). Sub disciplines include phosphoproteomics, glycoproteomics and other methods to detect chemically modified proteins; glycomics, organismal, tissue, or cell-level measurements of carbohydrates;  lipidomics, organismal, tissue, or cell level measurements of lipids.[citation needed]
The molecular interactions within the cell are also studied, this is called interactomics.[22] A discipline in this field of study is protein-protein interactions, although interactomics includes the interactions of other molecules.[citation needed] Neuroelectrodynamics, where the computer's or a brain's computing function as a dynamic system is studied along with its (bio)physical mechanisms;[23] and fluxomics, measurements of the rates of metabolic reactions in a biological system (cell, tissue, or organism).[21]
In approaching a systems biology problem there are two main approaches. These are the top down and bottom up approach. The top down approach takes as much of the system into account as possible and relies largely on experimental results. The RNA-Seq technique is an example of an experimental top down approach. Conversely, the bottom up approach is used to create detailed models while also incorporating experimental data. An example of the bottom up approach is the use of circuit models to describe a simple gene network.[24]
Various technologies utilized to capture dynamic changes in mRNA, proteins, and post-translational modifications. Mechanobiology, forces and physical properties at all scales, their interplay with other regulatory mechanisms;[25] biosemiotics, analysis of the system of sign relations of an organism or other biosystems; Physiomics, a systematic study of physiome in biology.
Cancer systems biology is an example of the systems biology approach, which can be distinguished by the specific object of study (tumorigenesis and treatment of cancer). It works with the specific data  (patient samples, high-throughput data with particular attention to characterizing cancer genome in patient tumour samples) and tools (immortalized cancer cell lines, mouse models of tumorigenesis, xenograft models, high-throughput sequencing methods, siRNA-based gene knocking down high-throughput screenings, computational modeling of the consequences of somatic mutations and genome instability).[26] The long-term objective of the systems biology of cancer is ability to better diagnose cancer, classify it and better predict the outcome of a suggested treatment, which is a basis for personalized cancer medicine and virtual cancer patient in more distant prospective. Significant efforts in computational systems biology of cancer have been made in creating realistic multi-scale in silico models of various tumours.[27]
The systems biology approach often involves the development of mechanistic models, such as the reconstruction of dynamic systems from the quantitative properties of their elementary building blocks.[28][29][30][31] For instance, a cellular network can be modelled mathematically using methods coming from chemical kinetics[32] and control theory. Due to the large number of parameters, variables and constraints in cellular networks, numerical and computational techniques are often used (e.g., flux balance analysis).[30][32]
Other aspects of computer science, informatics, and statistics are also used in systems biology. These include new forms of computational models, such as the use of process calculi to model biological processes (notable approaches include stochastic -calculus, BioAmbients, Beta Binders, BioPEPA, and Brane calculus) and constraint-based modeling; integration of information from the literature, using techniques of information extraction and text mining;[33] development of online databases and repositories for sharing data and models, approaches to database integration and software interoperability via loose coupling of software, websites and databases, or commercial suits; network-based approaches for analyzing high dimensional genomic data sets. For example, weighted correlation network analysis is often used for identifying clusters (referred to as modules), modeling the relationship between clusters, calculating fuzzy measures of cluster (module) membership, identifying intramodular hubs, and for studying cluster preservation in other data sets; pathway-based methods for omics data analysis, e.g. approaches to identify and score pathways with differential activity of their gene, protein, or metabolite members.[34] Much of the analysis of genomic data sets also include identifying correlations. Additionally, as much of the information comes from different fields, the development of syntactically and semantically sound ways of representing biological models is needed.[35]
Researchers begin by choosing a biological pathway and diagramming all of the protein interactions. After determining all of the interactions of the proteins, mass action kinetics is utilized to describe the speed of the reactions in the system. Mass action kinetics will provide differential equations to model the biological system as a mathematical model in which experiments can determine the parameter values to use in the differential equations.[37] These parameter values will be the reaction rates of each proteins interaction in the system. This model determines the behavior of certain proteins in biological systems and bring new insight to the specific activities of individual proteins. Sometimes it is not possible to gather all reaction rates of a system. Unknown reaction rates are determined by simulating the model of known parameters and target behavior which provides possible parameter values.[38][36]
The use of constraint-based reconstruction and analysis (COBRA) methods has become popular among systems biologists to simulate and predict the metabolic phenotypes, using genome-scale models. One of the methods is the flux balance analysis (FBA) approach, by which one can study the biochemical networks and analyze the flow of metabolites through a particular metabolic network, by maximizing the object of interest.[39]
living systems
In biological classification, the order (Latin: ordo) is
What does and does not belong to each order is determined by a taxonomist, as is whether a particular order should be recognized at all. Often there is no exact agreement, with different taxonomists each taking a different position. There are no hard rules that a taxonomist needs to follow in describing or recognizing an order. Some taxa are accepted almost universally, while others are recognized only rarely.[1]
The name of an order is usually written with a capital letter.[2] For some groups of organisms, their orders may follow consistent naming schemes. Orders of plants, fungi, and algae use the suffix -ales (e.g. Dictyotales).[3] Orders of birds and fishes use the Latin suffix -(i)formes meaning 'having the form of' (e.g. Passeriformes), but orders of mammals and invertebrates are not so consistent (e.g. Artiodactyla, Actiniaria, Primates).
For some clades covered by the International Code of Zoological Nomenclature, several additional classifications are sometimes used, although not all of these are officially recognized.
In their 1997 classification of mammals, McKenna and Bell used two extra levels between superorder and order: grandorder and mirorder.[4] Michael Novacek (1986) inserted them at the same position. Michael Benton (2005) inserted them between superorder and magnorder instead.[5] This position was adopted by Systema Naturae 2000 and others.
In botany, the ranks of subclass and suborder are secondary ranks pre-defined as respectively above and below the rank of order.[6] Any number of further ranks can be used as long as they are clearly defined.[6]
The superorder rank is commonly used, with the ending -anae that was initiated by Armen Takhtajan's publications from 1966 onwards.[7]
The order as a distinct rank of biological classification having its own distinctive name (and not just called a higher genus (genus summum)) was first introduced by the German botanist Augustus Quirinus Rivinus in his classification of plants that appeared in a series of treatises in the 1690s. Carl Linnaeus was the first to apply it consistently to the division of all three kingdoms of nature (then minerals, plants, and animals) in his Systema Naturae (1735, 1st. Ed.).
For plants, Linnaeus' orders in the Systema Naturae and the Species Plantarum were strictly artificial, introduced to subdivide the artificial classes into more comprehensible smaller groups.  When the word ordo was first consistently used for natural units of plants, in 19th century works such as the Prodromus of de Candolle and the Genera Plantarum of Bentham & Hooker, it indicated taxa that are now given the rank of family. (See ordo naturalis, 'natural order'.)
In French botanical publications, from Michel Adanson's Familles naturelles des plantes (1763) and until the end of the 19th century, the word famille (plural: familles) was used as a French equivalent for this Latin ordo. This equivalence was explicitly stated in the Alphonse De Candolle's Lois de la nomenclature botanique (1868), the precursor of the currently used International Code of Nomenclature for algae, fungi, and plants.
In the first international Rules of botanical nomenclature from the International Botanical Congress of 1905, the word family (familia) was assigned to the rank indicated by the French famille, while order (ordo) was reserved for a higher rank, for what in the 19th century had often been named a cohors[9] (plural cohortes).
Some of the plant families still retain the names of Linnaean "natural orders" or even the names of pre-Linnaean natural groups recognised by Linnaeus as orders in his natural classification (e.g. Palmae or Labiatae). Such names are known as descriptive family names.
In zoology, the Linnaean orders were used more consistently. That is, the orders in the zoology part of the Systema Naturae refer to natural groups. Some of his ordinal names are still in use (e.g. Lepidoptera for the order of moths and butterflies; Diptera for the order of flies, mosquitoes, midges, and gnats).[citation needed]
In virology, the International Committee on Taxonomy of Viruses's virus classification includes fifteen taxa to be applied for viruses, viroids and satellite nucleic acids: realm, subrealm, kingdom, subkingdom, phylum, subphylum, class, subclass, order, suborder, family, subfamily, genus, subgenus, and species.[10] There are currently fourteen viral orders, each ending in the suffix -virales.[11]


Family (Latin: familia, plural familiae) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy; it is classified between order and genus. A family may be divided into subfamilies, which are intermediate ranks between the ranks of family and genus. The official family names are Latin in origin; however, popular names are often used: for example, walnut trees and hickory trees belong to the family Juglandaceae, but that family is commonly referred to as being the "walnut family".
What belongs to a familyor if a described family should be recognized at allare proposed and determined by practicing taxonomists. There are no hard rules for describing or recognizing a family, but in plants, they can be characterized on the basis of both vegetative and reproductive feature of plant species. Taxonomists often take different positions about descriptions, and there may be no broad consensus across the scientific community for some time. The publishing of new data and opinions often enables adjustments and consensus.
The naming of families is codified by various international bodies using the following suffixes:
The taxonomic term familia was first used by French botanist Pierre Magnol in his Prodromus historiae generalis plantarum, in quo familiae plantarum per tabulas disponuntur (1689) where he called the seventy-six groups of plants he recognised in his tables families (familiae). The concept of rank at that time was not yet settled, and in the preface to the Prodromus Magnol spoke of uniting his families into larger genera, which is far from how the term is used today.
Carl Linnaeus used the word familia in his Philosophia botanica (1751) to denote major groups of plants: trees, herbs, ferns, palms, and so on. He used this term only in the morphological section of the book, discussing the vegetative and generative organs of plants.
Subsequently, in French botanical publications, from Michel Adanson's Familles naturelles des plantes (1763) and until the end of the 19th century, the word famille was used as a French equivalent of the Latin ordo (or ordo naturalis).
In zoology, the family as a rank intermediate between order and genus was introduced by Pierre Andr Latreille in his Prcis des caractres gnriques des insectes, disposs dans un ordre naturel (1796). He used families (some of them were not named) in some but not in all his orders of "insects" (which then included all arthropods).
In nineteenth-century works such as the Prodromus of Augustin Pyramus de Candolle and the Genera Plantarum of George Bentham and Joseph Dalton Hooker this word ordo was used for what now is given the rank of family.
Families can be used for evolutionary, palaeontological and genetic studies because they are more stable than lower taxonomic levels such as genera and species.[4][5]


Mathematics (from Greek: , mthma, 'knowledge, study, learning') includes the study of such topics as numbers (arithmetic and number theory),[1] formulas and related structures (algebra),[2] shapes and spaces in which they are contained (geometry),[1] and quantities and their changes (calculus and analysis).[3][4][5] There is no general consensus about its exact scope or epistemological status.[6][7]
Most of mathematical activity consists of discovering and proving (by pure reasoning) properties of abstract objects. These objects are either abstractions from nature (such as natural numbers or "a line"), or (in modern mathematics) abstract entities that are defined by their basic properties, called axioms. A proof consists of a succession of applications of some deductive rules to already known results, including previously proved theorems, axioms and (in case of abstraction from nature) some basic properties that are considered as true starting points of the theory under consideration. The result of a proof is called a theorem. Contrary to physical laws, the validity of a theorem (its truth) does not rely on any experimentation but on the correctness of its reasoning (though experimentation is often useful for discovering new theorems of interest).
Mathematics is widely used in science for modeling phenomena. This enables the extraction of quantitative predictions from experimental laws. For example, the movement of planets can be predicted with high accuracy using Newton's law of gravitation combined with mathematical computation. The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model for describing the reality. So when some inaccurate predictions arise, it means that the model must be improved or changed, not that the mathematics is wrong. For example, the perihelion precession of Mercury cannot be explained by Newton's law of gravitation, but is accurately explained by Einstein's general relativity. This experimental validation of Einstein's theory shows that Newton's law of gravitation is only an approximation (which still is very accurate in everyday life). 
Mathematics is essential in many fields, including natural sciences, engineering, medicine, finance, computer science and social sciences.
Some areas of mathematics, such as statistics and game theory, are developed in direct correlation with their applications, and are often grouped under the name of applied mathematics. Other mathematical areas are developed independently from any application (and are therefore called pure mathematics), but practical applications are often discovered later.[8][9] A fitting example is the problem of integer factorization which goes back to Euclid but had no practical application before its use in the RSA cryptosystem (for the security of computer networks).
Mathematics has been a human activity from as far back as written records exist. However, the concept of a "proof" and its associated "mathematical rigour" first appeared in Greek mathematics, most notably in Euclid's Elements.[10] Mathematics developed at a relatively slow pace until the Renaissance, when algebra and infinitesimal calculus were added to arithmetic and geometry as main areas of mathematics. Since then the interaction between mathematical innovations and scientific discoveries have led to a rapid increase in the rate of mathematical discoveries. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method. This, in turn, gave rise to a dramatic increase in the number of mathematics areas and their fields of applications; a witness of this is the Mathematics Subject Classification, which lists more than sixty first-level areas of mathematics.
Before the Renaissance, mathematics was divided into two main areas, arithmetic, devoted to the manipulation of numbers, and geometry, devoted to the study of shapes. There was also some pseudo-science, such as numerology and astrology that were not clearly distinguished from mathematics.
Around the Renaissance, two new main areas appeared. The introduction of mathematical notation led to algebra, which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, a shorthand of infinitesimal calculus and integral calculus, is the study of continuous functions, which model the change of, and the relationship between varying quantities (variables). This division into four main areas remained valid until the end of the 19th century, although some areas, such as celestial mechanics and solid mechanics, which were often considered as mathematics, are now considered as belonging to physics. Also, some subjects developed during this period predate mathematics (being divided into different) areas, such as probability theory and combinatorics, which only later became regarded as autonomous areas of their own.
At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion in the amount of areas of mathematics. The Mathematics Subject Classification contains more than 60 first-level areas. Some of these areas correspond to the older division in four main areas. This is the case of 11:number theory (the modern name for higher arithmetic) and 51:Geometry. However, there are several other first-level areas that have "geometry" in their name or are commonly considered as belonging to geometry. Algebra and calculus do not appear as first-level areas, but are each split into several first-level areas. Other first-level areas did not exist at all before the 20th century (for example 18:Category theory; homological algebra, and 68:computer science) or were not considered before as mathematics, such as 03:Mathematical logic and foundations (including model theory, computability theory, set theory, proof theory, and algebraic logic).
Number theory started with the manipulation of numbers, that is, natural numbers




(

N

)
,


{\displaystyle (\mathbb {N} ),}

 later expanded to integers 



(

Z

)


{\displaystyle (\mathbb {Z} )}

 and rational numbers 



(

Q

)
.


{\displaystyle (\mathbb {Q} ).}

 Number theory was formerly called arithmetic, but nowadays this term is mostly used for the methods of calculation with numbers.
A specificity of number theory is that many problems that can be stated very elementarily are very difficult, and, when solved, have a solution that require very sophisticated methods coming from various parts of mathematics. A notable example is Fermat's Last theorem that was stated in 1637 by Pierre de Fermat and proved only in 1994 by Andrew Wiles, using, among other tools, algebraic geometry (more specifically scheme theory), category theory and homological algebra. Another example is Goldbach's conjecture, that asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach it remains unproven despite considerable effort.
In view of the diversity of the studied problems and the solving methods, number theory is presently split in several subareas, which include analytic number theory, algebraic number theory, geometry of numbers (method oriented), Diophantine equations and transcendence theory (problem oriented).
Geometry is, with arithmetic, one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the need of surveying and architecture. 
A fundamental innovation was the elaboration of proofs by ancient Greeks: it is not sufficient to verify by measurement that, say, two lengths are equal. Such a property must be proved by abstract reasoning from previously proven results (theorems) and basic properties (which are considered as self-evident because they are too basic for being the subject of a proof (postulates)). This principle, which is foundational for all mathematics, was elaborated for the sake of geometry, and was systematized by Euclid around 300 BC in his book Elements.
The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the (three-dimensional) Euclidean space.[b]
Euclidean geometry was developed without a change of methods or scope until the 17th century, when Ren Descartes introduced what is now called Cartesian coordinates. This was a major change of paradigm, since instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using numbers (their coordinates), and for the use of algebra and later, calculus for solving geometrical problems. This split geometry in two parts that differ only by their methods, synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.
Analytic geometry allows the study of new shapes, in particular curves that are not  related to circles and lines; these curves are defined either as graph of functions (whose study led to differential geometry), or by implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry makes it possible to consider spaces dimensions higher than three (it suffices to consider more than three coordinates), which are no longer a model of the physical space. 
Geometry expanded quickly during the 19th century. A major event was the discovery (in the second half of the 19th century) of non-Euclidean geometries, which are geometries where the parallel postulate is abandoned. This is, besides Russel's paradox, one of the starting points of the foundational crisis of mathematics, by taking into question the truth of the aforementioned postulate. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem. In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that are invariant under specific transformations of the space. This results in a number of subareas and generalizations of geometry that include:
Pythagorean theorem
Conic Sections
Elliptic curve
Triangle on a paraboloid
Torus
Fractal
Algebra may be viewed as the art of manipulating equations and formulas. Diophantus (3d century) and Al-Khowarazmi (9th century) were two main precursors of algebra. The first one solved some relations between unknown natural numbers (that is, equations) by deducing new relations until getting the solution. The second one introduced systematic methods for transforming equations (such as moving a term from a side of an equation into the other side). The term algebra is derived from the Arabic word that he used for naming one of these methods in the title of his main treatise.
Algebra began to be a specific area only with Franois Vite (15401603), who introduced the use of letters (variables) for representing unknown or unspecified numbers. This allows describing consisely the operations that have to be done on the numbers represented by the variables.
Until the 19th century, algebra consisted mainly of the study of linear equations that is called presently linear algebra, and polynomial equations in a single unknown, which were called algebraic equations (a term that is still in use, although it may be ambiguous). During the 19th century, variables began to represent other things than numbers (such as matrices, modular integers, and geometric transformations), on which some operations can operate, which are often generalizations of arithmetic operations. For dealing with this, the concept of algebraic structure was introduced, which consist of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. So, the scope of algebra evolved for becoming essentially the study of algebraic sructures. This object of algebra was called modern algebra or abstract algebra, the latter term being still used, mainly in an educational context, in opposition with elementary algebra which is concerned with the older way of manipulating formulas.
Some types of algebraic structures have properties that are useful, and often fundamental, in many areas of mathematics. Their study are nowadays autonomous parts of algebra, which include:
The study of types algebraic structures as mathematical objects is the object of universal algebra and category theory. The latter applies to every mathematical structure (not only the algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.
These subjects belong to mathematics since the end of the 19th century. Before this period, sets were not considered as mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy, and was not specifically studied by mathematicians.
Before the study of infinite sets by Georg Cantor, mathematicians were reluctant to consider collections that are actually infinite, and considered infinity as the result of an endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets, but also by showing that this implies different sizes of infinity (see Cantor's diagonal argument) and the existence of mathematical objects that cannot be computed, and not even be explicitly described (for example, Hamel bases of the real numbers over the rational numbers). This led to the controversy over Cantor's set theory.
In the same period, it appeared in various areas of mathematics that the former intuitive definitions of the basic mathematical objects were insufficient for insuring mathematical rigour. Examples of such intuitive definitions are "a set is a collection of objects", "natural number is what is used for counting", "a point is a shape with a zero length in every direction", "a curve is a trace left by a moving point", etc.
This is the origin of the foundational crisis of mathematics.[11] It has been eventually solved in the mainstream of mathematics by systematize the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have. For example, in Peano arithmetic, the natural numbers are defined by "zero is a number", "each number as a unique successor", "each number but zero has a unique predecessor", and some rules of reasoning. The "nature" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even as many mathematicians have opinions on this nature, and use their opinionsometimes called "intuition"to guide their study and finding proofs.
This approach allows considering "logics" (that is, sets of allowed deducing rules), theorems, proofs, etc.) as mathematical objects, and to prove theorems about them. For example, Gdel's incompleteness theorems assert, roughly speaking that, in every theory that contains the natural numbers, there are theorems that are true (that is provable in a larger theory), but not provable inside the theory. 
This approach of the foundations of the mathematics was challenged during the first half of the 20th century by mathematicians leaded by L. E. J. Brouwer who promoted an intuitionistic logic that excludes the law of excluded middle.
These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theory), proof theory, type theory, computability theory and computational complexity theory. These aspects of mathematical logic were introdeced before the rise of computers. There use for compiler design, program certification, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[12]
Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a tool to investigate it. Functions arise here as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.
Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, "applied mathematics" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the "formulation, study, and use of mathematical models" in science, engineering, and other areas of mathematical practice.
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.
Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) "create data that makes sense" with random sampling and with randomized experiments;[13] the design of a statistical sample or experiment specifies the analysis of the data (before the data becomes available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians "make sense of the data" using the art of modelling and the theory of inferencewith model selection and estimation; the estimated models and consequential predictions should be tested on new data.[c]
Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[14] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[15]
Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretisation with special focus on rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.
The history of mathematics can be seen as an ever-increasing series of abstractions. Evolutionarily speaking, the first abstraction to ever take place, which is shared by many animals,[16] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely the quantity of their members. As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like timedays, seasons, or years.[17][18]
Evidence for more complex mathematics does not appear until around 3000BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[19] The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.[20]
Beginning in the 6th century BC with the Pythagoreans, with Greek mathematics the Ancient Greeks began a systematic study of mathematics as a subject in its own right.[21] Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and influential textbook of all time.[22] The greatest mathematician of antiquity is often held to be Archimedes (c. 287212 BC) of Syracuse.[23] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[24] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[25] trigonometry (Hipparchus of Nicaea, 2nd century BC),[26] and the beginnings of algebra (Diophantus, 3rd century AD).[27]
The HinduArabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.
During the Golden Age of Islam, especially during the 9th and 10thcenturies, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system.[28] Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dn al-s.
During the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Isaac Newton and Gottfried Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gdel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic systemif powerful enough to describe arithmeticwill contain true propositions that cannot be proved.
Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January2006 issue of the Bulletin of the American Mathematical Society, "The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9million, and more than 75thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs."[29]
The word mathematics comes from Ancient Greek mthma (), meaning "that which is learnt,"[30] "what one gets to know," hence also "study" and "science". The word for "mathematics" came to have the narrower and more technical meaning "mathematical study" even in Classical times.[31] Its adjective is mathmatiks (), meaning "related to learning" or "studious," which likewise further came to mean "mathematical." In particular, mathmatik tkhn ( ; Latin: ars mathematica) meant "the mathematical art."
Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathmatikoi ()which at the time meant "learners" rather than "mathematicians" in the modern sense.
In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.[32]
The apparent plural form in English, like the French plural form les mathmatiques (and the less commonly used singular derivative la mathmatique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathmatik ( ), used by Aristotle (384322BC), and meaning roughly "all things mathematical", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.[33] In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.[34]
There is no general consensus about the exact definition or epistemological status of mathematics.[6][7] Aristotle defined mathematics as "the science of quantity" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property "separable in thought" from real instances set mathematics apart.[35]
In the 19thcentury, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[36]
A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[6] There is not even consensus on whether mathematics is an art or a science.[7] Some just say, "Mathematics is what mathematicians do."[6]
Three leading types of definition of mathematics today are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[37] All have severe flaws, none has widespread acceptance, and no reconciliation seems possible.[37]
An early definition of mathematics in terms of logic was that of Benjamin Peirce (1870): "the science that draws necessary conclusions."[38] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. An example of a logicist definition of mathematics is Russell's (1903) "All Mathematics is Symbolic Logic."[39]
Intuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is "Mathematics is the mental activity which consists in carrying out constructs one after the other."[37] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct. Intuitionists also reject the law of excluded middle (i.e., 



P


P


{\displaystyle P\vee \neg P}

). While this stance does force them to reject one common version of proof by contradiction as a viable proof method, namely the inference of 



P


{\displaystyle P}

 from 




P




{\displaystyle \neg P\to \bot }

, they are still able to infer 




P


{\displaystyle \neg P}

 from 



P




{\displaystyle P\to \bot }

. For them, 




(

P
)


{\displaystyle \neg (\neg P)}

 is a strictly weaker statement than 



P


{\displaystyle P}

.[40]
Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as "the science of formal systems".[41] A formal system is a set of symbols, or tokens, and some rules on how the tokens are to be combined into formulas. In formal systems, the word axiom has a special meaning different from the ordinary meaning of "a self-evident truth", and is used to refer to a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.
The German mathematician Carl Friedrich Gauss referred to mathematics as "the Queen of the Sciences".[42] More recently, Marcus du Sautoy has called mathematics "the Queen of Science... the main driving force behind scientific discovery".[43] The philosopher Karl Popper observed that "most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."[44] Popper also noted that "I shall certainly admit a system as empirical or scientific only if it is capable of being tested by experience."[45]
Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.
Several authors consider that mathematics is not a science because it does not rely on empirical evidence.[46][47][48][49]
The opinions of mathematicians on this matter are varied. Many mathematicians[50] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics.[51] One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in philosophy of mathematics.[52]
Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences pose problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[53]
Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography.
This remarkable fact, that even the "purest" mathematics often turns out to have practical applications, is what the physicist Eugene Wigner has named "the unreasonable effectiveness of mathematics".[9] The philosopher of mathematics Mark Steiner has written extensively on this matter and acknowledges that the applicability of mathematics constitutes a challenge to naturalism.[54] For the philosopher of mathematics Mary Leng, the fact that the physical world acts in accordance with the dictates of non-causal mathematical entities existing beyond the universe is "a happy coincidence".[55] On the other hand, for some  anti-realists, connections, which are acquired among mathematical things, just mirror the connections acquiring among objects in the universe, so there is no "happy coincidence".[55]
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46pages.[56] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.
For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds up calculation, such as the fast Fourier transform. G. H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[57] Mathematical research often seeks critical features of a mathematical object. A theorem expressed as a characterization of an object by these features is the prize. Examples of particularly succinct and revelatory mathematical arguments have been published in Proofs from THE BOOK.
The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. At the other social extreme, philosophers continue to find problems in philosophy of mathematics, such as the nature of mathematical proof.[58]
Most of the mathematical notation in use today was not invented until the 16th century.[59] Before that, mathematics was written out in words, limiting mathematical discovery.[60] Euler (17071783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.[61] Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.[62] Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.[63]
Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for "if and only if" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as "rigor".
Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken "theorems", based on fallible intuitions, of which many instances have occurred in the history of the subject.[d] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19thcentury. Misunderstanding the rigor is a notable cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may be erroneous if the used computer program is erroneous.[e][64] On the other hand, proof assistants allow for the verification of all details that cannot be given in a hand-written proof, and provide certainty of the correctness of long proofs such as that of the FeitThompson theorem.[f]
Axioms in traditional thought were "self-evident truths", but that conception is problematic.[65] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gdel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless, mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[66]
Arguably the most prestigious award in mathematics is the Fields Medal,[67][68] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.
The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.
A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the "Millennium Prize Problems", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward. Currently, only one of these problems, the Poincar conjecture, has been solved.

Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying "smoothly", the objects studied in discrete mathematics  such as integers, graphs, and statements in logic[1][2]  do not vary smoothly in this way, but have distinct, separated values.[3][4] Discrete mathematics therefore excludes topics in "continuous mathematics" such as calculus or Euclidean geometry. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[5] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term "discrete mathematics."[6] Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.
The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.
Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research.
Although the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.
In university curricula, "Discrete Mathematics" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well.[7][8] Some high-school-level discrete mathematics textbooks have appeared as well.[9] At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.[10]
The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.
The history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).[11]
In logic, the second problem on David Hilbert's list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gdel's second incompleteness theorem, proved in 1931, showed that this was not possible  at least not within arithmetic itself. Hilbert's tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.
The need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers.[12] At the same time, military requirements motivated advances in operations research. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. Operations research remained important as a tool in business and project management, with the critical path method being developed in the 1950s. The telecommunication industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.
Computational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.
Several fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.[13]
Currently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.[14]
Theoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical computer science is the study of algorithms and data structures. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time, space, and other resources taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.
Information theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.
Logic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce's law (((PQ)P)P) is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has applications to automated theorem proving and formal verification of software.
Logical formulas are discrete structures, as are proofs, which form finite trees[15] or, more generally, directed acyclic graph structures[16][17] (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: true and false, but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied,[18] e.g. infinitary logic.
Set theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.
In discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.
Combinatorics studies the way in which discrete structures can be combined or arranged.
Enumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.
Analytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.
Design theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.
Partition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.
Order theory is the study of partially ordered sets, both finite and infinite.
Graph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right.[19] Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory. There are also continuous graphs; however, for the most part, research in graph theory falls within the domain of discrete mathematics.
Discrete probability theory deals with events that occur in countable sample spaces. For example, count observations such as the numbers of birds in flocks comprise only natural number values {0, 1, 2, ...}. On the other hand, continuous observations such as the weights of birds comprise real number values and would typically be modeled by a continuous probability distribution such as the normal. Discrete probability distributions can be used to approximate continuous ones and vice versa. For highly constrained situations such as throwing dice or experiments with decks of cards, calculating the probability of events is basically enumerative combinatorics.
Number theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography and cryptanalysis, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.
Algebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.
A function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance, where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals, there are discrete transforms for discrete functions or digital signals. As well as the discrete metric there are more general discrete or finite metric spaces and finite topological spaces.
Discrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane. Computational geometry applies algorithms to geometrical problems.
Although topology is the field of mathematics that formalizes and generalizes the intuitive notion of "continuous deformation" of objects, it gives rise to many discrete topics; this can be attributed in part to the focus on topological invariants, which themselves usually take discrete values.
See combinatorial topology, topological graph theory, topological combinatorics, computational topology, discrete topological space, finite topological space, topology (chemistry).
Operations research provides techniques for solving practical problems in engineering, business, and other fields  problems such as allocating resources to maximize profit, and scheduling project activities to minimize risk. Operations research techniques include linear programming and other areas of optimization, queuing theory, scheduling theory, and network theory. Operations research also includes continuous topics such as continuous-time Markov process, continuous-time martingales, process optimization, and continuous and hybrid control theory.
Decision theory is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision.
Utility theory is about measures of the relative economic satisfaction from, or desirability of, consumption of various goods and services.
Social choice theory is about voting. A more puzzle-based approach to voting is ballot theory.
Game theory deals with situations where success depends on the choices of others, which makes choosing the best course of action more complex. There are even continuous games, see differential game. Topics include auction theory and fair division.
Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.
There are many concepts in continuous mathematics which have discrete versions, such as discrete calculus, discrete probability distributions, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, difference equations, discrete dynamical systems, and discrete vectormeasures.
In applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation.
In algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form 



V
(
x

c
)

Spec

K
[
x
]
=


A


1




{\displaystyle V(x-c)\subset \operatorname {Spec} K[x]=\mathbb {A} ^{1}}

 for 



K


{\displaystyle K}

 a field can be studied either as 



Spec

K
[
x
]

/

(
x

c
)

Spec

K


{\displaystyle \operatorname {Spec} K[x]/(x-c)\cong \operatorname {Spec} K}

, a point, or as the spectrum 



Spec

K
[
x

]

(
x

c
)




{\displaystyle \operatorname {Spec} K[x]_{(x-c)}}

 of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.
The time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical systems.

The area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the patterns in nature, the field of astronomy and to record time and formulate calendars.
The earliest mathematical texts available are from Mesopotamia and Egypt  Plimpton 322 (Babylonian c. 2000  1900 BC),[2] the Rhind Mathematical Papyrus (Egyptian c. 1800 BC)[3] and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a "demonstrative discipline" began in the 6th century BC with the Pythagoreans, who coined the term "mathematics" from the ancient Greek  (mathema), meaning "subject of instruction".[4] Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics.[5] Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers.[6][7] The HinduArabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muammad ibn Ms al-Khwrizm.[8][9] Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations.[10] Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.
Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.[citation needed]
The origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form.[11] Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the "number" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between "one", "two", and "many", but not of numbers larger than two.[11]
The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a tally of the earliest known demonstration of sequences of prime numbers[12] or a six-month lunar calendar.[13] Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that "no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10."[14] The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.[15]
Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design.[16] All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.[17]
Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity.[18] The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period).[19] It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.
In contrast to the sparsity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s.[20] Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.[21]
The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onward, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.[22]
Babylonian mathematics were written using a sexagesimal (base-60) numeral system.[20] From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60  6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30.[20] Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a place-value system, where digits written in the left column represented larger values, much as in the decimal system.[19] The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different from multiplying integers, similar to modern notation.[19] The notational system of the Babylonians was the best of any civilization until the Renaissance,[23] and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of 2 accurate to five decimal places.[23] The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.[19] By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions.[19] This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.[19]
Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular numbers, and their reciprocal pairs.[24] The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time.[25] Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem.[26] However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.[21]
Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.
The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 20001800 BC.[27] It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge,[28] including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6).[29] It also shows how to solve first order linear equations[30] as well as arithmetic and geometric series.[31]
Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC.[32] It consists of what are today called word problems or story problems, which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).
Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.[33]
Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD.[34] Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.[35]
Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.[36]
Greek mathematics is thought to have begun with Thales of Miletus (c. 624c.546 BC) and Pythagoras of Samos (c. 582c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.
Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed.[37] Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number".[38] It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem,[39] though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers.[40][41] Although he was preceded by the Babylonians and the Chinese,[42] the Neopythagorean mathematician Nicomachus (60120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum).[43] The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the mensa Pythagorica.[44]
Plato (428/427 BC  348/347 BC) is important in the history of mathematics for inspiring and guiding others.[45] His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came.[46] Plato also discussed the foundations of mathematics,[47] clarified some of the definitions (e.g. that of a line as "breadthless length"), and reorganized the assumptions.[48] The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.[46]
Eudoxus (408c. 355 BC) developed the method of exhaustion, a precursor of modern integration[49] and a theory of ratios that avoided the problem of incommensurable magnitudes.[50] The former allowed the calculations of areas and volumes of curvilinear figures,[51] while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.[52]
In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria.[54] It was there that Euclid (c. 300 BC) taught, and wrote the Elements, widely considered the most successful and influential textbook of all time.[1] The Elements introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[55] The Elements was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today.[56] In addition to the familiar theorems of Euclidean geometry, the Elements was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry,[55] including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.[57]
Archimedes (c. 287212 BC) of Syracuse, widely considered the greatest mathematician of antiquity,[58] used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[59] He also showed one could use the method of exhaustion to calculate the value of  with as much precision as desired, and obtained the most accurate value of  then known, 310/71 <  < 310/70.[60] He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid),[59] and an ingenious method of exponentiation for expressing very large numbers.[61] While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles.[62] He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.[63]
Apollonius of Perga (c. 262190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone.[64] He also coined the terminology in use today for conic sections, namely parabola ("place beside" or "comparison"), "ellipse" ("deficiency"), and "hyperbola" ("a throw beyond").[65] His work Conics is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton.[66] While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.[67]
Around the same time, Eratosthenes of Cyrene (c. 276194 BC) devised the Sieve of Eratosthenes for finding prime numbers.[68] The 3rd century BC is generally regarded as the "Golden Age" of Greek mathematics, with advances in pure mathematics henceforth in relative decline.[69] Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers.[69] Hipparchus of Nicaea (c. 190120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle.[70] Heron of Alexandria (c. 1070 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots.[71] Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem.[72] The most complete and influential trigonometric work of antiquity is the Almagest of Ptolemy (c. AD 90168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years.[73] Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of  outside of China until the medieval period, 3.1416.[74]
Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the "Silver Age" of Greek mathematics.[75] During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as "Diophantine analysis".[76] The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the Arithmetica, a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations.[77] The Arithmetica had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the Arithmetica (that of dividing a square into two squares).[78] Diophantus also made significant advances in notation, the Arithmetica being the first instance of algebraic symbolism and syncopation.[77]
Among the last great Greek mathematicians is Pappus of Alexandria (4th  century AD).  He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His Collection is a major source of knowledge on Greek mathematics as most of it has survived.[79] Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library[citation needed] and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed.[80] Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius.[81]  Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics.  The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia.[82]  Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.[83]
Although ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison.[84][85] Ancient Romans such as Cicero (10643 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks.[86] It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.[87]
Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury.[88] Siculus Flaccus, one of the Roman gromatici (i.e. land surveyor), wrote the Categories of Fields, which aided Roman surveyors in measuring the surface areas of allotted lands and territories.[89] Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns.[90] Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.[91][92]
The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year.[93] In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February.[94] This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (10044 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle.[95] This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (r.15721585), virtually the same solar calendar used in modern times as the international standard calendar.[96]
At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC  c. 15 BC).[97] The device was used at least until the reign of emperor Commodus (r.177192 AD), but its design seems to have been lost until experiments were made during the 15th century in Western Europe.[98] Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2m) in diameter turning four-hundred times in one Roman mile (roughly 4590ft/1400m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.[99]
An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development.[100] The oldest extant mathematical text from China is the Zhoubi Suanjing, variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable.[101] However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.[42]
Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called "rod numerals" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten.[102] Thus, the number 123 would be written using the symbol for "1", followed by the symbol for "100", then the symbol for "2" followed by the symbol for "10", followed by the symbol for "3". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system.[103] Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the suan pan, or Chinese abacus. The date of the invention of the suan pan is not certain, but the earliest written mention dates from AD 190, in Xu Yue's Supplementary Notes on the Art of Figures.
The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well.[104] It also defined the concepts of circumference, diameter, radius, and volume.[105]
In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is The Nine Chapters on the Mathematical Art, the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles.[101] It created mathematical proof for the Pythagorean theorem,[106] and a mathematical formula for Gaussian elimination.[107] The treatise also provides values of ,[101] which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78139) approximated pi as 3.1724,[108] as well as 3.162 by taking the square root of 10.[109][110] Liu Hui commented on the Nine Chapters in the 3rd century AD and gave a value of  accurate to 5 decimal places (i.e. 3.14159).[111][112] Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of  to seven decimal places (i.e. 3.141592), which remained the most accurate value of  for almost the next 1000 years.[111][113] He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.[114]
The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (9601279), with the development of Chinese algebra. The most important text from that period is the Precious Mirror of the Four Elements by Zhu Shijie (12491314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method.[111] The Precious Mirror also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100.[115] The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 12381298).[115]
Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.[115]
Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere.[116] Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (13681644).[117] For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Ch Nm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers.[118] Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.[119]
The earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.[121]
The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD),[122] appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others.[123] As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual.[122] The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of .[124][125][a] In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem.[125] All of these results are present in Babylonian mathematics, indicating Mesopotamian influence.[122] It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.[122]
Pini (c. 5th century BC) formulated the rules for Sanskrit grammar.[126] His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion.[127] Pingala (roughly 3rd1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system.[128][129] His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called mtrmeru).[130]
The next significant mathematical documents from India after the Sulba Sutras are the Siddhantas, astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence.[131] They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry.[132] Through a series of translation errors, the words "sine" and "cosine" derive from the Sanskrit "jiya" and "kojiya".[132]
Around 500 AD, Aryabhata wrote the Aryabhatiya, a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology.[133] Though about half of the entries are wrong, it is in the Aryabhatiya that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the Aryabhatiya as a "mix of common pebbles and costly crystals".[134]
In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in Brahma-sphuta-siddhanta, he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the HinduArabic numeral system.[135] It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the HinduArabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.[citation needed]
In the 12th century, Bhskara II[136] lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.[137]
In the 14th century, Madhava of Sangamagrama, the founder of the  Kerala School of Mathematics, found the MadhavaLeibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of  as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions.[138] In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the Yukti-bh.[139]
[140] It has been argued that the advances of the Kerala school, which laid the foundations of the calculus, were transmitted to Europe in the 16th century.[141] via Jesuit missionaries and traders who were active around the ancient port of Muziris at the time and, as a result, directly influenced later European developments in analysis and calculus.[142] However, other scholars argue that the Kerala School did not formulate a systematic theory of differentiation and integration, and that there is not any direct evidence of their results being transmitted outside Kerala.[143][144][145][146]
The Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.
In the 9th century, the Persian mathematician Muammad ibn Ms al-Khwrizm wrote an important book on the HinduArabic numerals and one on methods for solving equations. His book On the Calculation with Hindu Numerals, written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word algorithm is derived from the Latinization of his name, Algoritmi, and the word algebra from the title of one of his works, Al-Kitb al-mukhtaar f hsb al-abr wal-muqbala (The Compendious Book on Calculation by Completion and Balancing). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots,[147] and he was the first to teach algebra in an elementary form and for its own sake.[148] He also discussed the fundamental method of "reduction" and "balancing", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwrizm originally described as al-jabr.[149] His algebra was also no longer concerned "with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study." He also studied an equation for its own sake and "in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems."[150]
In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions.[151] His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.
Further developments in algebra were made by Al-Karaji in his treatise al-Fakhri, where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes.[152] The historian of mathematics, F. Woepcke,[153] praised Al-Karaji for being "the first who introduced the theory of algebraic calculus." Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.[154]
In the late 11th century, Omar Khayyam wrote Discussions of the Difficulties in Euclid, a book about what he perceived as flaws in Euclid's Elements, especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.[155]
In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of  to the 16th decimal place. Kashi also had an algorithm for calculating nth roots, which was a special case of the methods given many centuries later by Ruffini and Horner.
Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasd.[156]
During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.
In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics.[157] Maya numerals utilized a base of twenty, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures.[157] The Maya used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy.[157] While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Maya developed a standard symbol for it.[157]
Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's Timaeus and the biblical passage (in the Book of Wisdom) that God had ordered all things in measure, and number, and weight.[158]
Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term quadrivium to describe the study of arithmetic, geometry, astronomy, and music. He wrote De institutione arithmetica, a free translation from the Greek of Nicomachus's Introduction to Arithmetic; De institutione musica, also derived from Greek sources; and a series of excerpts from Euclid's Elements. His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.[159][160]
In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwrizm's The Compendious Book on Calculation by Completion and Balancing, translated into Latin by Robert of Chester, and the complete text of Euclid's Elements, translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona.[161][162]  These and other new sources sparked a renewal of mathematics.
Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the HinduArabic numerals on a trip to what is now Bjaa, Algeria with his merchant father.  (Europe was still using Roman numerals.)  There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of HinduArabic numerals was much more efficient and greatly facilitated commerce.  Leonardo wrote Liber Abaci in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it.  The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.
The 14th century saw the development of new mathematical concepts to investigate a wide range of problems.[163] One important contribution was development of mathematics of local motion.
Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R).[164] Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.[165]
One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed "by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant".[167]
Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that "a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]".[168]
Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled.[169] In a later mathematical commentary on Euclid's Elements, Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.[170]
During the Renaissance, the development of mathematics and of accounting were intertwined.[171] While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as abbaco in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.
Piero della Francesca (c. 14151492) wrote books on solid geometry and linear perspective, including De Prospectiva Pingendi (On Perspective for Painting), Trattato dAbaco (Abacus Treatise), and De quinque corporibus regularibus (On the Five Regular Solids).[172][173][174]
Luca Pacioli's Summa de Arithmetica, Geometria, Proportioni et Proportionalit (Italian: "Review of Arithmetic, Geometry, Ratio and Proportion") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, "Particularis de Computis et Scripturis" (Italian: "Details of Calculation and Recording"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons.[175] In Summa Arithmetica, Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. Summa Arithmetica was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.
In Italy, during the first half of the 16th century, Scipione del Ferro and Niccol Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book Ars Magna, together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his L'Algebra in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.
Simon Stevin's book De Thiende ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.
Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Regiomontanus's table of sines and cosines was published in 1533.[176]
During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.[177]
The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Brgi. Kepler succeeded in formulating mathematical laws of planetary motion.[178]
The analytic geometry developed by Ren Descartes (15961650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.
Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.[179]
In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th19th century.
The most influential mathematician of the 18th century was arguably Leonhard Euler (17071783). His contributions range from founding the study of graph theory with the Seven Bridges of Knigsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol i, and he popularized the use of the Greek letter 






{\displaystyle \pi }

 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.
Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.
Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (17771855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.
This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician Jnos Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.
The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science.
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.
Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and variste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (AbelRuffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.
Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.
In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.
The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Socit Mathmatique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.
In 1897, Hensel introduced p-adic numbers.
The 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.
In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.
Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gdel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.
Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the "enormous theorem"), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonn and Andr Weil, publishing under the pseudonym "Nicolas Bourbaki", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.[180]
Differential geometry came into its own when Albert Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincar had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and Ren Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.
Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.
The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rzsa Pter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.
At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus either addition or multiplication (but not both), was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gdel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gdel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.
One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (18871920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.
Paul Erds published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erds number of a mathematician. This describes the "collaborative distance" between a person and Paul Erds, as measured by joint authorship of mathematical papers.
Emmy Noether has been described by many as the most important woman in the history of mathematics.[181] She studied the theories of rings, fields, and algebras.
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long.[182] More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.
In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincar conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).
Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive toward open access publishing, first popularized by the arXiv.
There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is expanding exponentially.[citation needed]

Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, finance, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models. 
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.
Historically, applied mathematics consisted principally of applied analysis, most notably differential equations; approximation theory (broadly construed, to include representations, asymptotic methods, variational methods, and numerical analysis); and applied probability. These areas of mathematics related directly to the development of Newtonian physics, and in fact, the distinction between mathematicians and physicists was not sharply drawn before the mid-19th century. This history left a pedagogical legacy in the United States: until the early 20th century, subjects such as classical mechanics were often taught in applied mathematics departments at American universities rather than in physics departments, and fluid mechanics may still be taught in applied mathematics departments.[1] Engineering and computer science departments have traditionally made use of applied mathematics.
Today, the term "applied mathematics" is used in a broader sense. It includes the classical areas noted above as well as other areas that have become increasingly important in applications. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se. Sometimes, the term "applicable mathematics" is used to distinguish between the traditional applied mathematics that developed alongside physics and the many areas of mathematics that are applicable to real-world problems today.
There is no consensus as to what the various branches of applied mathematics are. Such categorizations are made difficult by the way mathematics and science change over time, and also by the way universities organize departments, courses, and degrees.
Many mathematicians distinguish between "applied mathematics, which is concerned with mathematical methods, and the "applications of mathematics" within science and engineering. A biologist using a population model and applying known mathematics would not be doing applied mathematics, but rather using it; however, mathematical biologists have posed problems that have stimulated the growth of pure mathematics. Mathematicians such as Poincar and Arnold deny the existence of "applied mathematics" and claim that there are only "applications of mathematics." Similarly, non-mathematicians blend applied mathematics and applications of mathematics. The use and development of mathematics to solve industrial problems is also called "industrial mathematics".[2]
The success of modern numerical mathematical methods and software has led to the emergence of computational mathematics, computational science, and computational engineering, which use high-performance computing for the simulation of phenomena and the solution of problems in the sciences and engineering. These are often considered interdisciplinary.
Historically, mathematics was most important in the natural sciences and engineering. However, since World War II, fields outside the physical sciences have spawned the creation of new areas of mathematics, such as game theory and social choice theory, which grew out of economic considerations. Further, the utilization and development of mathematical methods expanded into other areas leading to the creation of new fields such as mathematical finance and data science. 
The advent of the computer has enabled new applications: studying and using the new computer technology itself (computer science) to study problems arising in other areas of science (computational science) as well as the mathematics of computation (for example, theoretical computer science, computer algebra,[3][4][5][6] numerical analysis[7][8][9][10]). Statistics is probably the most widespread mathematical science used in the social sciences, but other areas of mathematics, most notably economics, are proving increasingly useful in these disciplines.
Academic institutions are not consistent in the way they group and label courses, programs, and degrees in applied mathematics. At some schools, there is a single mathematics department, whereas others have separate departments for Applied Mathematics and (Pure) Mathematics. It is very common for Statistics departments to be separated at schools with graduate programs, but many undergraduate-only institutions include statistics under the mathematics department.
Many applied mathematics programs (as opposed to departments) consist primarily of cross-listed courses and jointly appointed faculty in departments representing applications. Some Ph.D. programs in applied mathematics require little or no coursework outside mathematics, while others require substantial coursework in a specific area of application. In some respects this difference reflects the distinction between "application of mathematics" and "applied mathematics".
Some universities in the UK host departments of Applied Mathematics and Theoretical Physics,[11][12][13] but it is now much less common to have separate departments of pure and applied mathematics. A notable exception to this is the Department of Applied Mathematics and Theoretical Physics at the University of Cambridge, housing the Lucasian Professor of Mathematics whose past holders include Isaac Newton, Charles Babbage, James Lighthill, Paul Dirac and Stephen Hawking.
Schools with separate applied mathematics departments range from Brown University, which has a large Division of Applied Mathematics that offers degrees through the doctorate, to Santa Clara University, which offers only the M.S. in applied mathematics.[16] Research universities dividing their mathematics department into pure and applied sections include MIT. Brigham Young University also has an Applied and Computational Emphasis (ACME), a program that allows students to graduate with a Mathematics degree, with an emphasis in Applied Math. Students in this program also learn another skill (Computer Science, Engineering, Physics, Pure Math, etc.) to supplement their applied math skills.
Applied mathematics is associated with the following mathematical sciences:
Scientific computing includes applied mathematics (especially numerical analysis[7][8][9][10][17]), computing science (especially high-performance computing[18][19]), and mathematical modelling in a scientific discipline.
Computer science relies on logic, algebra, discrete mathematics such as graph theory,[20][21] and combinatorics.
Operations research[22] and management science are often taught in faculties of engineering, business, and public policy.
Applied mathematics has substantial overlap with the discipline of statistics. Statistical theorists study and improve statistical procedures with mathematics, and statistical research often raises mathematical questions. Statistical theory relies on probability and decision theory, and makes extensive use of scientific computing, analysis, and optimization; for the design of experiments, statisticians use algebra and combinatorial design. Applied mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities).
Actuarial science applies probability, statistics, and economic theory to assess risk in insurance, finance and other industries and professions.[23]
Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.[24][25][26] The applied methods usually refer to nontrivial mathematical techniques or approaches. Mathematical economics is based on statistics, probability, mathematical programming (as well as other computational methods), operations research, game theory, and some methods from mathematical analysis. In this regard, it resembles (but is distinct from) financial mathematics, another part of applied mathematics.[27]
According to the Mathematics Subject Classification (MSC), mathematical economics falls into the Applied mathematics/other classification of category 91:
with MSC2010 classifications for 'Game theory' at codes 91Axx and for 'Mathematical economics' at codes 91Bxx.
Applicable mathematics is a subdiscipline of applied mathematics, although there is no consensus as to a precise definition.[28] Sometimes the term "applicable mathematics" is used to distinguish between the traditional applied mathematics that developed alongside physics and the many areas of mathematics that are applicable to real-world problems today.
Mathematicians often distinguish between "applied mathematics" on the one hand, and the "applications of mathematics" or "applicable mathematics" both within and outside of science and engineering, on the other.[28] Some mathematicians emphasize the term applicable mathematics to separate or delineate the traditional applied areas from new applications arising from fields that were previously seen as pure mathematics.[29] For example, from this viewpoint, an ecologist or geographer using population models and applying known mathematics would not be doing applied, but rather applicable, mathematics. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se. Such descriptions can lead to applicable mathematics being seen as a collection of mathematical methods such as real analysis, linear algebra, mathematical modelling, optimisation, combinatorics, probability and statistics, which are useful in areas outside traditional mathematics and not specific to mathematical physics. 
Other authors prefer describing applicable mathematics as a union of "new" mathematical applications with the traditional fields of applied mathematics.[29][30][31] With this outlook, the terms applied mathematics and applicable mathematics are thus interchangeable.
The line between applied mathematics and specific areas of application is often blurred. Many universities teach mathematical and statistical courses outside the respective departments, in departments and areas including business, engineering, physics, chemistry, psychology, biology, computer science, scientific computation, and mathematical physics.

In mathematics, a function[note 1] is a binary relation between two sets that associates each element of the first set to exactly one element of the second set. Typical examples are functions from integers to integers, or from the real numbers to real numbers.
Functions were originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a function of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of a function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept.
A function is a process or a relation that associates each element x of a set X,  the domain of the function, to a single element y of another set Y (possibly the same set), the codomain of the function. It is customarily denoted by letters such as f, g and h.
If the function is called f, this relation is denoted by y = f(x) (which reads "f of x"), where the element x is the argument or input of the function, and y is the value of the function, the output, or the image of x by f.[1] The symbol that is used for representing the input is the variable of the function (e.g., f is a function of the variable x).[2]
A function is uniquely represented by the set of all pairs (x, f(x)), called the graph of the function.[note 2][3] When the domain and the codomain are sets of real numbers, each such pair may be thought of as the Cartesian coordinates of a point in the plane. The set of these points is called the graph of the function; it is a popular means of illustrating the function.
Functions are widely used in science, and in most fields of mathematics. It has been said that functions are "the central objects of investigation" in most fields of mathematics.[4]
Intuitively, a function is a process that associates each element of a set X, to a single element of a set Y.
Formally, a function f from a set X to a set Y is defined by a set G of ordered pairs (x, y) with x  X, y  Y, such that every element of X is the first component of exactly one ordered pair in G.[5][note 3] In other words, for every x in X, there is exactly one element y such that the ordered pair (x, y) belongs to the set of pairs defining the function f. The set G is called the graph of the function. Occasionally, it may be identified with the function, but this hides the usual interpretation of a function as a process. Therefore, in common usage, the function is generally distinguished from its graph.
Functions are also called maps or mappings, though some authors make some distinction between "maps" and "functions" (see Other terms).
The fact of f being a function from the set X to the set Y is formally denoted by f: XY.
In the definition of a function, X and Y are respectively called the domain and the codomain of the function f.[6] If (x, y) belongs to the set defining f, then y is the image of x under f, or the value of f applied to the argument x. In the context of numbers in particular, one also says that y is the value of f for the value x of its variable, or, more concisely, that y is the value of f of x, denoted as y = f(x).
Two functions f and g are equal, if their domain and codomain sets are the same and their output values agree on the whole domain. More formally, f = g if f(x) = g(x) for all x  X, where f:X  Y and g:X  Y.[7][8][note 4]
The domain and codomain are not always explicitly given when a function is defined, and, without some (possibly difficult) computation, one might only know that the domain is contained in a larger set. Typically, this occurs in mathematical analysis, where "a function from X to Y " often refers to a function that may have a proper subset[note 5] of X as domain. For example, a "function from the reals to the reals" may refer to a real-valued function of a real variable. However, a "function from the reals to the reals" does not mean that the domain of the function is the whole set of the real numbers, but only that the domain is a set of real numbers that contains a non-empty open interval. Such a function is then called a partial function. For example, if f is a function that has the real numbers as domain and codomain, then a function mapping the value x to the value g(x) = 1/f(x) is a function g from the reals to the reals, whose domain is the set of the reals x, such that f(x)  0.
The range of a function is the set of the images of all elements in the domain.[9][10][11][12] However, range is sometimes used as a synonym of codomain,[12][13] generally in old textbooks.[citation needed]
Any subset of the Cartesian product of two sets X and Y defines a binary relation R  X  Y between these two sets. It is immediate that an arbitrary relation may contain pairs that violate the necessary conditions for a function given above.
A binary relation is functional (also called right-unique) if
A binary relation is serial (also called left-total) if
A partial function is a binary relation that is functional.
A function is a binary relation that is functional and serial.
Various properties of functions and function composition may be reformulated in the language of relations.[14] For example, a function is injective if the converse relation RT  Y  X is functional, where the converse relation is defined as RT = {(y, x) | (x, y)  R}.
The set of all functions from some given domain to a codomain is sometimes identified with the Cartesian product of copies of the codomain, indexed by the domain. Namely, given sets X and Y, any function f: X  Y is an element of the Cartesian product of copies of Ys over the index set X
Viewing f as tuple with coordinates, then for each x  X, the xth coordinate of this tuple is the value f(x)  Y. This reflects the intuition that for each x  X, the function picks some element y  Y, namely, f(x). (This point of view is used for example in the discussion of a choice function.)
Infinite Cartesian products are often simply "defined" as sets of functions.[15]
There are various standard ways for denoting functions. The most commonly used notation is functional notation, which defines the function using an equation that gives the names of the function and the argument explicitly. This gives rise to a subtle point which is often glossed over in elementary treatments of functions: functions are distinct from their values. Thus, a function f should be distinguished from its value f(x0) at the value x0 in its domain. To some extent, even working mathematicians will conflate the two in informal settings for convenience, and to avoid appearing pedantic. However, strictly speaking, it is an abuse of notation to write "let 



f
:

R



R



{\displaystyle f\colon \mathbb {R} \to \mathbb {R} }

 be the function f(x) = x2 ", since f(x) and x2 should both be understood as the value of f at x, rather than the function itself. Instead, it is correct, though long-winded, to write "let 



f
:

R



R



{\displaystyle f\colon \mathbb {R} \to \mathbb {R} }

 be the function defined by the equation f(x) = x2 for all x in 




R



{\displaystyle \mathbb {R} }

".  A compact phrasing is "let 



f
:

R



R



{\displaystyle f\colon \mathbb {R} \to \mathbb {R} }

 with f(x) = x2," where the redundant "be the function" is omitted and, by convention, "for all 



x


{\displaystyle x}

 in the domain of 



f


{\displaystyle f}

" is understood.
This distinction in language and notation can become important, in cases where functions themselves serve as inputs for other functions.  (A function taking another function as an input is termed a functional.)  Other approaches of notating functions, detailed below, avoid this problem but are less commonly used.
As first used by Leonhard Euler in 1734,[16] functions are denoted by a symbol consisting generally of a single letter in italic font, most often the lower-case letters f, g, h. Some widely used functions are represented by a symbol consisting of several letters (usually two or three, generally an abbreviation of their name). In which case, a roman type is customarily used instead, such as "sin" for the sine function, in contrast to italic font for single-letter symbols.
The notation (read: "y equals f of x")
means that the pair (x, y) belongs to the set of pairs defining the function f. If X is the domain of f, the set of pairs defining the function is thus, using set-builder notation,
Often, a definition of the function is given by what f does to the explicit argument x.  For example, a function f can be defined by the equation
for all real numbers x. In this example, f can be thought of as the composite of several simpler functions: squaring, adding 1, and taking the sine. However, of these, only the sine function has a common explicit symbol (sin), whereas the combination of squaring and then adding 1 is described by the polynomial expression x2 + 1. In order to explicitly reference functions such as squaring or adding 1 without introducing new function names (e.g. by defining function g and h by g(x) = x2 and h(x) = x + 1), one of the methods below (arrow notation or dot notation) could be used.
When the symbol denoting the function consists of several characters and no ambiguity may arise, the parentheses of functional notation might be omitted. For example, it is common to write sin x instead of sin(x).
For explicitly expressing domain X and the codomain Y of a function f, the arrow notation is often used (read: "the function f from X to Y" or "the function f mapping elements of  X to elements of Y"):
or
This is often used in relation with the arrow notation for elements (read: "f maps x to f(x)"), often stacked immediately below the arrow notation giving the function symbol, domain, and codomain:
For example, if a multiplication is defined on a set X, then the square function sqr on X is unambiguously defined by (read: "the function sqr from X to X that maps x to x  x")
the latter line being more commonly written
Often, the expression giving the function symbol, domain and codomain is omitted.  Thus, the arrow notation is useful for avoiding introducing a symbol for a function that is defined, as it is often the case, by a formula expressing the value of the function in terms of its argument.  As a common application of the arrow notation, suppose 



f
:
X

X

Y
;

(
x
,
t
)

f
(
x
,
t
)


{\displaystyle f\colon X\times X\to Y;\;(x,t)\mapsto f(x,t)}

 is a two-argument function, and we want to refer to a partially applied function 



X

Y


{\displaystyle X\to Y}

 produced by fixing the second argument to the value t0 without introducing a new function name.  The map in question could be denoted 



x

f
(
x
,

t

0


)


{\displaystyle x\mapsto f(x,t_{0})}

 using the arrow notation for elements. The expression 



x

f
(
x
,

t

0


)


{\displaystyle x\mapsto f(x,t_{0})}

 (read: "the map taking x to f(x, t0)") represents this new function with just one argument, whereas the expression f(x0, t0) refers to the value of the function f at the point (x0, t0).
Index notation is often used instead of functional notation. That is, instead of writing f(x), one writes 




f

x


.


{\displaystyle f_{x}.}


This is typically the case for functions whose domain is the set of the natural numbers. Such a function is called a sequence, and, in this case the element 




f

n




{\displaystyle f_{n}}

 is called the nth element of sequence.
The index notation is also often used for distinguishing some variables called parameters from the "true variables". In fact, parameters are specific variables that are considered as being fixed during the study of a problem.  For example, the map 



x

f
(
x
,
t
)


{\displaystyle x\mapsto f(x,t)}

 (see above) would be denoted 




f

t




{\displaystyle f_{t}}

 using index notation, if we define the collection of maps 




f

t




{\displaystyle f_{t}}

 by the formula 




f

t


(
x
)
=
f
(
x
,
t
)


{\displaystyle f_{t}(x)=f(x,t)}

 for all 



x
,
t

X


{\displaystyle x,t\in X}

.
In the notation




x

f
(
x
)
,


{\displaystyle x\mapsto f(x),}


the symbol x does not represent any value, it is simply a placeholder meaning that, if x is replaced by any value on the left of the arrow, it should be replaced by the same value on the right of the arrow. Therefore, x may be replaced by any symbol, often an interpunct "  ". This may be useful for distinguishing the function f() from its value f(x) at x.
For example, 



a
(


)

2




{\displaystyle a(\cdot )^{2}}

 may stand for the function 



x

a

x

2




{\displaystyle x\mapsto ax^{2}}

, and 






a



(

)


f
(
u
)

d
u


{\textstyle \int _{a}^{\,(\cdot )}f(u)\,du}

 may stand for a function defined by an integral with variable upper bound: 



x




a


x


f
(
u
)

d
u


{\textstyle x\mapsto \int _{a}^{x}f(u)\,du}

.
There are other, specialized notations for functions in sub-disciplines of mathematics.  For example, in linear algebra and functional analysis, linear forms and the vectors they act upon are denoted using a dual pair to show the underlying duality.  This is similar to the use of braket notation in quantum mechanics.  In logic and the theory of computation, the function notation of lambda calculus is used to explicitly express the basic notions of function abstraction and application.  In category theory and homological algebra, networks of functions are described in terms of how they and their compositions commute with each other using commutative diagrams that extend and generalize the arrow notation for functions described above.
A function is often also called a map or a mapping, but some authors make a distinction between the term "map" and "function". For example, the term "map" is often reserved for a "function" with some sort of special structure (e.g. maps of manifolds). In particular map is often used in place of homomorphism for the sake of succinctness (e.g., linear map or map from G to H instead of group homomorphism from G to H). Some authors[24] reserve the word mapping for the case where the structure of the codomain belongs explicitly to the definition of the function.
Some authors, such as Serge Lang,[25] use "function" only to refer to maps for which the codomain is a subset of the real or complex numbers, and use the term mapping for more general functions.
In the theory of dynamical systems, a map denotes an evolution function used to create discrete dynamical systems. See also Poincar map.
Whichever definition of map is used, related terms like domain, codomain, injective, continuous have the same meaning as for a function.
Given a function 



f


{\displaystyle f}

, by definition, to each element 



x


{\displaystyle x}

 of the domain of the function 



f


{\displaystyle f}

, there is a unique element associated to it, the value 



f
(
x
)


{\displaystyle f(x)}

 of 



f


{\displaystyle f}

 at 



x


{\displaystyle x}

. There are several ways to specify or describe how 



x


{\displaystyle x}

 is related to 



f
(
x
)


{\displaystyle f(x)}

, both explicitly and implicitly. Sometimes, a theorem or an axiom asserts the existence of a function having some properties, without describing it more precisely. Often, the specification or description is referred to as the definition of the function 



f


{\displaystyle f}

.
On a finite set, a function may be defined by listing the elements of the codomain that are associated to the elements of the domain. For example, if 



A
=
{
1
,
2
,
3
}


{\displaystyle A=\{1,2,3\}}

, then one can define a function 



f
:
A


R



{\displaystyle f\colon A\to \mathbb {R} }

 by 



f
(
1
)
=
2
,
f
(
2
)
=
3
,
f
(
3
)
=
4.


{\displaystyle f(1)=2,f(2)=3,f(3)=4.}


Functions are often defined by a formula that describes a combination of arithmetic operations and previously defined functions; such a formula allows computing the value of the function from the value of any element of the domain.
For example, in the above example, 



f


{\displaystyle f}

 can be defined by the formula 



f
(
n
)
=
n
+
1


{\displaystyle f(n)=n+1}

, for 



n

{
1
,
2
,
3
}


{\displaystyle n\in \{1,2,3\}}

.
When a function is defined this way, the determination of its domain is sometimes difficult. If the formula that defines the function contains divisions, the values of the variable for which a denominator is zero must be excluded from the domain; thus, for a complicated function, the determination of the domain passes through the computation of the zeros of auxiliary functions. Similarly, if square roots occur in the definition of a function from 




R



{\displaystyle \mathbb {R} }

 to 




R

,


{\displaystyle \mathbb {R} ,}

 the domain is included in the set of the values of the variable for which the arguments of the square roots are nonnegative.
For example, 



f
(
x
)
=


1
+

x

2






{\displaystyle f(x)={\sqrt {1+x^{2}}}}

 defines a function 



f
:

R



R



{\displaystyle f\colon \mathbb {R} \to \mathbb {R} }

 whose domain is 




R

,


{\displaystyle \mathbb {R} ,}

 because 



1
+

x

2




{\displaystyle 1+x^{2}}

 is always positive if x is a real number. On the other hand, 



f
(
x
)
=


1


x

2






{\displaystyle f(x)={\sqrt {1-x^{2}}}}

 defines a function from the reals to the reals whose domain is reduced to the interval [1, 1]. (In old texts, such a domain was called the domain of definition of the function.)
Functions are often classified by the nature of formulas that define them:
A function 



f
:
X

Y
,


{\displaystyle f\colon X\to Y,}

 with domain X and codomain Y, is bijective, if for every y in Y, there is one and only one element x in X such that y = f(x). In this case, the inverse function of f is the function 




f


1


:
Y

X


{\displaystyle f^{-1}\colon Y\to X}

 that maps 



y

Y


{\displaystyle y\in Y}

 to the element 



x

X


{\displaystyle x\in X}

 such that y = f(x). For example, the natural logarithm is a bijective function from the positive real numbers to the real numbers. It thus has an inverse, called the exponential function, that maps the real numbers onto the positive numbers.
If a function 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 is not bijective, it may occur that one can select subsets 



E

X


{\displaystyle E\subseteq X}

 and 



F

Y


{\displaystyle F\subseteq Y}

 such that the restriction of f to E is a bijection from E to F, and has thus an inverse. The inverse trigonometric functions are defined this way. For example, the cosine function induces, by restriction, a bijection from the interval [0, ] onto the interval [1, 1], and its inverse function, called arccosine, maps [1, 1] onto [0, ]. The other inverse trigonometric functions are defined similarly.
More generally, given a binary relation R between two sets X and Y, let E be a subset of X such that, for every 



x

E
,


{\displaystyle x\in E,}

 there is some 



y

Y


{\displaystyle y\in Y}

 such that x R y. If one has a criterion allowing selecting such an y for every 



x

E
,


{\displaystyle x\in E,}

 this defines a function 



f
:
E

Y
,


{\displaystyle f\colon E\to Y,}

 called an implicit function, because it is implicitly defined by the relation R.
For example, the equation of the unit circle 




x

2


+

y

2


=
1


{\displaystyle x^{2}+y^{2}=1}

 defines a relation on real numbers. If 1 < x < 1 there are two possible values of y, one positive and one negative. For x =  1, these two values become both equal to 0. Otherwise, there is no possible value of y. This means that the equation defines two implicit functions with domain [1, 1] and respective codomains [0, +) and (, 0].
In this example, the equation can be solved in y, giving 



y
=



1


x

2




,


{\displaystyle y=\pm {\sqrt {1-x^{2}}},}

 but, in more complicated examples, this is impossible. For example, the relation 




y

5


+
x
+
1
=
0


{\displaystyle y^{5}+x+1=0}

 defines y as an implicit function of x, called the Bring radical, which has 




R



{\displaystyle \mathbb {R} }

 as domain and range. The Bring radical cannot be expressed in terms of the four arithmetic operations and nth roots.
The implicit function theorem provides mild differentiability conditions for existence and uniqueness of an implicit function in the neighborhood of a point.
Many functions can be defined as the antiderivative of another function. This is the case of the natural logarithm, which is the antiderivative of 1/x that is 0 for x = 1. Another common example is the error function.
More generally, many functions, including most special functions, can be defined as solutions of differential equations. The simplest example is probably the exponential function, which can be defined as the unique function that is equal to its derivative and takes the value 1 for x = 0.
Power series can be used to define functions on the domain in which they converge. For example, the exponential function is given by 




e

x


=



n
=
0








x

n



n
!





{\displaystyle e^{x}=\sum _{n=0}^{\infty }{x^{n} \over n!}}

. However, as the coefficients of a series are quite arbitrary, a function that is the sum of a convergent series is generally defined otherwise, and the sequence of the coefficients is the result of some computation based on another definition. Then, the power series can be used to enlarge the domain of the function. Typically, if a function for a real variable is the sum of its Taylor series in some interval, this power series allows immediately enlarging the domain to a subset of the complex numbers, the disc of convergence of the series. Then analytic continuation allows enlarging further the domain for including almost the whole complex plane. This process is the method that is generally used for defining the logarithm, the exponential and the trigonometric functions of a complex number.
Functions whose domain are the nonnegative integers, known as sequences, are often defined by recurrence relations.
The factorial function on the nonnegative integers (



n

n
!


{\displaystyle n\mapsto n!}

) is a basic example, as it can be defined by the recurrence relation
and the initial condition
A graph is commonly used to give an intuitive picture of a function.  As an example of how a graph helps to understand a function, it is easy to see from its graph whether a function is increasing or decreasing. Some functions may also be represented by bar charts.
Given a function 



f
:
X

Y
,


{\displaystyle f\colon X\to Y,}

 its graph is, formally, the set
In the frequent case where X and Y are subsets of the real numbers (or may be identified with such subsets, e.g. intervals), an element 



(
x
,
y
)

G


{\displaystyle (x,y)\in G}

 may be identified with a point having coordinates x, y in a 2-dimensional coordinate system, e.g. the Cartesian plane. Parts of this may create a plot that represents (parts of) the function. The use of plots is so ubiquitous that they too are called the graph of the function. Graphic representations of functions are also possible in other coordinate systems. For example, the graph of the square function
consisting of all points with coordinates 



(
x
,

x

2


)


{\displaystyle (x,x^{2})}

 for 



x


R

,


{\displaystyle x\in \mathbb {R} ,}

 yields, when depicted in Cartesian coordinates, the well known parabola. If the same quadratic function 



x


x

2


,


{\displaystyle x\mapsto x^{2},}

 with the same formal graph, consisting of pairs of numbers, is plotted instead in polar coordinates 



(
r
,

)
=
(
x
,

x

2


)
,


{\displaystyle (r,\theta )=(x,x^{2}),}

 the plot obtained is Fermat's spiral.
A function can be represented as a table of values.  If the domain of a function is finite, then the function can be completely specified in this way.  For example, the multiplication function 



f
:
{
1
,

,
5

}

2




R



{\displaystyle f\colon \{1,\ldots ,5\}^{2}\to \mathbb {R} }

 defined as 



f
(
x
,
y
)
=
x
y


{\displaystyle f(x,y)=xy}

 can be represented by the familiar multiplication table
On the other hand, if a function's domain is continuous, a table can give the values of the function at specific values of the domain.  If an intermediate value is needed, interpolation can be used to estimate the value of the function.  For example, a portion of a table for the sine function might be given as follows, with values rounded to 6 decimal places:
Before the advent of handheld calculators and personal computers, such tables were often compiled and published for functions such as logarithms and trigonometric functions.
Bar charts are often used for representing functions whose domain is a finite set, the natural numbers, or the integers. In this case, an element x of the domain is represented by an interval of the x-axis, and the corresponding value of the function, f(x), is represented by a rectangle whose base is the interval corresponding to x and whose height is f(x) (possibly negative, in which case the bar extends below the x-axis).
This section describes general properties of functions, that are independent of specific properties of the domain and the codomain.
There are a number of standard functions that occur frequently:
Given two functions 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 and 



g
:
Y

Z


{\displaystyle g\colon Y\to Z}

 such that the domain of g is the codomain of f, their composition is the function 



g

f
:
X

Z


{\displaystyle g\circ f\colon X\rightarrow Z}

 defined by
That is, the value of 



g

f


{\displaystyle g\circ f}

 is obtained by first applying f to x to obtain y = f(x) and then applying g to the result y to obtain g(y) = g(f(x)). In the notation the function that is applied first is always written on the right.
The composition 



g

f


{\displaystyle g\circ f}

 is an operation on functions that is defined only if the codomain of the first function is the domain of the second one. Even when both 



g

f


{\displaystyle g\circ f}

 and 



f

g


{\displaystyle f\circ g}

 satisfy these conditions, the composition is not necessarily commutative, that is, the functions 



g

f


{\displaystyle g\circ f}

 and 



f

g


{\displaystyle f\circ g}

 need not be equal, but may deliver different values for the same argument. For example, let f(x) = x2 and g(x) = x + 1, then 



g
(
f
(
x
)
)
=

x

2


+
1


{\displaystyle g(f(x))=x^{2}+1}

 and 



f
(
g
(
x
)
)
=
(
x
+
1

)

2




{\displaystyle f(g(x))=(x+1)^{2}}

 agree just for 



x
=
0.


{\displaystyle x=0.}


The function composition is associative in the sense that, if one of 



(
h

g
)

f


{\displaystyle (h\circ g)\circ f}

 and 



h

(
g

f
)


{\displaystyle h\circ (g\circ f)}

 is defined, then the other is also defined, and they are equal. Thus, one writes
The identity functions 




id

X




{\displaystyle \operatorname {id} _{X}}

 and 




id

Y




{\displaystyle \operatorname {id} _{Y}}

 are respectively a right identity and a left identity for functions from X to Y. That is, if f is a function with domain X, and codomain Y, one has




f


id

X


=

id

Y



f
=
f
.


{\displaystyle f\circ \operatorname {id} _{X}=\operatorname {id} _{Y}\circ f=f.}


A composite function g(f(x)) can be visualized as the combination of two "machines".
A simple example of a function composition
Another composition. In this example, (gf)(c) = #.
Let 



f
:
X

Y
.


{\displaystyle f\colon X\to Y.}

 The image under f of an element x of the domain X is f(x).[9] If A is any subset of X, then the image of A under f, denoted f(A), is the subset of the codomain Y consisting of all images of elements of A,[9] that is,
The image of f is the image of the whole domain, that is, f(X).[13] It is also called the range of f,[9][10][11][12] although the term range may also refer to the codomain.[12][13][26]
On the other hand, the inverse image or preimage under f of an element y of the codomain Y is the set of all elements of the domain X whose images under f equal y.[9] In symbols, the preimage of y is denoted by 




f


1


(
y
)


{\displaystyle f^{-1}(y)}

 and is given by the equation
Likewise, the preimage of a subset B of the codomain Y is the set of the preimages of the elements of B, that is, it is the subset of the domain X consisting of all elements of X whose images belong to B.[9] It is denoted by 




f


1


(
B
)


{\displaystyle f^{-1}(B)}

 and is given by the equation
For example, the preimage of 



{
4
,
9
}


{\displaystyle \{4,9\}}

 under the square function is the set 



{

3
,

2
,
2
,
3
}


{\displaystyle \{-3,-2,2,3\}}

.
By definition of a function, the image of an element x of the domain is always a single element of the codomain. However, the preimage 




f


1


(
y
)


{\displaystyle f^{-1}(y)}

 of an element y of the codomain may be empty or contain any number of elements. For example, if f is the function from the integers to themselves that maps every integer to 0, then 




f


1


(
0
)
=

Z



{\displaystyle f^{-1}(0)=\mathbb {Z} }

.
If 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 is a function, A and B are subsets of X, and C and D are subsets of Y, then one has the following properties:
The preimage by f of an element y of the codomain is sometimes called, in some contexts, the fiber of y under f.
If a function f has an inverse (see below), this inverse is denoted 




f


1


.


{\displaystyle f^{-1}.}

 In this case 




f


1


(
C
)


{\displaystyle f^{-1}(C)}

 may denote either the image by 




f


1




{\displaystyle f^{-1}}

 or the preimage by f of C. This is not a problem, as these sets are equal. The notation 



f
(
A
)


{\displaystyle f(A)}

 and 




f


1


(
C
)


{\displaystyle f^{-1}(C)}

 may be ambiguous in the case of sets that contain some subsets as elements, such as 



{
x
,
{
x
}
}
.


{\displaystyle \{x,\{x\}\}.}

 In this case, some care may be needed, for example, by using square brackets 



f
[
A
]
,

f


1


[
C
]


{\displaystyle f[A],f^{-1}[C]}

 for images and preimages of subsets and ordinary parentheses for images and preimages of elements.
Let 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 be a function.
The function f is injective (or one-to-one, or is an injection) if f(a)  f(b) for any two different elements a and b of X.[13][27] Equivalently, f is injective if and only if, for any 



y

Y
,


{\displaystyle y\in Y,}

 the preimage 




f


1


(
y
)


{\displaystyle f^{-1}(y)}

 contains at most one element. An empty function is always injective. If X is not the empty set, then f is injective if and only if there exists a function 



g
:
Y

X


{\displaystyle g\colon Y\to X}

 such that 



g

f
=

id

X


,


{\displaystyle g\circ f=\operatorname {id} _{X},}

 that is, if f has a left inverse.[27] Proof: If f is injective, for defining g, one chooses an element 




x

0




{\displaystyle x_{0}}

 in X (which exists as X is supposed to be nonempty),[note 8] and one defines g by 



g
(
y
)
=
x


{\displaystyle g(y)=x}

 if 



y
=
f
(
x
)


{\displaystyle y=f(x)}

 and 



g
(
y
)
=

x

0




{\displaystyle g(y)=x_{0}}

 if 



y

f
(
X
)
.


{\displaystyle y\not \in f(X).}

 Conversely, if 



g

f
=

id

X


,


{\displaystyle g\circ f=\operatorname {id} _{X},}

 and 



y
=
f
(
x
)
,


{\displaystyle y=f(x),}

  then 



x
=
g
(
y
)
,


{\displaystyle x=g(y),}

 and thus 




f


1


(
y
)
=
{
x
}
.


{\displaystyle f^{-1}(y)=\{x\}.}


The function f is surjective (or onto, or is a surjection) if its range 



f
(
X
)


{\displaystyle f(X)}

 equals its codomain 



Y


{\displaystyle Y}

, that is, if, for each element 



y


{\displaystyle y}

 of the codomain, there exists some element 



x


{\displaystyle x}

 of the domain such that 



f
(
x
)
=
y


{\displaystyle f(x)=y}

 (in other words, the preimage 




f


1


(
y
)


{\displaystyle f^{-1}(y)}

 of every 



y

Y


{\displaystyle y\in Y}

 is nonempty).[13][28] If, as usual in modern mathematics, the axiom of choice is assumed, then f is surjective if and only if there exists a function 



g
:
Y

X


{\displaystyle g\colon Y\to X}

 such that 



f

g
=

id

Y


,


{\displaystyle f\circ g=\operatorname {id} _{Y},}

 that is, if f has a right inverse.[28] The axiom of choice is needed, because, if f is surjective, one defines g by 



g
(
y
)
=
x
,


{\displaystyle g(y)=x,}

 where 



x


{\displaystyle x}

 is an arbitrarily chosen element of 




f


1


(
y
)
.


{\displaystyle f^{-1}(y).}


The function f is bijective (or is a bijection or a one-to-one correspondence) if it is both injective and surjective.[13][29] That is, f is bijective if, for any 



y

Y
,


{\displaystyle y\in Y,}

 the preimage 




f


1


(
y
)


{\displaystyle f^{-1}(y)}

 contains exactly one element. The function f is bijective if and only if it admits an inverse function, that is, a function 



g
:
Y

X


{\displaystyle g\colon Y\to X}

 such that 



g

f
=

id

X




{\displaystyle g\circ f=\operatorname {id} _{X}}

 and 



f

g
=

id

Y


.


{\displaystyle f\circ g=\operatorname {id} _{Y}.}

[29] (Contrarily to the case of surjections, this does not require the axiom of choice; the proof is straightforward).
Every function 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 may be factorized as the composition 



i

s


{\displaystyle i\circ s}

 of a surjection followed by an injection, where s is the canonical surjection of X onto f(X) and i is the canonical injection of f(X) into Y. This is the canonical factorization of f.
"One-to-one" and "onto" are terms that were more common in the older English language literature; "injective", "surjective", and "bijective" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English.[citation needed]  As a word of caution, "a one-to-one function" is one that is injective, while a "one-to-one correspondence" refers to a bijective function.  Also, the statement "f maps X onto Y" differs from "f  maps X into B", in that the former implies that f is surjective, while the latter makes no assertion about the nature of f. In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage of being more symmetrical.
If 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 is a function and S is a subset of X, then the restriction of 



f


{\displaystyle f}

 to S, denoted 



f


|


S




{\displaystyle f|_{S}}

, is the function from S to Y defined by
for all x in S. Restrictions can be used to define partial inverse functions: if there is a subset S of the domain of a function 



f


{\displaystyle f}

 such that 



f


|


S




{\displaystyle f|_{S}}

 is injective, then the canonical surjection of 



f


|


S




{\displaystyle f|_{S}}

 onto its image 



f


|


S


(
S
)
=
f
(
S
)


{\displaystyle f|_{S}(S)=f(S)}

 is a bijection, and thus has an inverse function from 



f
(
S
)


{\displaystyle f(S)}

 to S. One application is the definition of inverse trigonometric functions. For example, the cosine function is injective when restricted to the interval [0, ]. The image of this restriction is the interval [1, 1], and thus the restriction has an inverse function from [1, 1] to [0, ], which is called arccosine and is denoted arccos.
Function restriction may also be used for "gluing" functions together. Let 



X
=



i

I



U

i




{\textstyle X=\bigcup _{i\in I}U_{i}}

 be the decomposition of X as a union of subsets, and suppose that a function 




f

i


:

U

i



Y


{\displaystyle f_{i}\colon U_{i}\to Y}

 is defined on each 




U

i




{\displaystyle U_{i}}

 such that for each pair 



i
,
j


{\displaystyle i,j}

 of indices, the restrictions of 




f

i




{\displaystyle f_{i}}

 and 




f

j




{\displaystyle f_{j}}

 to 




U

i




U

j




{\displaystyle U_{i}\cap U_{j}}

 are equal. Then this defines a unique function 



f
:
X

Y


{\displaystyle f\colon X\to Y}

 such that 



f


|



U

i




=

f

i




{\displaystyle f|_{U_{i}}=f_{i}}

 for all i. This is the way that functions on manifolds are defined.
An extension of a function f is a function g such that f is a restriction of g. A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.
Here is another classical example of a function extension that is encountered when studying homographies of the real line. A homography is a function 



h
(
x
)
=



a
x
+
b


c
x
+
d





{\displaystyle h(x)={\frac {ax+b}{cx+d}}}

 such that ad  bc  0. Its domain is the set of all real numbers different from 




d

/

c
,


{\displaystyle -d/c,}

 and its image is the set of all real numbers different from 



a

/

c
.


{\displaystyle a/c.}

 If one extends the real line to the projectively extended real line by including , one may extend h to a bijection from the extended real line to itself by setting 



h
(

)
=
a

/

c


{\displaystyle h(\infty )=a/c}

 and 



h
(

d

/

c
)
=



{\displaystyle h(-d/c)=\infty }

.
A multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time travelled and its average speed.
More formally, a function of n variables is a function whose domain is a set of n-tuples.
For example, multiplication of integers is a function of two variables, or bivariate function, whose domain is the set of all pairs (2-tuples) of integers, and whose codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.
The Cartesian product 




X

1






X

n




{\displaystyle X_{1}\times \cdots \times X_{n}}

 of n sets 




X

1


,

,

X

n




{\displaystyle X_{1},\ldots ,X_{n}}

 is the set of all n-tuples 



(

x

1


,

,

x

n


)


{\displaystyle (x_{1},\ldots ,x_{n})}

 such that 




x

i




X

i




{\displaystyle x_{i}\in X_{i}}

 for every i with 



1

i

n


{\displaystyle 1\leq i\leq n}

. Therefore, a function of n variables is a function
where the domain U has the form
When using function notation, one usually omits the parentheses surrounding tuples, writing 



f
(

x

1


,

x

2


)


{\displaystyle f(x_{1},x_{2})}

 instead of 



f
(
(

x

1


,

x

2


)
)
.


{\displaystyle f((x_{1},x_{2})).}


In the case where all the 




X

i




{\displaystyle X_{i}}

 are equal to the set 




R



{\displaystyle \mathbb {R} }

 of real numbers, one has a function of several real variables. If the 




X

i




{\displaystyle X_{i}}

 are equal to the set 




C



{\displaystyle \mathbb {C} }

 of complex numbers, one has a function of several complex variables.
It is common to also consider functions whose codomain is a product of sets. For example, Euclidean division maps every pair (a, b) of integers with b  0 to a pair of integers called the quotient and the remainder:
The codomain may also be a vector space. In this case, one talks of a vector-valued function. If the domain is contained in a Euclidean space, or more generally a manifold, a vector-valued function is often called a vector field.
The idea of function, starting in the 17th century, was fundamental to the new infinitesimal calculus (see History of the function concept). At that time, only real-valued functions of a real variable were considered, and all functions were assumed to be smooth. But the definition was soon extended to functions of several variables and to functions of a complex variable. In the second half of the 19th century, the mathematically rigorous definition of a function was introduced, and functions with arbitrary domains and codomains were defined.
Functions are now used throughout all areas of mathematics. In introductory calculus, when the word function is used without qualification, it means a real-valued function of a single real variable. The more general definition of a function is usually introduced to second or third year college students with STEM majors, and in their senior year they are introduced to calculus in a larger, more rigorous setting in courses such as real analysis and complex analysis.
A real function is a real-valued function of a real variable, that is, a function whose codomain is the field of real numbers and whose domain is a set of real numbers that contains an interval. In this section, these functions are simply called functions.
The functions that are most commonly considered in mathematics and its applications have some regularity, that is they are continuous, differentiable, and even analytic. This regularity insures that these functions can be visualized by their graphs. In this section, all functions are differentiable in some interval.
Functions enjoy pointwise operations, that is, if f and g are functions, their sum, difference and product are functions defined by
The domains of the resulting functions are the intersection of the domains of f and g. The quotient of two functions is defined similarly by
but the domain of the resulting function is obtained by removing the zeros of g from the intersection of the domains of f and g.
The polynomial functions are defined by polynomials, and their domain is the whole set of real numbers. They include constant functions, linear functions and quadratic functions. Rational functions are quotients of two polynomial functions, and their domain is the real numbers with a finite number of them removed to avoid division by zero. The simplest rational function is the function 



x



1
x


,


{\displaystyle x\mapsto {\frac {1}{x}},}

 whose graph is a hyperbola, and whose domain is the whole real line except for 0.
The derivative of a real differentiable function is a real function. An antiderivative of a continuous real function is a real function that has the original function as a derivative. For example, the function 



x



1
x




{\displaystyle x\mapsto {\frac {1}{x}}}

 is continuous, and even differentiable, on the positive real numbers. Thus one antiderivative, which takes the value zero for x = 1, is a differentiable function called the natural logarithm.
A real function f is monotonic in an interval if the sign of 






f
(
x
)

f
(
y
)


x

y





{\displaystyle {\frac {f(x)-f(y)}{x-y}}}

 does not depend of the choice of x and y in the interval. If the function is differentiable in the interval, it is monotonic if the sign of the derivative is constant in the interval. If a real function f is monotonic in an interval I, it has an inverse function, which is a real function with domain f(I) and image I. This is how inverse trigonometric functions are defined in terms of trigonometric functions, where the trigonometric functions are monotonic. Another example: the natural logarithm is monotonic on the positive real numbers, and its image is the whole real line; therefore it has an inverse function that is a bijection between the real numbers and the positive real numbers. This inverse is the exponential function.
Many other real functions are defined either by the implicit function theorem (the inverse function is a particular instance) or as solutions of differential equations. For example, the sine and the cosine functions are the solutions of the linear differential equation
such that
When the elements of the codomain of a function are vectors, the function is said to be a vector-valued function. These functions are particularly useful in applications, for example modeling physical properties. For example, the function that associates to each point of a fluid its velocity vector is a vector-valued function.
Some vector-valued functions are defined on a subset of 





R


n




{\displaystyle \mathbb {R} ^{n}}

 or other spaces that share geometric or topological properties of 





R


n




{\displaystyle \mathbb {R} ^{n}}

, such as manifolds. These vector-valued functions are given the name vector fields.
In mathematical analysis, and more specifically in functional analysis, a function space is a set of scalar-valued or vector-valued functions, which share a specific property and form a topological vector space. For example, the real smooth functions with a compact support (that is, they are zero outside some compact set) form a function space that is at the basis of the theory of distributions.
Function spaces play a fundamental role in advanced mathematical analysis, by allowing the use of their algebraic and topological properties for studying properties of functions. For example, all theorems of existence and uniqueness of solutions of ordinary or partial differential equations result of the study of function spaces.
Several methods for specifying functions of real or complex variables start from a local definition of the function at a point or on a neighbourhood of a point, and then extend by continuity the function to a much larger domain. Frequently, for a starting point 




x

0


,


{\displaystyle x_{0},}

 there are several possible starting values for the function.
For example, in defining the square root as the inverse function of the square function, for any positive real number 




x

0


,


{\displaystyle x_{0},}

 there are two choices for the value of the square root, one of which is positive and denoted 






x

0




,


{\displaystyle {\sqrt {x_{0}}},}

 and another which is negative and denoted 







x

0




.


{\displaystyle -{\sqrt {x_{0}}}.}

 These choices define two continuous functions, both having the nonnegative real numbers as a domain, and having either the nonnegative or the nonpositive real numbers as images. When looking at the graphs of these functions, one can see that, together, they form a single smooth curve. It is therefore often useful to consider these two square root functions as a single function that has two values for positive x, one value for 0 and no value for negative x.
In the preceding example, one choice, the positive square root, is more natural than the other. This is not the case in general. For example, let consider the implicit function that maps y to a root x of 




x

3



3
x

y
=
0


{\displaystyle x^{3}-3x-y=0}

 (see the figure on the right). For y = 0 one may choose either 



0
,


3


,

or




3




{\displaystyle 0,{\sqrt {3}},{\text{ or }}-{\sqrt {3}}}

 for x. By the implicit function theorem, each choice defines a function; for the first one, the (maximal) domain is the interval [2, 2] and the image is [1, 1]; for the second one, the domain is [2, ) and the image is [1, ); for the last one, the domain is (, 2] and the image is (, 1]. As the three graphs together form a smooth curve, and there is no reason for preferring one choice, these three functions are often considered as a single multi-valued function of y that has three values for 2 < y < 2, and only one value for y  2 and y  2.
Usefulness of the concept of multi-valued functions is clearer when considering complex functions, typically analytic functions. The domain to which a complex function may be extended by analytic continuation generally consists of almost the whole complex plane. However, when extending the domain through two different paths, one often gets different values. For example, when extending the domain of the square root function, along a path of complex numbers with positive imaginary parts, one gets i for the square root of 1; while, when extending through complex numbers with negative imaginary parts, one gets i. There are generally two ways of solving the problem. One may define a function that is not continuous along some curve, called a branch cut. Such a function is called the principal value of the function. The other way is to consider that one has a multi-valued function, which is analytic everywhere except for isolated singularities, but whose value may "jump" if one follows a closed loop around a singularity. This jump is called the monodromy.
The definition of a function that is given in this article requires the concept of set, since the domain and the codomain of a function must be a set. This is not a problem in usual mathematics, as it is generally not difficult to consider only functions whose domain and codomain are sets, which are well defined, even if the domain is not explicitly defined. However, it is sometimes useful to consider more general functions.
For example, the singleton set may be considered as a function 



x

{
x
}
.


{\displaystyle x\mapsto \{x\}.}

 Its domain would include all sets, and therefore would not be a set. In usual mathematics, one avoids this kind of problem by specifying a domain, which means that one has many singleton functions. However, when establishing foundations of mathematics, one may have to use functions whose domain, codomain or both are not specified, and some authors, often logicians, give precise definition for these weakly specified functions.[30]
These generalized functions may be critical in the development of a formalization of the foundations of mathematics. For example, Von NeumannBernaysGdel set theory, is an extension of the set theory in which the collection of all sets is a class. This theory includes the replacement axiom, which may be stated as: If X is a set and F is a function, then F[X] is a set.
In computer programming, a function is, in general, a piece of a computer program, which implements the abstract concept of function. That is, it is a program unit that produces an output for each input. However, in many programming languages every subroutine is called a function, even when there is no output, and when the functionality consists simply of modifying some data in the computer memory.
Functional programming is the programming paradigm consisting of building programs by using only subroutines that behave like mathematical functions. For example, if_then_else is a function that takes three functions as arguments, and, depending on the result of the first function (true or false), returns the result of either the second or the third function. An important advantage of functional programming is that it makes easier program proofs, as being based on a well founded theory, the lambda calculus (see below).
Except for computer-language terminology, "function" has the usual mathematical meaning in computer science. In this area, a property of major interest is the computability of a function. For giving a precise meaning to this concept, and to the related concept of algorithm, several models of computation have been introduced, the old ones being general recursive functions, lambda calculus and Turing machine. The fundamental theorem of computability theory is that these three models of computation define the same set of computable functions, and that all the other models of computation that have ever been proposed define the same set of computable functions or a smaller one. The ChurchTuring thesis is the claim that every philosophically acceptable definition of a computable function defines also the same functions.
General recursive functions are partial functions from integers to integers that can be defined from 
via the operators
Although defined only for functions from integers to integers, they can model any computable function as a consequence of the following properties:
Lambda calculus is a theory that defines computable functions without using set theory, and is the theoretical background of functional programming. It consists of terms that are either variables, function definitions (-terms), or applications of functions to terms. Terms are manipulated through some rules, (the -equivalence, the -reduction, and the -conversion), which are the axioms of the theory and may be interpreted as rules of computation.
In its original form, lambda calculus does not include the concepts of domain and codomain of a function. Roughly speaking, they have been introduced in the theory under the name of type in typed lambda calculus. Most kinds of typed lambda calculi can define fewer functions than untyped lambda calculus.

In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.
For example, 
Without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents composition of linear maps.
Not all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices.[1] This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such.
Square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant.
In geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this involves often to compute with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.
A matrix is a rectangular array of numbers (or other mathematical objects) for which operations such as addition and multiplication are defined.[2] Most commonly, a matrix over a field F is a rectangular array of scalars, each of which is a member of F.[3][4] A real matrix and a complex matrix are matrices whose entries are respectively real numbers or complex numbers. More general types of entries are discussed below. For instance, this is a real matrix:
The numbers, symbols, or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.
The size of a matrix is defined by the number of rows and columns it contains. There is no limit to the numbers of rows and columns a matrix (in the usual sense) can have as long as they are positive integers. A matrix with m rows and n columns is called an mn matrix, or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 32 matrix.
Matrices with a single row are called row vectors, and those with a single column are called column vectors. A matrix with the same number of rows and columns is called a square matrix.[5] A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.
Matrices are commonly written in box brackets or parentheses:
The specifics of symbolic matrix notation vary widely, with some prevailing trends. Matrices are usually symbolized using upper-case letters (such as A in the examples above), while the corresponding lower-case letters, with two subscript indices (e.g., a11, or a1,1), represent the entries. In addition to using upper-case letters to symbolize matrices, many authors use a special typographical style, commonly boldface upright (non-italic), to further distinguish matrices from other mathematical objects. An alternative notation involves the use of a double-underline with the variable name, with or without boldface style (as in the case of 






A
_

_




{\displaystyle {\underline {\underline {A}}}}

).
The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):
Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by the formula aij = i  j.
In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parentheses. For example, the matrix above is defined as A = [ij], or A = ((ij)). If matrix size is m  n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or indicated using m  n as a subscript. For instance, the matrix A above is 3  4, and can be defined as A = [i  j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i  j]34.
Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m--n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0  i  m  1 and 0  j  n  1.[6] This article follows the more common convention in mathematical writing where enumeration starts from 1.
An asterisk is occasionally used to refer to whole rows or columns in a matrix. For example, ai, refers to the ith row of A, and a,j refers to the jth column of A. 
The set of all m-by-n real matrices is often denoted 





M


(
m
,
n
)
,


{\displaystyle {\mathcal {M}}(m,n),}

 or 






M



m

n



R

.


{\displaystyle {\mathcal {M}}_{m\times n}\mathbb {R} .}

 The set of all m-by-n matrices matrices over another field or over a ring R, is similarly denoted 





M


(
m
,
n
,
R
)
,


{\displaystyle {\mathcal {M}}(m,n,R),}

 or 






M



m

n


(
R
)
.


{\displaystyle {\mathcal {M}}_{m\times n}(R).}

 If m = n, that is, in the case of square matrices, one does not repeat the dimension: 





M


(
n
,
R
)
,


{\displaystyle {\mathcal {M}}(n,R),}

 or 






M



n


(
R
)
.


{\displaystyle {\mathcal {M}}_{n}(R).}

[7] Often, 



M


{\displaystyle M}

 is used in place of 





M


.


{\displaystyle {\mathcal {M}}.}


There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.[9]






[



1


3


1




1


0


0



]


+


[



0


0


5




7


5


0



]


=


[



1
+
0


3
+
0


1
+
5




1
+
7


0
+
5


0
+
0



]


=


[



1


3


6




8


5


0



]




{\displaystyle {\begin{bmatrix}1&3&1\\1&0&0\end{bmatrix}}+{\begin{bmatrix}0&0&5\\7&5&0\end{bmatrix}}={\begin{bmatrix}1+0&3+0&1+5\\1+7&0+5&0+0\end{bmatrix}}={\begin{bmatrix}1&3&6\\8&5&0\end{bmatrix}}}


This operation is called scalar multiplication, but its result is not named "scalar product" to avoid confusion, since "scalar product" is sometimes used as a synonym for "inner product".
Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, that is, the matrix sum does not depend on the order of the summands: A+B=B+A.[10]
The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A+B)T=AT+BT. Finally, (AT)T=A.
Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:[11]
where 1  i  m and 1  j  p.[12] For example, the underlined entry 2340 in the product is calculated as (2  1000) + (3  100) + (4  10) = 2340:
Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA + CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[13] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m  k. Even if both products are defined, they generally need not be equal, that is:
In other words, matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers, whose product is independent of the order of the factors.[11] An example of two matrices not commuting with each other is:
whereas
Besides the ordinary matrix multiplication just described, other less frequently used operations on matrices that can be considered forms of multiplication also exist, such as the Hadamard product and the Kronecker product.[14] They arise in solving matrix equations such as the Sylvester equation.
There are three types of row operations:
These operations are used in several ways, including solving linear equations and finding matrix inverses.
A submatrix of a matrix is obtained by deleting any collection of rows and/or columns.[15][16][17] For example, from the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:
The minors and cofactors of a matrix are found by computing the determinant of certain submatrices.[17][18]
A principal submatrix is a square submatrix obtained by removing certain rows and columns. The definition varies from author to author. According to some authors, a principal submatrix is a submatrix in which the set of row indices that remain is the same as the set of column indices that remain.[19][20] Other authors define a principal submatrix as one in which the first k rows and columns, for some number k, are the ones that remain;[21] this type of submatrix has also been called a leading principal submatrix.[22]
Matrices can be used to compactly write and work with multiple linear equations, that is, systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (that is, n1-matrix) of n variables x1, x2, ..., xn, and b is an m1-column vector, then the matrix equation
is equivalent to the system of linear equations[23]
Using matrices, this can be solved more compactly than would be possible by writing out all the equations separately. If n = m and the equations are independent, then this can be done by writing
where A1 is the inverse matrix of A. If A has no inverse, solutionsif anycan be found using its generalized inverse.
Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn  Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn  Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.
For example, the 22 matrix
can be viewed as the transform of the unit square into a parallelogram with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d). The parallelogram pictured at the right is obtained by multiplying A with each of the column vectors 





[



0




0



]


,


[



1




0



]


,


[



1




1



]




{\displaystyle {\begin{bmatrix}0\\0\end{bmatrix}},{\begin{bmatrix}1\\0\end{bmatrix}},{\begin{bmatrix}1\\1\end{bmatrix}}}

, and 





[



0




1



]




{\displaystyle {\begin{bmatrix}0\\1\end{bmatrix}}}

 in turn. These vectors define the vertices of the unit square.
The following table shows several 22 real matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.
Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps:[24] if a k-by-m matrix B represents another linear map g: Rm  Rk, then the composition g  f is represented by BA since
The last equality follows from the above-mentioned associativity of matrix multiplication.
The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors.[25] Equivalently it is the dimension of the image of the linear map represented by A.[26] The ranknullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.[27]
A square matrix is a matrix with the same number of rows and columns.[5] An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied.
The entries aii form the main diagonal of a square matrix. They lie on the imaginary line that runs from the top left corner to the bottom right corner of the matrix.
If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.
The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, for example,
It is a square matrix of order n, and also a special kind of diagonal matrix. It is called an identity matrix because multiplication with it leaves a matrix unchanged:
A nonzero scalar multiple of an identity matrix is called a scalar matrix. If the matrix entries come from a field, the scalar matrices form a group, under matrix multiplication, that is isomorphic to the multiplicative group of nonzero elements of the field.
A square matrix A that is equal to its transpose, that is, A = AT, is a symmetric matrix. If instead, A is equal to the negative of its transpose, that is, A = AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A = A, where the star or asterisk denotes the conjugate transpose of the matrix, that is, the transpose of the complex conjugate of A.
By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; that is, every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real.[28] This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.
A square matrix A is called invertible or non-singular if there exists a matrix B such that
where In is the nn identity matrix with 1s on the main diagonal and 0s elsewhere. If B exists, it is unique and is called the inverse matrix of A, denoted A1.
A symmetric real matrix A is called positive-definite if the associated quadratic form
has a positive value for every nonzero vector x in Rn. If f(x) only yields negative values then A is negative-definite; if f does produce both negative and positive values then A is indefinite.[31] If the quadratic form f yields only non-negative values (positive or zero), the symmetric matrix is called positive-semidefinite (or if only non-positive values, then negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.
A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, that is, the matrix is positive-semidefinite and it is invertible.[32] The table at the right shows two possibilities for 2-by-2 matrices.
Allowing as input two different vectors instead yields the bilinear form associated to A:[33]
In the case of complex matrices, the same terminology and result apply, with symmetric matrix, quadratic form, bilinear form, and transpose xT replaced respectively by  Hermitian matrix, Hermitian form, sesquilinear form, and conjugate transpose xH.
An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (that is, orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:
which entails
where In is the identity matrix of size n.
An orthogonal matrix A is necessarily invertible (with inverse A1 = AT), unitary (A1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or 1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation without reflection, i.e., the transformation preserves the orientation of the transformed structure, while every orthogonal matrix with determinant -1 reverses the orientation, i.e., is a composition of a pure reflection and a (possibly null) rotation. The identity matrices have determinant 1, and are pure rotations by an angle zero.
The complex analogue of an orthogonal matrix is a unitary matrix.
The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:
This is immediate from the definition of matrix multiplication:
It follows that the trace of the product of more than two matrices is independent of cyclic permutations of the matrices, however this does not in general apply for arbitrary permutations (for example, tr(ABC)  tr(BAC), in general). Also, the trace of a matrix is equal to that of its transpose, that is,
The determinant of a square matrix A (denoted det(A) or |A|) is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.
The determinant of 2-by-2 matrices is given by
The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.[35]
The determinant of a product of square matrices equals the product of their determinants:
Adding a multiple of any row to another row, or a multiple of any column to another column does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by 1.[37] Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices, the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, that is, determinants of smaller matrices.[38] This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.[39]
A number  and a non-zero vector v satisfying
are called an eigenvalue and an eigenvector of A, respectively.[40][41] The number  is an eigenvalue of an nn-matrix A if and only if AIn is not invertible, which is equivalent to
The polynomial pA in an indeterminate X given by evaluation of the determinant det(XInA) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA()=0 has at most n different solutions, that is, eigenvalues of the matrix.[43] They may be complex even if the entries of A are real. According to the CayleyHamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.
Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.[44]
To choose the most appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra.[45] As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.
Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, for example, multiplication of matrices. Calculating the matrix product of two n-by-n matrices using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications.[46] A refined approach also incorporates specific features of the computing devices.
In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, that is, matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.[47]
An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace expansion (adj(A) denotes the adjugate matrix of A)
may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.[48]
Most computer programming languages support arrays but are not designed with built-in commands for matrices. Instead, available external libraries provide matrix operations on arrays, in nearly all currently used programming languages. Matrix manipulation was among the earliest numerical applications of computers.[49] The original Dartmouth BASIC had built-in commands for matrix arithmetic on arrays from its second edition implementation in 1964. As early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[50]
There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank, or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.
The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U).[51] Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form.[52] Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV, where U and V are unitary matrices and D is a diagonal matrix.
The eigendecomposition or diagonalization expresses A as a product VDV1, where D is a diagonal matrix and V is a suitable invertible matrix.[53] If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues 1 to n of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right.[54] Given the eigendecomposition, the nth power of A (that is, n-fold iterated matrix multiplication) can be calculated via
and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices.[55] To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.[56]
Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension is tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realized as sequences of numbers, while matrices are rectangular or two-dimensional arrays of numbers.[57] Matrices, subject to certain requirements tend to form groups known as matrix groups. Similarly under certain conditions matrices form rings known as matrix rings. Though the product of matrices is not in general commutative yet certain matrices form fields known as matrix fields.
This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, that is, a set where addition, subtraction, multiplication, and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance, they may be complex in the case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (for example, to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.
More generally, matrices with entries in a ring R are widely used in mathematics.[58] Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) (also denoted Mn(R)[7]) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn.[59] If the ring R is commutative, that is, its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible.[60] Matrices over superrings are called supermatrices.[61]
Matrices do not always have all their entries in the same ring or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be square matrices, and thus need not be members of any ring; but their sizes must fulfill certain compatibility conditions.
Linear maps Rn  Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V  W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that
In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. The matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices.[62] Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.[63]
These properties can be restated more naturally: the category of all matrices with entries in a field 



k


{\displaystyle k}

 with multiplication as composition is equivalent to the category of finite-dimensional vector spaces and linear maps over this field.
More generally, the set of mn matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n=m composition of these maps is possible, and this gives rise to the matrix ring of nn matrices representing the endomorphism ring of Rn.
A group is a mathematical structure consisting of a set of objects together with a binary operation, that is, an operation combining any two objects to a third, subject to certain requirements.[64] A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group.[65][66] Since a group every element must be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.
Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (that is, a smaller group contained in) their general linear group, called a special linear group.[67] Orthogonal matrices, determined by the condition
form the orthogonal group.[68] Every orthogonal matrix has determinant 1 or 1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.
Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group.[69] General groups can be studied using matrix groups, which are comparatively well understood, by means of representation theory.[70]
It is also possible to consider matrices with infinitely many rows and/or columns[71] even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication, and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.
If R is any ring with unity, then the ring of endomorphisms of 



M
=



i

I


R


{\displaystyle M=\bigoplus _{i\in I}R}

 as a right R module is isomorphic to the ring of column finite matrices 





C
F
M


I


(
R
)


{\displaystyle \mathrm {CFM} _{I}(R)}

 whose entries are indexed by 



I

I


{\displaystyle I\times I}

, and whose columns each contain only finitely many nonzero entries. The endomorphisms of M considered as a left R module result in an analogous object, the row finite matrices 





R
F
M


I


(
R
)


{\displaystyle \mathrm {RFM} _{I}(R)}

 whose rows each only have finitely many nonzero entries.
If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: VW, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vectorv of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product Av there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover, this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries because each of those columns does. Products of two matrices of the given type are well defined (provided that the column-index and row-index sets match), are of the same type, and correspond to the composition of linear maps.
If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously, the matrices whose row sums are absolutely convergent series also form a ring.
Infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that must be imposed. However, the explicit point of view of matrices tends to obfuscate the matter,[72] and the abstract and more powerful tools of functional analysis can be used instead.
An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[73][74] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite-dimensional space to itself has determinant1, a fact that is often used as a part of the characterization of determinants.
There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose.[75] Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.[76]
Complex numbers can be represented by particular real 2-by-2 matrices via
under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions[77] and Clifford algebras in general.
Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[78] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[79] Matrices over a polynomial ring are important in the study of control theory.
Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the HartreeFock method.
The adjacency matrix of a finite graph is a basic notion of graph theory.[80] It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges.[81] These concepts can be applied to websites connected by hyperlinks or cities connected by roads etc., in which case (unless the connection network is extremely dense) the matrices tend to be sparse, that is, contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.
The Hessian matrix of a differentiable function : Rn  R consists of the second derivatives of  with respect to the several coordinate directions, that is,[82]
It encodes information about the local growth behaviour of the function: given a critical point x=(x1,...,xn), that is, a point where the first partial derivatives 




f

/



x

i




{\displaystyle \partial f/\partial x_{i}}

 of  vanish, the function has a local minimum if the Hessian matrix is positive definite. Quadratic programming can be used to find global minima or maxima of quadratic functions closely related to the ones attached to matrices (see above).[83]
Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn  Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as[84]
If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.[85]
Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has a decisive influence on the set of possible solutions of the equation in question.[86]
The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen concerning a sufficiently fine grid, which in turn can be recast as a matrix equation.[87]
Stochastic matrices are square matrices whose rows are probability vectors, that is, whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states.[88] A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain-like absorbing states, that is, states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.[89]
Statistics also makes use of matrices in many different forms.[90] Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables.[91] Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function
which can be formulated in terms of matrices, related to the singular value decomposition of matrices.[92]
Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.[93][94]
Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors.[95] For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The CabibboKobayashiMaskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.[96]
The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states.[97] This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.[98]
Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.[99]
A general application of matrices in physics is the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms.[100] They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.[101]
Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix analysis: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies.
The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.[102]
Traditional mesh analysis and nodal analysis in electronics lead to a system of linear equations that can be described with a matrix.
The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H  A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21), and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.
Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th2nd century BCE is the first example of the use of array methods to solve simultaneous equations,[103] including the concept of determinants. In 1545 Italian mathematician Gerolamo Cardano introduced the method to Europe when he published Ars Magna.[104] The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683.[105] The Dutch mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659).[106] Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays.[104] Cramer presented his rule in 1750.
The term "matrix" (Latin for "womb", derived from matermother[107]) was coined by James Joseph Sylvester in 1850,[108] who understood a matrix as an object giving rise to several determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:
Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead, he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition.[104] Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his A memoir on the theory of matrices[110][111] in which he proposed and demonstrated the CayleyHamilton theorem.[104]
The English mathematician Cuthbert Edmund Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use of the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.[104]
The modern study of determinants sprang from several sources.[112] Number-theoretical problems led Gauss to relate coefficients of quadratic forms, that is, expressions such as x2 + xy  2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial
where  denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real.[113] Jacobi studied "functional determinants"later called Jacobi determinants by Sylvesterwhich can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen ber die Theorie der Determinanten[114] and Weierstrass' Zur Determinantentheorie,[115] both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.
Many theorems were first established for small matrices only, for example, the CayleyHamilton theorem was proved for 22 matrices by Cayley in the aforementioned memoir, and by Hamilton for 44 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century, the GaussJordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra,[116] partially due to their use in classification of the hypercomplex number systems of the previous century.
The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns.[117] Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.
The word has been used in unusual ways by at least two authors of historical importance.
Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (19101913) use the word "matrix" in the context of their axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the "bottom" (0 order) the function is identical to its extension:
For example, a function (x, y) of two variables x and y can be reduced to a collection of functions of a single variable, for example, y, by "considering" the function for all possible values of "individuals" ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, that is, ai: (ai, y), can be reduced to a "matrix" of values by "considering" the function for all possible values of "individuals" bi substituted in place of variable y:
Alfred Tarski in his 1946 Introduction to Logic used the word "matrix" synonymously with the notion of truth table as used in mathematical logic.[119]



Analysis is the branch of mathematics dealing with limits
and related theories, such as differentiation, integration, measure, sequences, series, and analytic functions.[1][2]
These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).
Mathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century AD to find the area of a circle.[7] From Jain literature, it appears that Hindus were in possession of the formulae for the sum of the arithmetic and geometric series as early as the 4th century B.C.[8]
crya Bhadrabhu uses the sum of a geometric series in his Kalpastra in 433 B.C.  
[9] In Indian mathematics, particular instances of arithmetic series have been found to implicitly occur in Vedic Literature as early as 2000 B.C.
Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[10] In the 12th century, the Indian mathematician Bhskara II gave examples of derivatives and used what is now known as Rolle's theorem.[11]
In the 14th century, Madhava of Sangamagrama developed infinite series expansions, now called Taylor series, of functions such as sine, cosine, tangent and arctangent.[12] Alongside his development of Taylor series of trigonometric functions, he also estimated the magnitude of the error terms resulting of truncating these series, and gave a rational approximation of some infinite series.  His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.
The modern foundations of mathematical analysis were established in 17th century Europe.[3] This began when Fermat and Descartes developed analytic geometry, which is the precursor to modern calculus. Fermat's method of adequality allowed him to determine the maxima and minima of functions and the tangents of curves.[13] Descartes' publication of La Gomtrie in 1637, which introduced the Cartesian coordinate system, is considered to be the establishment of mathematical analysis. It would be a few decades later that Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18thcentury, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.
In the 18th century, Euler introduced the notion of mathematical function.[14] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[15] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler.  Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals.  Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y.  He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis.  The contributions of these mathematicians and others, such as Weierstrass, developed the (, )-definition of limit approach, thus founding the modern field of mathematical analysis.
In the middle of the 19th century Riemann introduced his theory of integration. The last third of the century saw the arithmetization of analysis by Weierstrass, who thought that geometric reasoning was inherently misleading, and introduced the "epsilon-delta" definition of limit. Then, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the "gaps" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the "size" of the set of discontinuities of real functions.
Also, "monsters" (nowhere continuous functions, continuous but nowhere differentiable functions, space-filling curves) began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue solved the problem of measure, and Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.
In mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.
Much of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).
Formally, a metric space is an ordered pair 



(
M
,
d
)


{\displaystyle (M,d)}

 where 



M


{\displaystyle M}

 is a set and 



d


{\displaystyle d}

 is a metric on 



M


{\displaystyle M}

, i.e., a function
such that for any 



x
,
y
,
z

M


{\displaystyle x,y,z\in M}

, the following holds:
By taking the third property and letting 



z
=
x


{\displaystyle z=x}

, it can be shown that 



d
(
x
,
y
)

0


{\displaystyle d(x,y)\geq 0}

  (non-negative).
A sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.
One of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n  , denoted
Real analysis (traditionally, the theory of functions of a real variable) is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[16][17] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.
Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers.[18] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.
Complex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.
Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[19][20] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces.  This point of view turned out to be particularly useful for the study of differential and integral equations.
A differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[21][22][23] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.
Differential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.
A measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size.[24] In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the 



n


{\displaystyle n}

-dimensional Euclidean space 





R


n




{\displaystyle \mathbb {R} ^{n}}

. For instance, the Lebesgue measure of the interval 




[

0
,
1

]



{\displaystyle \left[0,1\right]}

 in the real numbers is its length in the everyday sense of the wordspecifically, 1.
Technically, a measure is a function that assigns a non-negative real number or + to (certain) subsets of a set 



X


{\displaystyle X}

. It must assign 0 to the empty set and be (countably) additive: the measure of a 'large' subset that can be decomposed into a finite (or countable) number of 'smaller' disjoint subsets, is the sum of the measures of the "smaller" subsets. In general, if one wants to associate a consistent size to each subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called measurable subsets, which are required to form a 






{\displaystyle \sigma }

-algebra. This means that countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement. Indeed, their existence is a non-trivial consequence of the axiom of choice.
Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[25]
Modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.
Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21stcentury, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.
Techniques from analysis are also found in other areas such as:
The vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schrdinger equation, and the Einstein field equations.
Functional analysis is also a major factor in quantum mechanics.
When processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal.  A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[26]
Techniques from analysis are used in many areas of mathematics, including:

In mathematics, a set is a collection of elements.[1][2][3]  The elements that make up a set can be any kind of mathematical objects: numbers, symbols, points in space, lines, other geometrical shapes, variables, or even other sets.[4]  The set with no element is the empty set; a set with a single element is a singleton.  A set may have a finite number of elements or be an infinite set.  Two sets are equal if and only if they have precisely the same elements.[5]
Sets are ubiquitous in modern mathematics. Indeed, set theory, more specifically ZermeloFraenkel set theory, has been the standard way to provide rigorous foundations for all branches of mathematics since the first half of the 20th century.[4]

The concept of a set emerged in mathematics at the end of the 19th century.[6] The German word for set, Menge, was coined by Bernard Bolzano in his work Paradoxes of the Infinite.[7][8][9]Georg Cantor, one of the founders of set theory,   gave the following definition at the beginning of his Beitrge zur Begrndung der transfiniten Mengenlehre:[10]
A set is a gathering together into a whole of definite, distinct objects of our perception or our thoughtwhich are called elements of the set.Bertrand Russell called a set a class: "When mathematicians deal with what they call a manifold, aggregate, Menge, ensemble, or some equivalent name, it is  common, especially where the number of terms involved is finite, to regard the object in question (which is in fact a class) as defined by the enumeration of its terms, and as consisting possibly of a single term, which is in that case is the class."[11]
The foremost property of a set is that it can have elements, also called members. Two sets are equal when they have the same elements. More precisely, sets A and B are equal if every element of A is a member of B, and every element of B is an element of A; this property is called the extensionality of sets.[12]
The simple concept of a set has proved enormously useful in mathematics, but paradoxes arise if no restrictions are placed on how sets can be constructed:
Nave set theory defines a set as any well-defined collection of distinct elements, but problems arise from the vagueness of the term well-defined.
In subsequent efforts to resolve these paradoxes since the time of the original formulation of nave set theory, the properties of sets have been defined by axioms. Axiomatic set theory takes the concept of a set as a primitive notion.[13] The purpose of the axioms is to provide a basic framework from which to deduce the truth or falsity of particular mathematical propositions (statements) about sets, using first-order logic. According to Gdel's incompleteness theorems however, it is not possible to use first-order logic to prove any such particular axiomatic set theory is free from paradox.[citation needed]
Mathematical texts commonly denote sets by capital letters[14][4] in italic, such as A, B, C.[15] A set may also be called a collection or family, especially when its elements are themselves sets.
Roster or enumeration notation defines a set by listing its elements between curly brackets, separated by commas:[16][17][18][19]
In a set, all that matters is whether each element is in it or not, so the ordering of the elements in roster notation is irrelevant (in contrast, in a sequence, a tuple, or a permutation of a set, the ordering of the terms matters). For example, {2, 4, 6} and {4, 6, 4, 2} represent the same set.[20][15][21]
For sets with many elements, especially those following an implicit pattern, the list of members can be abbreviated using an ellipsis '...'.[22][23] For instance, the set of the first thousand positive integers may be specified in roster notation as
An infinite set is a set with an endless list of elements. To describe an infinite set in roster notation, an ellipsis is placed at the end of the list, or at both ends, to indicate that the list continues forever. For example, the set of nonnegative integers is
and the set of all integers is
Another way to define a set is to use a rule to determine what the elements are:
Such a definition is called a semantic description.[24][25]
Set-builder notation specifies a set as a selection from a larger set, determined by a condition on the elements.[25][26][27] For example, a set F can be defined as follows:
In this notation, the vertical bar "|" means "such that", and the description can be interpreted as "F is the set of all numbers n such that n is an integer in the range from 0 to 19 inclusive". Some authors use a colon ":" instead of the vertical bar.[28]
Philosophy uses specific terms to classify types of definitions:
If B is a set and x is an element of B, this is written in shorthand as x  B, which can also be read as "x belongs to B", or "x is in B".[12] The statement "y is not an element of B" is written as y  B, which can also be read as or "y is not in B".[29][30]
For example, with respect to the sets A = {1, 2, 3, 4}, B = {blue, white, red}, and F = {n | n is an integer, and 0  n  19},
The empty set (or null set) is the unique set that has no members. It is denoted  or 






{\displaystyle \emptyset }

 or {}[31][32] or [33] (or ).[34]
A singleton set is a set with exactly one element; such a set may also be called a unit set.[5] Any such set can be written as {x}, where x is the element.
The set {x} and the element x mean different things; Halmos[35] draws the analogy that a box containing a hat is not the same as the hat.
If every element of set A is also in B, then A is described as being a subset of B, or contained in B, written A  B,[36] or B  A.[37] The latter notation may be read B contains A, B includes A, or B is a superset of A. The relationship between sets established by  is called inclusion or containment. Two sets are equal if they contain each other: A  B and B  A is equivalent to A = B.[26]
If A is a subset of B, but A is not equal to B, then A is called a proper subset of B. This can be written A  B. Likewise, B  A means B is a proper superset of A, i.e. B contains A, and is not equal to A.
A third pair of operators  and  are used differently by different authors: some authors use A  B and B  A to mean A is any subset of B (and not necessarily a proper subset),[38][29] while others reserve A  B and B  A for cases where A is a proper subset of B.[36]
Examples:
The empty set is a subset of every set,[31] and every set is a subset of itself:[38]
An Euler diagram is a graphical representation of a collection of sets; each set is depicted as a planar region enclosed by a loop, with its elements inside. If A is a subset of B, then the region representing A is completely inside the region representing B. If two sets have no elements in common, the regions do not overlap. 
A Venn diagram, in contrast, is a graphical representation of n sets in which the n loops divide the plane into 2n zones such that for each way of selecting some of the n sets (possibly all or none), there is a zone for the elements that belong to all the selected sets and none of the others. For example, if the sets are A, B, and C, there should be a zone for the elements that are inside A and C and outside B (even if such elements do not exist).
There are sets of such mathematical importance, to which mathematicians refer so frequently, that they have acquired special names and notational conventions to identify them. 
Many of these important sets are represented in mathematical texts using bold (e.g. 





Z




{\displaystyle {\mathbf {Z}}}

) or blackboard bold (e.g. 




Z



{\displaystyle \mathbb {Z} }

) typeface.[39] These include
Each of the above sets of numbers has an infinite number of elements. Each is a subset of the sets listed below it. 
Sets of positive or negative numbers are sometimes denoted by superscript plus and minus signs, respectively. For example, 





Q


+




{\displaystyle \mathbf {Q} ^{+}}

 represents the set of positive rational numbers.
A function (or mapping) from a set A to a set B is a rule that assigns to each "input" element of A an "output" that is an element of B; more formally, a function is a special kind of relation, one that relates each element of A to exactly one element of B. A function is called
An injective function is called an injection, a surjective function is called a surjection, and a bijective function is called a bijection or one-to-one correspondence.
The cardinality of a set S, denoted |S|, is the number of members of S.[40] For example, if B = {blue, white, red}, then |B| = 3. Repeated members in roster notation are not counted,[41][42] so |{blue, white, red, blue, white}| = 3, too.
More formally, two sets share the same cardinality if there exists a one-to-one correspondence between them.
The cardinality of the empty set is zero.[43]
The list of elements of some sets is endless, or infinite. For example, the set 




N



{\displaystyle \mathbb {N} }

 of natural numbers is infinite.[26] In fact, all the special sets of numbers mentioned in the section above, are infinite. Infinite sets have infinite cardinality. 
Some infinite cardinalities are greater than others. Arguably one of the most significant results from set theory is that the set of real numbers has greater cardinality than the set of natural numbers.[44] Sets with cardinality less than or equal to that of 




N



{\displaystyle \mathbb {N} }

 are called countable sets; these are either finite sets or countably infinite sets (sets of the same cardinality as 




N



{\displaystyle \mathbb {N} }

); some authors use "countable" to mean "countably infinite". Sets with cardinality strictly greater than that of 




N



{\displaystyle \mathbb {N} }

 are called uncountable sets.
However, it can be shown that the cardinality of a straight line (i.e., the number of points on a line) is the same as the cardinality of any segment of that line, of the entire plane, and indeed of any finite-dimensional Euclidean space.[45]
The Continuum Hypothesis, formulated by Georg Cantor in 1878, is the statement that there is no set with cardinality strictly between the cardinality of the natural numbers and the cardinality of a straight line.[46] In 1963, Paul Cohen proved that the Continuum Hypothesis is independent of the axiom system ZFC consisting of ZermeloFraenkel set theory with the axiom of choice.[47] 
(ZFC is the most widely-studied version of axiomatic set theory.)
The power set of a set S is the set of all subsets of S.[26] The empty set and S itself are elements of the power set of S, because these are both subsets of S. For example, the power set of {1, 2, 3} is {, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}. The power set of a set S is commonly written as P(S) or 2S.[26][48][15]
If S has n elements, then P(S) has 2n elements.[49] For example, {1, 2, 3} has three elements, and its power set has 23 = 8 elements, as shown above.
If S is infinite (whether countable or uncountable), then P(S) is uncountable. Moreover, the power set is always strictly "bigger" than the original set, in the sense that any attempt to pair up the elements of S with the elements of P(S) will leave some elements of P(S) unpaired. (There is never a bijection from S onto P(S).)[50]
A partition of a set S is a set of nonempty subsets of S, such that every element x in S is in exactly one of these subsets. That is, the subsets are pairwise disjoint (meaning any two sets of the partition contain no element in common), and the union of all the subsets of the partition is S.[51][52]
There are several fundamental operations for constructing new sets from given sets.
Two sets can be joined: the union of A and B, denoted by A  B, is the set of all things that are members of A or of B or of both.
Examples:
Some basic properties of unions:
A new set can also be constructed by determining which members two sets have "in common". The intersection of A and B, denoted by A  B, is the set of all things that are members of both A and B. If A  B = , then A and B are said to be disjoint.
Examples:
Some basic properties of intersections:
Two sets can also be "subtracted". The relative complement of B in A (also called the set-theoretic difference of A and B), denoted by A \ B (or A  B), is the set of all elements that are members of A, but not members of B. It is valid to "subtract" members of a set that are not in the set, such as removing the element green from the set {1, 2, 3}; doing so will not affect the elements in the set.
In certain settings, all sets under discussion are considered to be subsets of a given universal set U. In such cases, U \ A is called the absolute complement or simply complement of A, and is denoted by A or Ac.
Examples:
Some basic properties of complements include the following:
An extension of the complement is the symmetric difference, defined for sets A, B as
For example, the symmetric difference of {7, 8, 9, 10} and {9, 10, 11, 12} is the set {7, 8, 11, 12}. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring (with the empty set as neutral element) and intersection as the multiplication of the ring.
A new set can be constructed by associating every element of one set with every element of another set. The Cartesian product of two sets A and B, denoted by A  B, is the set of all ordered pairs (a, b) such that a is a member of A and b is a member of B.
Examples:
Some basic properties of Cartesian products:
Let A and B be finite sets; then the cardinality of the Cartesian product is the product of the cardinalities:
Sets are ubiquitous in modern mathematics. For example, structures in abstract algebra, such as groups, fields and rings, are sets closed under one or more operations.
One of the main applications of naive set theory is in the construction of relations. A relation from a domain A to a codomain B is a subset of the Cartesian product A  B. For example, considering the set S = {rock, paper, scissors} of shapes in the game of the same name, the relation "beats" from S to S is the set B = {(scissors,paper), (paper,rock), (rock,scissors)}; thus x beats y in the game if the pair (x,y) is a member of B. Another example is the set F of all pairs (x, x2), where x is real. This relation is a subset of R  R, because the set of all squares is subset of the set of all real numbers. Since for every x in R, one and only one pair (x,...) is found in F, it is called a function. In functional notation, this relation can be written as F(x) = x2.
The inclusionexclusion principle is a counting technique that can be used to count the number of elements in a union of two setsif the size of each set and the size of their intersection are known. It can be expressed symbolically as
A more general form of the principle can be used to find the cardinality of any finite union of sets:
Augustus De Morgan stated two laws about sets.
If A and B are any two sets then,
The complement of A union B equals the complement of A intersected with the complement of B.
The complement of A intersected with B is equal to the complement of A union to the complement of B.

Recreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity or as a part of a student's formal education. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.[1]
The Mathematical Association of America (MAA) includes recreational mathematics as one of its seventeen Special Interest Groups, commenting:
Recreational mathematics is not easily defined because it is more than mathematics done as a diversion or playing games that involve mathematics. Recreational mathematics is inspired by deep ideas that are hidden in puzzles, games, and other forms of play. The aim of the SIGMAA on Recreational Mathematics (SIGMAA-Rec) is to bring together enthusiasts and researchers in the myriad of topics that fall under recreational math. We will share results and ideas from our work, show that real, deep mathematics is there awaiting those who look, and welcome those who wish to become involved in this branch of mathematics.[2]Mathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.
Some of the more well-known topics in recreational mathematics are Rubik's Cubes, magic squares, fractals, logic puzzles and mathematical chess problems, but this area of mathematics includes the aesthetics and culture of mathematics, peculiar or amusing stories and coincidences about mathematics, and the personal lives of mathematicians.
Mathematical games are multiplayer games whose rules, strategies, and outcomes can be studied and explained using mathematics. The players of the game may not need to use explicit mathematics in order to play mathematical games. For example, Mancala is studied in the mathematical field of combinatorial game theory, but no mathematics is necessary in order to play it.
Mathematical puzzles require mathematics in order to solve them. They have specific rules, as do multiplayer games, but mathematical puzzles don't usually involve competition between two or more players. Instead, in order to solve such a puzzle, the solver must find a solution that satisfies the given conditions.
Logic puzzles and classical ciphers are common examples of mathematical puzzles. Cellular automata and fractals are also considered mathematical puzzles, even though the solver only interacts with them by providing a set of initial conditions.
As they often include or require game-like features or thinking, mathematical puzzles are sometimes also called mathematical games.
Magic tricks based on mathematical principles can produce self-working but surprising effects. For instance, a mathemagician might use the combinatorial properties of a deck of playing cards to guess a volunteer's selected card, or Hamming codes to identify whether a volunteer is lying.[3]
Other curiosities and pastimes of non-trivial mathematical interest include:
There are many blogs and audio or video series devoted to recreational mathematics.  Among the notable are the following:
Prominent practitioners and advocates of recreational mathematics have included:

Ronald Maurice Bean, better known professionally as Mathematics (also known as Allah Mathematics) (born October 21, 1972), is a hip hop producer and DJ for the Wu-Tang Clan and its solo and affiliate projects. He designed the Wu-Tang Clan logo.[1]
Born and raised in Jamaica, Queens, New York, Mathematics was introduced to hip hop by his brother who used to bring home recordings of the genre's pioneers like Grandmaster Flash & The Furious Five, Treacherous Three and Cold Crush Brothers. He began his career in 1987 DJing block parties and park jams in Baisley Projects, going by the name Supreme Cut Master. In 1988, he became the full-time DJ for experienced rapper Victor C, doing countless shows in clubs and colleges in New York City.
In 1990, Mathematics linked up with GZA/Genius, who would soon become one of the Wu-Tang Clan's founding members, but at the time was struggling to build a career on the Cold Chillin' label. This partnership earned Mathematics a spot on his first official tour, The Cold Chillin Blizzard Tour (with popular acts such as Biz Markie, Big Daddy Kane, Kool G. Rap & DJ Polo and Marley Marl).
GZA left Cold Chillin after his first album, Words from the Genius, did not achieve the sales target that was anticipated. He and Mathematics took to the road again, but this time with the help of GZA's cousins, RZA and Ol' Dirty Bastard. These three soon became the founding members of Wu-Tang Clan, then known as All In Together Now. The group soon dissolved, however, and the trio set their minds on creating the Wu group. During the group's inception, Mathematics used his experience as a graffiti artist to design a logo for the up-and-coming crew, as well as various other logos and designs the Wu-Tang's artists would use. In the years to come, he became a Wu-Element under the guidance of RZA.
Mathematics' first real exposure to production came late one night when he attended a session where he assisted RZA, his mentor, in constructing a beat from nothing. The track eventually developed into "Ice Cream" on Raekwon's Only Built 4 Cuban Linx album. RZA inspired Mathematics to follow the Wu-Tang, giving him advice on the nuances of hip hop production over the coming years. In 1996, Mathematics began record producing, in Staten Island, New York (Shaolin) and at home, in the P-Funk City, Plainfield, New Jersey, between heavily scheduled tour dates and rigorous road travelling with his father's gospel group, The Soul Seekers.
His first track, "Fast Life" featuring Ghostface Killah and the American football star Andre Rison, was included in the NFL Jams compilation album. Though this track faded into obscurity somewhat, it led to several more collaborations between Mathematics and Ghostface; Mathematics also began to produce for many other Wu-Tang members and affiliates, including several tracks on GZA's second album Beneath The Surface as well as Method Man's Tical 2000: Judgement Day, Inspectah Deck's Uncontrolled Substance and Method Man & Redman's Blackout!. Eventually, he produced for the Clan as a group, with "Do You Really (Thang, Thang)", "Careful (Click, Click)".
In 2003, Mathematics moved into TV work, as he produced the main theme and all original music for the short lived show Wanda At Large, which starred Wanda Sykes and was broadcast by the Fox Network. During this time and between continuous touring, Mathematics started work on his first solo full length project, Love, Hell Or Right. Completely mixed, arranged and produced by himself, Love Hell or Right was released fall 2003 on his own Quewisha Records label in conjunction with High Times Records, and it went on to sell 30,000 units.
As well as using rappers from Mathematics' home borough of Queens, Eyes-Low and Buddah Bless, Love Hell Or Right had appearances from all the Wu-Tang Clan members except GZA and the then-imprisoned Ol' Dirty Bastard. Mathematics soon signed to the popular independent hip hop label Nature Sounds (home to Wu-Tang colleague Masta Killa as well as MF DOOM) and released his second album The Problem in 2005. On this album the entire Wu-Tang Clan appeared, including a posthumous appearance from Ol' Dirty Bastard. As well as working on his solo albums, Mathematics has continued to contribute beats to many Wu-Tang releases, including the first albums by Masta Killa and Streetlife. In January 2012 he was to release a sequel to The Problem entitled The Answer, entirely produced by him with Wu-Tang members such as Raekwon, GZA, Method Man, Cappadonna, Masta Killa, Ol' Dirty Bastard, Ghostface Killah. Other artists include Redman and artists he is developing such as Ali Vegas, Eyeslow and Bad Luck.[2]
In August 2017, it was confirmed that Wu-Tang Clan will release a new album, Wu-Tang: The Saga Continues, which was entirely produced by Mathematics and was released on October 13, 2017.[3] The first track off the album is titled "People Say" and features Redman.



Physics is the natural science that studies matter,[a] its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force.[2] Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves.[b][3][4][5]
Physics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps the oldest.[6] Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right.[c] Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences[3] and suggest new avenues of research in academic disciplines such as mathematics and philosophy.
Advances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons;[3] advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.
The word "physics" comes from Ancient Greek:  (), romanized:physik (epistm), meaning "knowledge of nature".[8][9][10]
Astronomy is one of the oldest natural sciences. Early civilizations dating back before 3000BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilisation, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky,[6] which however did not explain the positions of the planets.
According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy.[11] Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies,[12] while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere.[13]
Natural philosophy has its origins in Greece during the Archaic period (650 BCE  480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause.[14] They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment;[15] for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.[16]
The Western Roman Empire fell in the fifth century, and this resulted in a decline in intellectual pursuits in the western part of Europe. By contrast, the Eastern Roman Empire (also known as the Byzantine Empire) resisted the attacks from the barbarians, and continued to advance various fields of learning, including physics.[17]
In the sixth century, Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest.

In sixth-century Europe John Philoponus, a Byzantine scholar, questioned Aristotle's teaching of physics and noted its flaws. He introduced the theory of impetus. Aristotle's physics was not scrutinized until Philoponus appeared; unlike Aristotle, who based his physics on verbal argument, Philoponus relied on observation. On Aristotle's physics Philoponus wrote:But this is completely erroneous, and our view may be corroborated by actual observation more effectively than by any sort of verbal argument. For if you let fall from the same height two weights of which one is many times as heavy as the other, you will see that the ratio of the times required for the motion does not depend on the ratio of the weights, but that the difference in time is a very small one. And so, if the difference in the weights is not considerable, that is, of one is, let us say, double the other, there will be no difference, or else an imperceptible difference, in time, though the difference in weight is by no means negligible, with one body weighing twice as much as the other[19]Philoponus' criticism of Aristotelian principles of physics served as an inspiration for Galileo Galilei ten centuries later,[20] during the Scientific Revolution. Galileo cited Philoponus substantially in his works when arguing that Aristotelian physics was flawed.[21][22] In the 1300s Jean Buridan, a teacher in the faculty of arts at the University of Paris, developed the concept of impetus. It was a step toward the modern ideas of inertia and momentum.[23]
Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and a priori reasoning, developing early forms of the scientific method.
The most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was The Book of Optics (also known as Kitb al-Manir), written by Ibn al-Haytham, in which he conclusively disproved the ancient Greek idea about vision, but also came up with a new theory. In the book, he presented a study of the phenomenon of the camera obscura (his thousand-year-old version of the pinhole camera) and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye. He asserted that the light ray is focused, but the actual explanation of how light projected to the back of the eye had to wait until 1604. His Treatise on Light explained the camera obscura, hundreds of years before the modern development of photography.[24]
The seven-volume Book of Optics (Kitab al-Manathir) hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to Ren Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.
The translation of The Book of Optics had a huge impact on Europe. From it, later European scholars were able to build devices that replicated those Ibn al-Haytham had built, and understand the way light works. From this, important inventions such as eyeglasses, magnifying glasses, telescopes, and cameras were developed.
Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.[25][pageneeded]
Major developments in this period include the replacement of the geocentric model of the Solar System with the heliocentric Copernican model, the laws governing the motion of planetary bodies (determined by Kepler between 1609 and 1619), Galileo's pioneering work on telescopes and observational astronomy in the 16th and 17th Centuries, and Newton's discovery and unification of the laws of motion and universal gravitation (that would come to bear his name).[26] Newton also developed calculus,[d] the mathematical study of change, which provided new mathematical methods for solving physical problems.[27]
The discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased.[28] The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.
Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light.[29] Black-body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.[30]
Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrdinger and Paul Dirac.[30] From this early work, and work in related fields, the Standard Model of particle physics was derived.[31] Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012,[32] all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research.[33] Areas of mathematics in general are important to this field, such as the study of probabilities and groups.
In many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterize matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book Physics (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.[e]
By the 19th century, physics was realized as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its "scientific method" to advance our knowledge of the physical world.[35] The scientific method employs a priori reasoning as well as a posteriori reasoning and the use of Bayesian inference to measure the validity of a given theory.[36]
The development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.[37]
Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism,[38] and Schrdinger, who wrote on quantum mechanics.[39][40] The mathematical physicist Roger Penrose had been called a Platonist by Stephen Hawking,[41] a view Penrose discusses in his book, The Road to Reality.[42] Hawking referred to himself as an "unashamed reductionist" and took issue with Penrose's views.[43]
Though physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics, was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Newton (16421727).
These central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.
Classical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th centuryclassical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received.[44]  Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing,[45] and electroacoustics, the manipulation of audible sound waves using electronics.[46]
Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.
Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics study matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsensical notions of space, time, matter, and energy are no longer valid.[47]
The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.[48]
While physics aims to discover universal laws, its theories lie in explicit domains of applicability.
Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Planck, Schrdinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.
Mathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras,[49] Plato,[50] Galileo,[51] and Newton.
Physics uses mathematics[52] to organise and formulate experimental results. From those results, precise or estimated solutions are obtained, or quantitative results, from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.
Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.
The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical.[53] The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.[clarification needed]
Pure physics is a branch of fundamental science (also called basic science. Physics is also called "the fundamental science" because all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics.[54] Similarly, chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the molecular and atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge. Physics is applied in industries like engineering and medicine.
Applied physics is a general term for physics research, which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.
The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.
Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.
With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering that drastically speed up the development of a new technology.
But there is also considerable interdisciplinarity, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).
Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.[55]
A scientific law is a concise verbal or mathematical statement of a relation that expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.[56]
Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they strongly affect and depend upon each other. Progress in physics frequently comes about when experimental results defy explanation by existing theories, prompting intense focus on applicable modelling, and when new theories generate experimentally testable predictions, which inspire the development of new experiments (and often related equipment).[57]
Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.[58]
Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way.[f] Beyond the known universe, the field of theoretical physics also deals with hypothetical issues,[g] such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories; they then explore the consequences of these ideas and work toward making testable predictions.
Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists who are involved in basic research, design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry, developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas that have not been explored well by theorists.[59]
Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science".[54] Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.
For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two.[60] This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one forceelectromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section Current research below for more information).[61]
Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.[62]
Since the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. "Universalists" such as Einstein (18791955) and Lev Landau (19081968), who worked in multiple fields of physics, are now very rare.[h]
The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.
Particle physics is the study of the elementary constituents of matter and energy and the interactions between them.[63] In addition, particle physicists design and develop the high-energy accelerators,[64] detectors,[65] and computer programs[66] necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.[67]
Currently, the interactions of elementary particles and fields are described by the Standard Model.[68] The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces.[68] Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively).[69] The Standard Model also predicts a particle known as the Higgs boson.[68] In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson,[70] an integral part of the Higgs mechanism.
Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.
Atomic, molecular, and optical physics (AMO) is the study of mattermatter and lightmatter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).
Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions,[71][72][73] low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics.
Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.
Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter.[74][75] In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.[76]
The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms.[77] More exotic condensed phases include the superfluid[78] and the BoseEinstein condensate[79] found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials,[80] and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.[81]
Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields.[82] The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research grouppreviously solid-state theoryin 1967.[83] In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics.[82] Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.[76]
Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.[84]
The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.
Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.
The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the CDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.
Numerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe.[85][86] In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years.[87] Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.
IBEX is already yielding new astrophysical discoveries: "No one knows what is creating the ENA (energetic neutral atoms) ribbon" along the termination shock of the solar wind, "but everyone agrees that it means the textbook picture of the heliospherein which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a cometis wrong."[88]
Research in physics is continually progressing on a large number of fronts.
In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity.[89] Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.[76][90]
In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing.[91]
Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity,[92] chaos,[93] or turbulence[94] are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.[i][95]
These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said:[96]
I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.

In physics, power is the amount of energy transferred or converted per unit time. In the International System of Units, the unit of power is the watt, equal to one joule per second. In older works, power is sometimes called activity.[1][2][3] Power is a scalar quantity.
Power is related to other quantities, for example the power involved in moving a ground vehicle is the product of the traction force on the wheels and the velocity of the vehicle. The output power of a motor is the product of the torque that the motor generates and the angular velocity of its output shaft.  Likewise, the power dissipated in an electrical element of a circuit is the product of the current flowing through the element and of the voltage across the element.[4][5]
Power is the rate with respect to time at which work is done; it is the time derivative of work:
where P is power, W is work, and t is time.
If a constant force F is applied throughout a distance x, the work done is defined as 



W
=

F



x



{\displaystyle W=\mathbf {F} \cdot \mathbf {x} }

. In this case, power can be written as:




P
=



d
W


d
t



=


d

d
t




(


F



x


)

=

F





d

x



d
t



=

F



v



{\displaystyle P={\frac {dW}{dt}}={\frac {d}{dt}}\left(\mathbf {F} \cdot \mathbf {x} \right)=\mathbf {F} \cdot {\frac {d\mathbf {x} }{dt}}=\mathbf {F} \cdot \mathbf {v} }


If instead the force is variable over a three-dimensional curve C, then the work is expressed in terms of the line integral:




W
=



C



F


d

r

=




t



F





d

r



d
t




d
t
=




t



F



v


d
t


{\displaystyle W=\int _{C}\mathbf {F} \cdot d\mathbf {r} =\int _{\Delta t}\mathbf {F} \cdot {\frac {d\mathbf {r} }{dt}}\ dt=\int _{\Delta t}\mathbf {F} \cdot \mathbf {v} \ dt}


From the fundamental theorem of calculus, we know that 



P
=



d
W


d
t



=


d

d
t







t



F



v


d
t
=

F



v



{\displaystyle P={\frac {dW}{dt}}={\frac {d}{dt}}\int _{\Delta t}\mathbf {F} \cdot \mathbf {v} \ dt=\mathbf {F} \cdot \mathbf {v} }

. Hence the formula is valid for any general situation.
The dimension of power is energy divided by time. In the International System of Units (SI), the unit of power is the watt (W), which is equal to one joule per second. Other common and traditional measures are horsepower (hp), comparing to the power of a horse; one mechanical horsepower equals about 745.7 watts. Other units of power include ergs per second (erg/s), foot-pounds per minute, dBm, a logarithmic measure relative to a reference of 1 milliwatt, calories per hour, BTU per hour (BTU/h), and tons of refrigeration.
As a simple example, burning one kilogram of coal releases much more energy than detonating a kilogram of TNT,[6] but because the TNT reaction releases energy much more quickly, it delivers far more power than the coal.
If W  is the amount of work performed during a period of time of duration t, the average power Pavg over that period is given by the formula:
It is the average amount of work done or energy converted per unit of time. The average power is often simply called "power" when the context makes it clear.
The instantaneous power is then the limiting value of the average power as the time interval t approaches zero.
In the case of constant power P, the amount of work performed during a period of duration t is given by:
In the context of energy conversion, it is more customary to use the symbol E rather than W.
Power in mechanical systems is the combination of forces and movement. In particular, power is the product of a force on an object and the object's velocity, or the product of a torque on a shaft and the shaft's angular velocity.
Mechanical power is also described as the time derivative of work.  In mechanics, the work done by a force F on an object that travels along a curve C is given by the line integral:
where x defines the path C and v is the velocity along this path.
If the force F is derivable from a potential (conservative), then applying the gradient theorem (and remembering that force is the negative of the gradient of the potential energy) yields:
where A and B are the beginning and end of the path along which the work was done.
The power at any point along the curve C is the time derivative:
In one dimension, this can be simplified to:
In rotational systems, power is the product of the torque  and angular velocity ,
where  measured in radians per second.  The 






{\displaystyle \cdot }

 represents scalar product.
In fluid power systems such as hydraulic actuators, power is given by
where p is pressure in pascals, or N/m2 and Q is volumetric flow rate in m3/s in SI units.
If a mechanical system has no losses, then the input power must equal the output power. This provides a simple formula for the mechanical advantage of the system.
Let the input power to a device be a force FA acting on a point that moves with velocity vA and the output power be a force FB acts on a point that moves with velocity vB.  If there are no losses in the system, then
and the mechanical advantage of the system (output force per input force) is given by
The similar relationship is obtained for rotating systems, where TA and A are the torque and angular velocity of the input and TB and B are the torque and angular velocity of the output.  If there are no losses in the system, then
which yields the mechanical advantage
These relations are important because they define the maximum performance of a device in terms of velocity ratios determined by its physical dimensions.  See for example gear ratios.
The instantaneous electrical power P delivered to a component is given by
where 
If the component is a resistor with time-invariant voltage to current ratio, then:
where
is the resistance, measured in ohms.
In the case of a periodic signal 



s
(
t
)


{\displaystyle s(t)}

 of period 



T


{\displaystyle T}

, like a train of identical pulses, the instantaneous power 



p
(
t
)
=

|

s
(
t
)


|


2




{\displaystyle p(t)=|s(t)|^{2}}

 is also a periodic function of period 



T


{\displaystyle T}

.  The peak power is simply defined by:
The peak power is not always readily measurable, however, and the measurement of the average power 




P


a
v
g





{\displaystyle P_{\mathrm {avg} }}

 is more commonly performed by an instrument.  If one defines the energy per pulse as:
then the average power is:
One may define the pulse length 






{\displaystyle \tau }

 such that 




P

0



=




p
u
l
s
e





{\displaystyle P_{0}\tau =\epsilon _{\mathrm {pulse} }}

 so that the ratios
are equal. These ratios are called the duty cycle of the pulse train.
Power is related to intensity at a radius 



r


{\displaystyle r}

; the power emitted by a source can be written as:[citation needed]


Particle physics (also known as high energy physics) is a branch of physics that studies the nature of the particles that constitute matter and radiation. Although the word particle can refer to various types of very small objects (e.g. protons, gas particles, or even household dust), particle physics usually investigates the irreducibly smallest detectable particles and the fundamental interactions necessary to explain their behaviour. 
In current understanding, these elementary particles are excitations of the quantum fields that also govern their interactions. The currently dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. Thus, modern particle physics generally investigates the Standard Model and its various possible extensions, e.g. to the newest "known" particle, the Higgs boson, or even to the oldest known force field, gravity.[1][2]
Modern particle physics research is focused on subatomic particles, including atomic constituents, such as electrons, protons, and neutrons (protons and neutrons are composite particles called baryons, made of quarks), that are produced by radioactive and scattering processes, such particles are photons, neutrinos, and muons, as well as a wide range of exotic particles.
Dynamics of particles are also governed by quantum mechanics; they exhibit waveparticle duality, displaying particle-like behaviour under certain experimental conditions and wave-like behaviour in others. In more technical terms, they are described by quantum state vectors in a Hilbert space, which is also treated in quantum field theory. Following the convention of particle physicists, the term elementary particles is applied to those particles that are, according to current understanding, presumed to be indivisible and not composed of other particles.[3]
All particles and their interactions observed to date can be described almost entirely by a quantum field theory called the Standard Model.[4] The Standard Model, as currently formulated, has 61 elementary particles.[3]
Those elementary particles can combine to form composite particles, accounting for the hundreds of other species of particles that have been discovered since the 1960s.
The Standard Model has been found to agree with almost all the experimental tests conducted to date. However, most particle physicists believe that it is an incomplete description of nature and that a more fundamental theory awaits discovery (See Theory of Everything). In recent years, measurements of neutrino mass have provided the first experimental deviations from the Standard Model, since neutrinos are massless in the Standard Model.[5]
The idea that all matter is fundamentally composed of elementary particles dates from at least the 6th century BC.[6] In the 19th century, John Dalton, through his work on stoichiometry, concluded that each element of nature was composed of a single, unique type of particle.[7] The word atom, after the Greek word atomos meaning "indivisible", has since then denoted the smallest particle of a chemical element, but physicists soon discovered that atoms are not, in fact, the fundamental particles of nature, but are conglomerates of even smaller particles, such as the electron. The early 20th century explorations of nuclear physics and quantum physics led to proofs of nuclear fission in 1939 by Lise Meitner (based on experiments by Otto Hahn), and nuclear fusion by Hans Bethe in that same year; both discoveries also led to the development of nuclear weapons. Throughout the 1950s and 1960s, a bewildering variety of particles were found in collisions of particles from beams of increasingly high energy. It was referred to informally as the "particle zoo". Important discoveries such as the CP violation by James Cronin and Val Fitch brought new questions to matter-antimatter imbalance.[8] The term particle zoo was modified[citation needed] after the formulation of the Standard Model during the 1970s, in which the large number of particles was explained as combinations of a (relatively) small number of more fundamental particles, which marked the beginning of modern particle physics.[citation needed][9]
The current state of the classification of all elementary particles is explained by the Standard Model, which gained widespread acceptance in the mid-1970s after experimental confirmation of the existence of quarks. It describes the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons. The species of gauge bosons are eight gluons, W, W+ and Z bosons, and the photon.[4] The Standard Model also contains 24 fundamental fermions (12 particles and their associated anti-particles), which are the constituents of all matter.[10] Finally, the Standard Model also predicted the existence of a type of boson known as the Higgs boson. On 4 July 2012, physicists with the Large Hadron Collider at CERN announced they had found a new particle that behaves similarly to what is expected from the Higgs boson.[11]
The world's major particle physics laboratories are:
Many other particle accelerators also exist.
The techniques required for modern experimental particle physics are quite varied and complex, constituting a sub-specialty nearly completely distinct[citation needed] from the theoretical side of the field.
Theoretical particle physics attempts to develop the models, theoretical framework, and mathematical tools to understand current experiments and make predictions for future experiments (see also theoretical physics). There are several major interrelated efforts being made in theoretical particle physics today.
One important branch attempts to better understand the Standard Model and its tests. Theorists make quantitative predictions of observables at collider and astronomical experiments, which along with experimental measurements is used to extract the parameters of the Standard Model with less uncertainty. This work probes the limits of the Standard Model and therefore expands scientific understanding of nature's building blocks. Those efforts are made challenging by the difficulty of calculating high precision quantities in quantum chromodynamics. Some theorists working in this area use the tools of perturbative quantum field theory and effective field theory, referring to themselves as phenomenologists.[citation needed] Others make use of lattice field theory and call themselves lattice theorists.
Another major effort is in model building where model builders develop ideas for what physics may lie beyond the Standard Model (at higher energies or smaller distances). This work is often motivated by the hierarchy problem and is constrained by existing experimental data.[citation needed] It may involve work on supersymmetry, alternatives to the Higgs mechanism, extra spatial dimensions (such as the RandallSundrum models), Preon theory, combinations of these, or other ideas.
A third major effort in theoretical particle physics is string theory. String theorists attempt to construct a unified description of quantum mechanics and general relativity by building a theory based on small strings, and branes rather than particles. If the theory is successful, it may be considered a "Theory of Everything", or "TOE".
There are also other areas of work in theoretical particle physics ranging from particle cosmology to loop quantum gravity.[citation needed]
This division of efforts in particle physics is reflected in the names of categories on the arXiv, a preprint archive:[24] hep-th (theory), hep-ph (phenomenology), hep-ex (experiments), hep-lat (lattice gauge theory).
In principle, all physics (and practical applications developed therefrom) can be derived from the study of fundamental particles. In practice, even if "particle physics" is taken to mean only "high-energy atom smashers", many technologies have been developed during these pioneering investigations that later find wide uses in society. Particle accelerators are used to produce medical isotopes for research and treatment (for example, isotopes used in PET imaging), or used directly in external beam radiotherapy. The development of superconductors has been pushed forward by their use in particle physics. The World Wide Web and touchscreen technology were initially developed at CERN. Additional applications are found in medicine, national security, industry, computing, science, and workforce development, illustrating a long and growing list of beneficial practical applications with contributions from particle physics.[25]
The primary goal, which is pursued in several distinct ways, is to find and understand what physics may lie beyond the standard model. There are several powerful experimental reasons to expect new physics, including dark matter and neutrino mass. There are also theoretical hints that this new physics should be found at accessible energy scales.
Much of the effort to find this new physics are focused on new collider experiments. The Large Hadron Collider (LHC) was completed in 2008 to help continue the search for the Higgs boson, supersymmetric particles, and other new physics. An intermediate goal is the construction of the International Linear Collider (ILC), which will complement the LHC by allowing more precise measurements of the properties of newly found particles. In August 2004, a decision for the technology of the ILC was taken but the site has still to be agreed upon.
In addition, there are important non-collider experiments that also attempt to find and understand physics beyond the Standard Model. One important non-collider effort is the determination of the neutrino masses, since these masses may arise from neutrinos mixing with very heavy particles. In addition, cosmological observations provide many useful constraints on the dark matter, although it may be impossible to determine the exact nature of the dark matter without the colliders. Finally, lower bounds on the very long lifetime of the proton put constraints on Grand Unified Theories at energy scales much higher than collider experiments will be able to probe any time soon.
In May 2014, the Particle Physics Project Prioritization Panel released its report on particle physics funding priorities for the United States over the next decade. This report emphasized continued U.S. participation in the LHC and ILC, and expansion of the Deep Underground Neutrino Experiment, among other recommendations.


The Nobel Prize in Physics is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. Physics is traditionally the first award presented in the Nobel Prize ceremony.
The first Nobel Prize in Physics was awarded to physicist Wilhelm Rntgen in recognition of the extraordinary services he rendered by the discovery of X-rays. This award is administered by the Nobel Foundation and is widely regarded as the most prestigious award that a scientist can receive in physics. It is presented in Stockholm at an annual ceremony on 10 December, the anniversary of Nobel's death. As of 2021 a total of 219 individuals have been awarded the prize.[2]
Alfred Nobel, in his last will and testament, stated that his wealth should be used to create a series of prizes for those who confer the "greatest benefit on mankind" in the fields of physics, chemistry, peace, physiology or medicine, and literature.[3] Though Nobel wrote several wills during his lifetime, the last one was written a year before he died and was signed at the Swedish-Norwegian Club in Paris on 27 November 1895.[4][5] Nobel bequeathed 94% of his total assets, 31million Swedish kronor (US$198million, 176million in 2016), to establish and endow the five Nobel Prizes.[6] Owing to the level of skepticism surrounding the will, it was not until 26 April 1897 that it was approved by the Storting (Norwegian Parliament).[7][8] The executors of his will were Ragnar Sohlman and Rudolf Lilljequist, who formed the Nobel Foundation to take care of Nobel's fortune and organise the prizes.
The members of the Norwegian Nobel Committee who were to award the Peace Prize were appointed shortly after the will was approved. The other prize-awarding organisations followed: the Karolinska Institutet on 7 June, the Swedish Academy on 9 June, and the Royal Swedish Academy of Sciences on 11 June.[9][10] The Nobel Foundation then established guidelines for awarding the prizes. In 1900, the Nobel Foundation's newly created statutes were promulgated by King Oscar II.[8][11] According to Nobel's will, The Royal Swedish Academy of Sciences would award the Prize in Physics.[11]
A maximum of three Nobel laureates and two different works may be selected for the Nobel Prize in Physics.[12][13] Compared with other Nobel Prizes, the nomination and selection process for the prize in Physics is long and rigorous. This is a key reason why it has grown in importance over the years to become the most important prize in Physics.[14]
The Nobel laureates are selected by the Nobel Committee for Physics, a Nobel Committee that consists of five members elected by The Royal Swedish Academy of Sciences. During the first stage that begins in September, a group of about 3,000 selected university professors, Nobel Laureates in Physics and Chemistry, and others are sent confidential nomination forms. The completed forms must arrive at the Nobel Committee by 31 January of the following year. The nominees are scrutinized and discussed by experts and are narrowed to approximately fifteen names. The committee submits a report with recommendations on the final candidates to the Academy, where, in the Physics Class, it is further discussed. The Academy then makes the final selection of the Laureates in Physics by a majority vote.[15]
The names of the nominees are never publicly announced, and neither are they told that they have been considered for the Prize. Nomination records are sealed for fifty years.[16] While posthumous nominations are not permitted, awards can be made if the individual died in the months between the decision of the committee (typically in October) and the ceremony in December. Prior to 1974, posthumous awards were permitted if the candidate had died after being nominated.[17]
The rules for the Nobel Prize in Physics require that the significance of achievements being recognized has been "tested by time". In practice, that means that the lag between the discovery and the award is typically on the order of 20 years and can be much longer. For example, half of the 1983 Nobel Prize in Physics was awarded to Subrahmanyan Chandrasekhar for his work on stellar structure and evolution that was done during the 1930s. As a downside of this tested-by-time rule, not all scientists live long enough for their work to be recognized. Some important scientific discoveries are never considered for a prize, as the discoverers die by the time the impact of their work is appreciated.[18][19]
A Physics Nobel Prize laureate earns a gold medal, a diploma bearing a citation, and a sum of money.[20]
The Nobel Prize medals, minted by Myntverket[21] in Sweden and the Mint of Norway since 1902, are registered trademarks of the Nobel Foundation. Each medal has an image of Alfred Nobel in left profile on the obverse. The Nobel Prize medals for Physics, Chemistry, Physiology or Medicine, and Literature have identical obverses, showing the image of Alfred Nobel and the years of his birth and death (18331896). Nobel's portrait also appears on the obverse of the Nobel Peace Prize medal and the Medal for the Prize in Economics, but with a slightly different design.[22][23] The image on the reverse of a medal varies according to the institution awarding the prize. The reverse sides of the Nobel Prize medals for Chemistry and Physics share the same design of the Goddess of Nature, whose veil is held up by the Genius of Science. These medals, along with those for Physiology/Medicine and Literature, were designed by Erik Lindberg in 1902.[24]
Nobel laureates receive a diploma directly from the hands of the King of Sweden. Each diploma is uniquely designed by the prize-awarding institutions for the laureate who receives it.[25] The diploma contains a picture with the name of the laureate and normally a citation explaining their accomplishments.[25]
At the awards ceremony, the laureate is given a document indicating the award sum. The amount of the cash award may differ from year to year, based on the funding available from the Nobel Foundation. For example, in 2009 the total cash awarded was 10million SEK (US$1.4million),[26] but in 2012 following the Great Recession, the amount was 8million Swedish Kronor, or US$1.1million.[27] If there are two laureates in a particular category, the award grant is divided equally between the recipients, but if there are three, the awarding committee may opt to divide the grant equally, or award half to one recipient and a quarter to each of the two others.[28][29][30][31]
The committee and institution serving as the selection board for the prize typically announce the names of the laureates during the first week of October. The prize is then awarded at formal ceremonies held annually in Stockholm Concert Hall on 10 December, the anniversary of Nobel's death. The laureates receive a diploma, a medal and a document confirming the prize amount.[32]
After Nobels death, the Nobel Foundation was set up to carry out the provisions of his will and to administer his funds. In his will, he had stipulated that four different institutionsthree Swedish and one Norwegianshould award the prizes. From Stockholm, the Royal Swedish Academy of Sciences confers the prizes for physics, chemistry, and economics, the Karolinska Institute confers the prize for physiology or medicine, and the Swedish Academy confers the prize for literature. The Norwegian Nobel Committee based in Oslo confers the prize for peace. The Nobel Foundation is the legal owner and functional administrator of the funds and serves as the joint administrative body of the prize-awarding institutions, but it is not concerned with the prize deliberations or decisions, which rest exclusively with the four institutions. 


Plasma (from Ancient Greek  'moldable substance')[1] is one of the four fundamental states of matter. It consists of a gas of ions atoms or molecules which have at least one orbital electron stripped (or an extra electron attached) and, thus, an electric charge.
It is the most abundant form of ordinary matter in the universe,[2] being mostly associated with stars,[3] including the Sun.[4][5]
It extends to the rarefied intracluster medium and possibly to intergalactic regions.[6]
Plasma was first systematically studied by Irving Langmuir in the 1920s.[7][8] It can be artificially generated by heating a neutral gas or subjecting it to a strong electromagnetic field. The presence of charged particles makes plasma electrically conductive, with the dynamics of individual particles and macroscopic plasma motion governed by collective electromagnetic fields and very sensitive to externally applied fields.[9] The response of plasma to electromagnetic fields is used in many modern technological devices, such as plasma televisions or plasma etching.[10]
Depending on temperature and density, a certain amount of neutral particles may also be present, in which case plasma is called partially ionized. Neon signs and lightning are examples of partially ionized plasmas.[11]
Unlike the phase transitions between the other three states of matter, the transition to plasma is not well defined and is a matter of interpretation and context.[12] Whether a given degree of ionization suffices to call a substance 'plasma' depends on the specific phenomenon being considered.
Plasma was first identified in laboratory by Sir William Crookes. Crookes presented a lecture on what he called "radiant matter" to the British Association for the Advancement of Science, in Sheffield, on Friday, 22 August 1879.[13]
Systematic studies of plasma began with the research of Irving Langmuir and his colleagues in the 1920s. Langmuir also introduced the term "plasma" as a description of ionized gas in 1928:[14]
Except near the electrodes, where there are sheaths containing very few electrons, the ionized gas contains ions and electrons in about equal numbers so that the resultant space charge is very small. We shall use the name plasma to describe this region containing balanced charges of ions and electrons.Lewi Tonks and Harold Mott-Smith, both of whom worked with Langmuir in the 1920s, recall that Langmuir first used the term by analogy with the blood plasma.[15][16] Mott-Smith recalls, in particular, that the transport of electrons from thermionic filaments reminded Langmuir of "the way blood plasma carries red and white corpuscles and germs."[17]
Plasma is called the fourth state of matter after solid, liquid, and gas.[18][19][20]
It is a state of matter in which an ionized substance becomes highly electrically conductive to the point that long-range electric and magnetic fields dominate its behaviour.[21][22]
Plasma is typically an electrically quasineutral medium of unbound positive and negative particles (i.e. the overall charge of a plasma is roughly zero). Although these particles are unbound, they are not "free" in the sense of not experiencing forces. Moving charged particles generate electric currents, and any movement of a charged plasma particle affects and is affected by the fields created by the other charges. In turn this governs collective behaviour with many degrees of variation.[23][24]
Plasma is distinct from the other states of matter. In particular, describing a low-density plasma as merely an "ionized gas" is wrong and misleading, even though it is similar to the gas phase in that both assume no definite shape or volume. The following table summarizes some principal differences:
Three factors define an ideal plasma:[26][27]
The strength and range of the electric force and the good conductivity of plasmas usually ensure that the densities of positive and negative charges in any sizeable region are equal ("quasineutrality"). A plasma with a significant excess of charge density, or, in the extreme case, is composed of a single species, is called a non-neutral plasma. In such a plasma, electric fields play a dominant role. Examples are charged particle beams, an electron cloud in a Penning trap and positron plasmas.[32]
A dusty plasma contains tiny charged particles of dust (typically found in space). The dust particles acquire high charges and interact with each other. A plasma that contains larger particles is called grain plasma. Under laboratory conditions, dusty plasmas are also called complex plasmas.[33]
For plasma to exist, ionization is necessary. The term "plasma density" by itself usually refers to the electron density 




n

e




{\displaystyle n_{e}}

, that is, the number of charge-contributing electrons per unit volume. The degree of ionization 






{\displaystyle \alpha }

 is defined as fraction of neutral particles that are ionized:
where 




n

i




{\displaystyle n_{i}}

 is the ion density and 




n

n




{\displaystyle n_{n}}

 the neutral density (in number of particles per unit volume). In the case of fully ionized matter, 




=
1


{\displaystyle \alpha =1}

.
Because of the quasineutrality of plasma, the electron and ion densities are related by 




n

e


=


Z

i




n

i




{\displaystyle n_{e}=\langle Z_{i}\rangle n_{i}}

,
where 





Z

i





{\displaystyle \langle Z_{i}\rangle }

 is the average ion charge (in units of the elementary charge).
Plasma temperature, commonly measured in kelvin or electronvolts, is a measure of the thermal kinetic energy per particle. High temperatures are usually needed to sustain ionization, which is a defining feature of a plasma. The degree of plasma ionization is determined by the electron temperature relative to the ionization energy (and more weakly by the density). In thermal equilibrium, the relationship is given by the Saha equation. At low temperatures, ions and electrons tend to recombine into bound statesatoms[35]and the plasma will eventually become a gas.
In most cases, the electrons and heavy plasma particles (ions and neutral atoms) separately have a relatively well-defined temperature; that is, their energy distribution function is close to a Maxwellian even in the presence of strong electric or magnetic fields. However, because of the large difference in mass between electrons and ions, their temperatures may be different, sometimes significantly so. This is especially common in weakly ionized technological plasmas, where the ions are often near the ambient temperature while electrons reach thousands of kelvin.[citation needed] The opposite case is the z-pinch plasma where the ion temperature may exceed that of electrons.[36]
Since plasmas are very good electrical conductors, electric potentials play an important role.[clarification needed] The average potential in the space between charged particles, independent of how it can be measured, is called the "plasma potential", or the "space potential". If an electrode is inserted into a plasma, its potential will generally lie considerably below the plasma potential due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of "quasineutrality", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (




n

e


=

Z


n

i




{\displaystyle n_{e}=\langle Z\rangle n_{i}}

), but on the scale of the Debye length, there can be charge imbalance. In the special case that double layers are formed, the charge separation can extend some tens of Debye lengths.[38]
The magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:
Differentiating this relation provides a means to calculate the electric field from the density:
It is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small, otherwise, it will be dissipated by the repulsive electrostatic force.[39]
The existence of charged particles causes the plasma to generate, and be affected by, magnetic fields.
Plasma with a magnetic field strong enough to influence the motion of the charged particles is said to be magnetized. A common quantitative criterion is that a particle on average completes at least one gyration around the magnetic-field line before making a collision, i.e., 







c
e




/





c
o
l
l



>
1


{\displaystyle \nu _{\mathrm {ce} }/\nu _{\mathrm {coll} }>1}

, where 







c
e





{\displaystyle \nu _{\mathrm {ce} }}

 is the electron gyrofrequency and 







c
o
l
l





{\displaystyle \nu _{\mathrm {coll} }}

 is the electron collision rate. It is often the case that the electrons are magnetized while the ions are not. Magnetized plasmas are anisotropic, meaning that their properties in the direction parallel to the magnetic field are different from those perpendicular to it. While electric fields in plasmas are usually small due to the plasma high conductivity, the electric field associated with a plasma moving with velocity 




v



{\displaystyle \mathbf {v} }

 in the magnetic field 




B



{\displaystyle \mathbf {B} }

 is given by the usual Lorentz formula 




E

=


v



B



{\displaystyle \mathbf {E} =-\mathbf {v} \times \mathbf {B} }

, and is not affected by Debye shielding.[40]
To completely describe the state of a plasma, all of the
particle locations and velocities that describe the electromagnetic field in the plasma region would need to be written down.
However, it is generally not practical or necessary to keep track of all the particles in a plasma.[citation needed]
Therefore, plasma physicists commonly use less detailed descriptions, of which
there are two main types:
Fluid models describe plasmas in terms of smoothed quantities, like density and averaged velocity around each position (see Plasma parameters). One simple fluid model, magnetohydrodynamics, treats the plasma as a single fluid governed by a combination of Maxwell's equations and the NavierStokes equations. A more general description is the two-fluid plasma,[42] where the ions and electrons are described separately. Fluid models are often accurate when collisionality is sufficiently high to keep the plasma velocity distribution close to a MaxwellBoltzmann distribution. Because fluid models usually describe the plasma in terms of a single flow at a certain temperature at each spatial location, they can neither capture velocity space structures like beams or double layers, nor resolve wave-particle effects.[citation needed]
Kinetic models describe the particle velocity distribution function at each point in the plasma and therefore do not need to assume a MaxwellBoltzmann distribution. A kinetic description is often necessary for collisionless plasmas. There are two common approaches to kinetic description of a plasma. One is based on representing the smoothed distribution function on a grid in velocity and position. The other, known as the particle-in-cell (PIC) technique, includes kinetic information by following the trajectories of a large number of individual particles. Kinetic models are generally more computationally intensive than fluid models. The Vlasov equation may be used to describe the dynamics of a system of charged particles interacting with an electromagnetic field.
In magnetized plasmas, a gyrokinetic approach can substantially reduce the computational expense of a fully kinetic simulation.[citation needed]
Plasmas are the object of study of the academic field of plasma science or plasma physics,[43] including sub-disciplines such as space plasma physics. It currently involves the following fields of active research and features across many journals, whose interest includes:

Plasmas can appear in nature in various forms and locations, which can be usefully broadly summarised in the following Table:
Plasmas are by far the most common phase of ordinary matter in the universe, both by mass and by volume.[48]
Above the Earth's surface, the ionosphere is a plasma,[49] and the magnetosphere contains plasma.[50] Within our Solar System, interplanetary space is filled with the plasma expelled via the solar wind, extending from the Sun's surface out to the heliopause. Furthermore, all the distant stars, and much of interstellar space or intergalactic space is also likely filled with plasma, albeit at very low densities. Astrophysical plasmas are also observed in Accretion disks around stars or compact objects like white dwarfs, neutron stars, or black holes in close binary star systems.[51] Plasma is associated with ejection of material in astrophysical jets, which have been observed with accreting black holes[52] or in active galaxies like M87's jet that possibly extends out to 5,000 light-years.[53]
Most artificial plasmas are generated by the application of electric and/or magnetic fields through a gas. Plasma generated in a laboratory setting and for industrial use can be generally categorized by:
Just like the many uses of plasma, there are several means for its generation. However, one principle is common to all of them: there must be energy input to produce and sustain it.[54] For this case, plasma is generated when an electric current is applied across a dielectric gas or fluid (an electrically non-conducting material) as can be seen in the adjacent image, which shows a discharge tube as a simple example (DC used for simplicity).[citation needed]
The potential difference and subsequent electric field pull the bound electrons (negative) toward the anode (positive electrode) while the cathode (negative electrode) pulls the nucleus.[55] As the voltage increases, the current stresses the material (by electric polarization) beyond its dielectric limit (termed strength) into a stage of electrical breakdown, marked by an electric spark, where the material transforms from being an insulator into a conductor (as it becomes increasingly ionized). The underlying process is the Townsend avalanche, where collisions between electrons and neutral gas atoms create more ions and electrons (as can be seen in the figure on the right). The first impact of an electron on an atom results in one ion and two electrons. Therefore, the number of charged particles increases rapidly (in the millions) only "after about 20 successive sets of collisions",[56] mainly due to a small mean free path (average distance travelled between collisions).[citation needed]
[citation needed]
With ample current density and ionization, this forms a luminous electric arc (a continuous electric discharge similar to lightning) between the electrodes.[Note 1] Electrical resistance along the continuous electric arc creates heat, which dissociates more gas molecules and ionizes the resulting atoms (where degree of ionization is determined by temperature), and as per the sequence: solid-liquid-gas-plasma, the gas is gradually turned into a thermal plasma.[Note 2] A thermal plasma is in thermal equilibrium, which is to say that the temperature is relatively homogeneous throughout the heavy particles (i.e. atoms, molecules and ions) and electrons. This is so because when thermal plasmas are generated, electrical energy is given to electrons, which, due to their great mobility and large numbers, are able to disperse it rapidly and by elastic collision (without energy loss) to the heavy particles.[57][Note 3]
Because of their sizable temperature and density ranges, plasmas find applications in many fields of research, technology and industry. For example, in: industrial and extractive metallurgy,[57][58] surface treatments such as plasma spraying (coating), etching in microelectronics,[59] metal cutting[60] and welding; as well as in everyday vehicle exhaust cleanup and fluorescent/luminescent lamps,[54] fuel ignition, while even playing a part in supersonic combustion engines for aerospace engineering.[61]
A world effort was triggered in the 1960s to study magnetohydrodynamic converters in order to bring MHD power conversion to market with commercial power plants of a new kind, converting the kinetic energy of a high velocity plasma into electricity with no moving parts at a high efficiency. Research was also conducted in the field of supersonic and hypersonic aerodynamics to study plasma interaction with magnetic fields to eventually achieve passive and even active flow control around vehicles or projectiles, in order to soften and mitigate shock waves, lower thermal transfer and reduce drag.[citation needed]
Such ionized gases used in "plasma technology" ("technological" or "engineered" plasmas) are usually weakly ionized gases in the sense that only a tiny fraction of the gas molecules are ionized.[72] These kinds of weakly ionized gases are also nonthermal "cold" plasmas. In the presence of magnetics fields, the study of such magnetized nonthermal weakly ionized gases involves resistive magnetohydrodynamics with low magnetic Reynolds number, a challenging field of plasma physics where calculations require dyadic tensors in a 7-dimensional phase space. When used in combination with a high Hall parameter, a critical value triggers the problematic electrothermal instability which limited these technological developments.[citation needed]
Although the underlying equations governing plasmas are relatively simple, plasma behaviour is extraordinarily varied and subtle: the emergence of unexpected behaviour from a simple model is a typical feature of a complex system. Such systems lie in some sense on the boundary between ordered and disordered behaviour and cannot typically be described either by simple, smooth, mathematical functions, or by pure randomness. The spontaneous formation of interesting spatial features on a wide range of length scales is one manifestation of plasma complexity. The features are interesting, for example, because they are very sharp, spatially intermittent (the distance between features is much larger than the features themselves), or have a fractal form. Many of these features were first studied in the laboratory, and have subsequently been recognized throughout the universe.[citation needed] Examples of complexity and complex structures in plasmas include:
Striations or string-like structures,[73] also known as Birkeland currents, are seen in many plasmas, like the plasma ball, the aurora,[74] lightning,[75] electric arcs, solar flares,[76] and supernova remnants.[77] They are sometimes associated with larger current densities, and the interaction with the magnetic field can form a magnetic rope structure.[78] High power microwave breakdown at atmospheric pressure also leads to the formation of filamentary structures.[79] (See also Plasma pinch)
Filamentation also refers to the self-focusing of a high power laser pulse. At high powers, the nonlinear part of the index of refraction becomes important and causes a higher index of refraction in the center of the laser beam, where the laser is brighter than at the edges, causing a feedback that focuses the laser even more. The tighter focused laser has a higher peak brightness (irradiance) that forms a plasma. The plasma has an index of refraction lower than one, and causes a defocusing of the laser beam. The interplay of the focusing index of refraction, and the defocusing plasma makes the formation of a long filament of plasma that can be micrometers to kilometers in length.[80] One interesting aspect of the filamentation generated plasma is the relatively low ion density due to defocusing effects of the ionized electrons.[81] (See also Filament propagation)
Impermeable plasma is a type of thermal plasma which acts like an impermeable solid with respect to gas or cold plasma and can be physically pushed. Interaction of cold gas and thermal plasma was briefly studied by a group led by Hannes Alfvn in 1960s and 1970s for its possible applications in insulation of fusion plasma from the reactor walls.[82] However, later it was found that the external magnetic fields in this configuration could induce kink instabilities in the plasma and subsequently lead to an unexpectedly high heat loss to the walls.[83]
In 2013, a group of materials scientists reported that they have successfully generated stable impermeable plasma with no magnetic confinement using only an ultrahigh-pressure blanket of cold gas. While spectroscopic data on the characteristics of plasma were claimed to be difficult to obtain due to the high pressure, the passive effect of plasma on synthesis of different nanostructures clearly suggested the effective confinement. They also showed that upon maintaining the impermeability for a few tens of seconds, screening of ions at the plasma-gas interface could give rise to a strong secondary mode of heating (known as viscous heating) leading to different kinetics of reactions and formation of complex nanomaterials.[84]
Hall effect thruster. The electric field in a plasma double layer is so effective at accelerating ions that electric fields are used in ion drives.
Solar plasma
Plasma spraying
Tokamak plasma in nuclear fusion research
Argon Plasma in the Hawkeye Linearly Magnetized Experiment (HLMX) at the University of Iowa


Theoretical physics is a branch of physics that employs mathematical models and abstractions of physical objects and systems to rationalize, explain and predict natural phenomena. This is in contrast to experimental physics, which uses experimental tools to probe these phenomena.
The advancement of science generally depends on the interplay between experimental studies and theory. In some cases, theoretical physics adheres to standards of mathematical rigour while giving little weight to experiments and observations.[a] For example, while developing special relativity, Albert Einstein was concerned with the Lorentz transformation which left Maxwell's equations invariant, but was apparently uninterested in the MichelsonMorley experiment on Earth's drift through a luminiferous aether.[1] Conversely, Einstein was awarded the Nobel Prize for explaining the photoelectric effect, previously an experimental result lacking a theoretical formulation.[2]
A physical theory is a model of physical events. It is judged by the extent to which its predictions agree with empirical observations. The quality of a physical theory is also judged on its ability to make new predictions which can be verified by new observations. A physical theory differs from a mathematical theorem in that while both are based on some form of axioms, judgment of mathematical applicability is not based on agreement with any experimental results.[3][4] A physical theory similarly differs from a mathematical theory, in the sense that the word "theory" has a different meaning in mathematical terms.[b]





R
i
c

=
k

g


{\displaystyle \mathrm {Ric} =k\,g}


The equations for an Einstein manifold, used in general relativity to describe the curvature of spacetimeA physical theory involves one or more relationships between various measurable quantities. Archimedes realized that a ship floats by displacing its mass of water, Pythagoras understood the relation between the length of a vibrating string and the musical tone it produces.[5][6] Other examples include entropy as a measure of the uncertainty regarding the positions and motions of unseen particles and the quantum mechanical idea that (action and) energy are not continuously variable.
Theoretical physics consists of several different approaches. In this regard, theoretical particle physics forms a good example. For instance: "phenomenologists" might employ (semi-) empirical formulas and heuristics to agree with experimental results, often without deep physical understanding.[c] "Modelers" (also called "model-builders") often appear much like phenomenologists, but try to model speculative theories that have certain desirable features (rather than on experimental data), or apply the techniques of mathematical modeling  to physics problems.[d]  Some attempt to create approximate theories, called effective theories, because fully developed theories may be regarded as unsolvable or too complicated. Other theorists may try to unify, formalise, reinterpret or generalise extant theories, or create completely new ones altogether.[e] Sometimes the vision provided by pure mathematical systems can provide clues to how a physical system might be modeled;[f] e.g., the notion, due to Riemann and others, that space itself might be curved. Theoretical problems that need computational investigation are often the concern of computational physics.
Theoretical advances may consist in setting aside old, incorrect paradigms (e.g., aether theory of light propagation, caloric theory of heat, burning consisting of evolving phlogiston, or astronomical bodies revolving around the Earth) or may be an alternative model that provides answers that are more accurate or that can be more widely applied. In the latter case, a correspondence principle will be required to recover the previously known result.[7][8]  Sometimes though, advances may proceed along different paths. For example, an essentially correct theory may need some conceptual or factual revisions; atomic theory, first postulated millennia ago (by several thinkers in Greece and India) and the two-fluid theory of electricity[9]  are two cases in this point. However, an exception to all the above is the waveparticle duality,  a theory combining aspects of different, opposing models via the Bohr complementarity principle.
Physical theories become accepted if they are able to make correct predictions and no (or few) incorrect ones. The theory should have, at least as a secondary objective, a certain economy and elegance (compare to mathematical beauty), a notion sometimes called "Occam's razor" after the 13th-century English philosopher William of Occam (or Ockham), in which the simpler of two theories that describe the same matter just as adequately is preferred (but conceptual simplicity may mean mathematical complexity).[10] They are also more likely to be accepted if they connect a wide range of phenomena. Testing the consequences of a theory is part of the scientific method.
Physical theories can be grouped into three categories: mainstream theories, proposed theories and fringe theories.
Theoretical physics began at least 2,300 years ago, under the Pre-socratic philosophy, and continued by Plato and Aristotle, whose views held sway for a millennium. During the rise of medieval universities, the only acknowledged intellectual disciplines were the seven liberal arts of the Trivium like grammar, logic, and rhetoric and of the Quadrivium like arithmetic, geometry, music and astronomy. During the Middle Ages and Renaissance, the concept of experimental science, the counterpoint to theory, began with scholars such as Ibn al-Haytham and Francis Bacon. As the Scientific Revolution gathered pace, the concepts of matter, energy, space, time and causality slowly began to acquire the form we know today, and other sciences spun off from the rubric of natural philosophy. Thus began the modern era of theory with the Copernican paradigm shift in astronomy, soon followed by Johannes Kepler's expressions for planetary orbits, which summarized the meticulous observations of Tycho Brahe; the works of these men (alongside Galileo's) can perhaps be considered to constitute the Scientific Revolution.
The great push toward the modern concept of explanation started with Galileo, one of the few physicists who was both a consummate theoretician and a great experimentalist. The analytic geometry and mechanics of Descartes were incorporated into the calculus and mechanics of Isaac Newton, another theoretician/experimentalist of the highest order, writing Principia Mathematica.[11] In it contained a grand synthesis of the work of Copernicus, Galileo and Kepler; as well as Newton's theories of mechanics and gravitation, which held sway as worldviews until the early 20th century. Simultaneously, progress was also made in optics (in particular colour theory and the ancient science of geometrical optics), courtesy of Newton, Descartes and the Dutchmen Snell and Huygens. In the 18th and 19th centuries Joseph-Louis Lagrange, Leonhard Euler and William Rowan Hamilton would extend the theory of classical mechanics considerably.[12] They picked up the interactive intertwining of mathematics and physics begun two millennia earlier by Pythagoras.
Among the great conceptual achievements of the 19th and 20th centuries were the consolidation of the idea of energy (as well as its global conservation) by the inclusion of heat, electricity and magnetism, and then light. The laws of thermodynamics, and most importantly the introduction of the singular concept of entropy began to provide a macroscopic explanation for the properties of matter. Statistical mechanics (followed by statistical physics and Quantum statistical mechanics) emerged as an offshoot of thermodynamics late in the 19th century. Another important event in the 19th century was the discovery of electromagnetic theory, unifying the previously separate phenomena of electricity, magnetism and light.
The pillars of modern physics, and perhaps the most revolutionary theories in the history of physics, have been relativity theory and quantum mechanics. Newtonian mechanics was subsumed under special relativity and Newton's gravity was given a kinematic explanation by general relativity. Quantum mechanics led to an understanding of blackbody radiation (which indeed, was an original motivation for the theory) and of anomalies in the specific heats of solids  and finally to an understanding of the internal structures of atoms and molecules. Quantum mechanics soon gave way to the formulation of quantum field theory (QFT), begun in the late 1920s. In the aftermath of World War 2, more progress brought much renewed interest in QFT, which had since the early efforts, stagnated.  The same period also saw fresh attacks on the problems of superconductivity and phase transitions, as well as the first applications of QFT in the area of theoretical condensed matter. The 1960s and 70s saw the formulation of the Standard model of particle physics using QFT and progress in condensed matter physics (theoretical foundations of superconductivity and critical phenomena, among others), in parallel to the applications of relativity to problems in astronomy and cosmology respectively.
All of these achievements depended on the theoretical physics as a moving force both to suggest experiments and to consolidate results  often by ingenious application of existing mathematics, or, as in the case of Descartes and Newton (with Leibniz), by inventing new mathematics. Fourier's studies of heat conduction led to a new branch of mathematics: infinite, orthogonal series.[13]
Modern theoretical physics attempts to unify theories and explain phenomena in further attempts to understand the Universe, from the cosmological to the elementary particle scale. Where experimentation cannot be done, theoretical physics still tries to advance through the use of mathematical models.
Mainstream theories (sometimes referred to as central theories) are the body of knowledge of both factual and scientific views and possess a usual scientific quality of the tests of repeatability, consistency with existing well-established science and experimentation. There do exist mainstream theories that are generally accepted theories based solely upon their effects explaining a wide variety of data, although the detection, explanation, and possible composition are subjects of debate.
The proposed theories of physics are usually relatively new theories which deal with the study of physics which include scientific approaches, means for determining the validity of models and new types of reasoning used to arrive at the theory. However, some proposed theories include theories that have been around for decades and have eluded methods of discovery and testing. Proposed theories can include fringe theories in the process of becoming established (and, sometimes, gaining wider acceptance). Proposed theories usually have not been tested. In addition to the theories like those listed below, there are also different interpretations of quantum mechanics, which may or may not be considered different theories since it is debatable whether they yield different predictions for physical experiments, even in principle.
Fringe theories include any new area of scientific endeavor in the process of becoming established and some proposed theories. It can include speculative sciences. This includes physics fields and physical theories presented in accordance with known evidence, and a body of associated predictions have been made according to that theory.
Some fringe theories go on to become a widely accepted part of physics. Other fringe theories end up being disproven. Some fringe theories are a form of protoscience and others are a form of pseudoscience. The falsification of the original theory sometimes leads to reformulation of the theory.
"Thought" experiments are situations created in one's mind, asking a question akin to "suppose you are in this situation, assuming such is true, what would follow?". They are usually created to investigate phenomena that are not readily experienced in every-day situations. Famous examples of such thought experiments are Schrdinger's cat, the EPR thought experiment, simple illustrations of time dilation, and so on. These usually lead to real experiments designed to verify that the conclusion (and therefore the assumptions) of the thought experiments are correct. The EPR thought experiment led to the Bell inequalities, which were then tested to various degrees of rigor, leading to the acceptance of the current formulation of quantum mechanics and probabilism as a working hypothesis.


Quantum mechanics is a fundamental theory in physics that provides a description of the physical properties of nature at the scale of atoms and subatomic particles.[2]:1.1 It is the foundation of all quantum physics including quantum chemistry, quantum field theory, quantum technology, and quantum information science.
Classical physics, the collection of theories that existed before the advent of quantum mechanics, describes many aspects of nature at an ordinary (macroscopic) scale, but is not sufficient for describing them at small (atomic and subatomic) scales. Most theories in classical physics can be derived from quantum mechanics as an approximation valid at large (macroscopic) scale.[3]
Quantum mechanics differs from classical physics in that energy, momentum, angular momentum, and other quantities of a bound system are restricted to discrete values (quantization), objects have characteristics of both particles and waves (wave-particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).
Quantum mechanics arose gradually from theories to explain observations which could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the "old quantum theory", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrdinger, Werner Heisenberg, Max Born and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and sub-atomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms,[4] but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative.[5] Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.[note 1]
A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrdinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.
One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between different measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.
Another consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[6]:102111[2]:1.11.8 The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen  a result that would not be expected if light consisted of classical particles.[6] However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).[6]:109[7][8] However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through. Other atomic-scale entities, such as electrons, are found to exhibit the same behavior when fired towards a double slit.[2] This behavior is known as wave-particle duality.
Another counter-intuitive phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential.[9] In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy and the tunnel diode.[10]
When quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrdinger called entanglement "...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought".[11] Quantum entanglement enables the counter-intuitive properties of quantum pseudo-telepathy, and can be a valuable resource in communication protocols, such as quantum key distribution and superdense coding.[12] Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.[12]
Another possibility opened by entanglement is testing for "hidden variables", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory can provide. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed, using entangled particles, and they have shown results incompatible with the constraints imposed by local hidden variables.[13][14]
It is not possible to present these concepts in more than a superficial way without introducing the actual mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects.[note 2] Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.
In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector 






{\displaystyle \psi }

 belonging to a (separable) complex Hilbert space 





H




{\displaystyle {\mathcal {H}}}

. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys 





,


=
1


{\displaystyle \langle \psi ,\psi \rangle =1}

, and it is well-defined up to a complex number of modulus 1 (the global phase), that is, 






{\displaystyle \psi }

 and 




e

i






{\displaystyle e^{i\alpha }\psi }

 represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions 




L

2


(

C

)


{\displaystyle L^{2}(\mathbb {C} )}

, while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors 





C


2




{\displaystyle \mathbb {C} ^{2}}

 with the usual inner product.
Physical quantities of interest  position, momentum, energy, spin  are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue 






{\displaystyle \lambda }

 is non-degenerate and the probability is given by 




|










,




|


2




{\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}

, where 













{\displaystyle {\vec {\lambda }}}

 is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by 





,

P








{\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }

, where 




P






{\displaystyle P_{\lambda }}

 is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.
After the measurement, if result 






{\displaystyle \lambda }

 was obtained, the quantum state is postulated to collapse to 













{\displaystyle {\vec {\lambda }}}

, in the non-degenerate case, or to 




P






/





,

P










{\displaystyle P_{\lambda }\psi /{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}

, in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous BohrEinstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of "wave function collapse" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity. For details, see the article on measurement in quantum mechanics.[17]
The time evolution of a quantum state is described by the Schrdinger equation:
Here 



H


{\displaystyle H}

 denotes the Hamiltonian, the observable corresponding to the total energy of the system, and 






{\displaystyle \hbar }

 is the reduced Planck constant.  The constant 



i



{\displaystyle i\hbar }

 is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.
The solution of this differential equation is given by
The operator 



U
(
t
)
=

e


i
H
t

/






{\displaystyle U(t)=e^{-iHt/\hbar }}

 is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that given an initial quantum state 




(
0
)


{\displaystyle \psi (0)}

  it makes a definite prediction of what the quantum state 




(
t
)


{\displaystyle \psi (t)}

 will be at any later time.[18]
Some wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).
Analytic solutions of the Schrdinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom which contains just two electrons has defied all attempts at a fully analytic treatment.
However, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy. Another method is called "semi-classical equation of motion", which applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion. This approach is particularly important in the field of quantum chaos.
One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum.[19][20] Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator 






X
^





{\displaystyle {\hat {X}}}

 and momentum operator 






P
^





{\displaystyle {\hat {P}}}

 do not commute, but rather satisfy the canonical commutation relation:
Given a quantum state, the Born rule lets us compute expectation values for both 



X


{\displaystyle X}

 and 



P


{\displaystyle P}

, and moreover for powers of them. Defining 
the uncertainty for an observable by a standard deviation, we have
and likewise for the momentum:
The uncertainty principle states that
Either standard deviation can in principle be made arbitrarily small, but not both simultaneously.[21] This inequality generalizes to arbitrary pairs of self-adjoint operators 



A


{\displaystyle A}

 and 



B


{\displaystyle B}

. The commutator of these two operators is
and this provides the lower bound on the product of standard deviations:
Another consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an 



i

/




{\displaystyle i/\hbar }

 factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum 




p

i




{\displaystyle p_{i}}

 is replaced by 




i






x





{\displaystyle -i\hbar {\frac {\partial }{\partial x}}}

, and in particular in the non-relativistic Schrdinger equation in position space the momentum-squared term is replaced with a Laplacian times 







2




{\displaystyle -\hbar ^{2}}

.[19]
When two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces 






H



A




{\displaystyle {\mathcal {H}}_{A}}

 and 






H



B




{\displaystyle {\mathcal {H}}_{B}}

, respectively. The Hilbert space of the composite system is then
If the state for the first system is the vector 






A




{\displaystyle \psi _{A}}

 and the state for the second system is 






B




{\displaystyle \psi _{B}}

, then the state of the composite system is
Not all states in the joint Hilbert space 






H



A
B




{\displaystyle {\mathcal {H}}_{AB}}

 can be written in this form, however, because the superposition principle implies that linear combinations of these "separable" or "product states" are also valid. For example, if 






A




{\displaystyle \psi _{A}}

 and 






A




{\displaystyle \phi _{A}}

 are both possible states for system 



A


{\displaystyle A}

, and likewise  






B




{\displaystyle \psi _{B}}

 and 






B




{\displaystyle \phi _{B}}

 are both possible states for system 



B


{\displaystyle B}

, then
is a valid joint state that is not separable. States that are not separable are called entangled.[22][23]
If the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system.[22][23] Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.[22][24]
As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.[25]
There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrdinger).[26] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.
The Hamiltonian 



H


{\displaystyle H}

 is known as the generator of time evolution, since it defines a unitary time-evolution operator 



U
(
t
)
=

e


i
H
t

/






{\displaystyle U(t)=e^{-iHt/\hbar }}

 for each value of 



t


{\displaystyle t}

. From this relation between 



U
(
t
)


{\displaystyle U(t)}

 and 



H


{\displaystyle H}

, it follows that any observable 



A


{\displaystyle A}

 that commutes with 



H


{\displaystyle H}

 will be conserved: its expectation value will not change over time. This statement generalizes, as mathematically, any Hermitian operator 



A


{\displaystyle A}

 can generate a family of unitary operators parameterized by a variable 



t


{\displaystyle t}

. Under the evolution generated by 



A


{\displaystyle A}

, any observable 



B


{\displaystyle B}

 that commutes with 



A


{\displaystyle A}

 will be conserved. Moreover, if 



B


{\displaystyle B}

 is conserved by evolution under 



A


{\displaystyle A}

, then 



A


{\displaystyle A}

 is conserved under the evolution generated by 



B


{\displaystyle B}

. This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law.
The simplest example of quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:
The general solution of the Schrdinger equation is given by
which is a superposition of all possible plane waves 




e

i
(
k
x






k

2




2
m



t
)




{\displaystyle e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}}

, which are eigenstates of the momentum operator with momentum 



p
=

k


{\displaystyle p=\hbar k}

. The coefficients of the superposition are 







^



(
k
,
0
)


{\displaystyle {\hat {\psi }}(k,0)}

, which is the Fourier transform of the initial quantum state 




(
x
,
0
)


{\displaystyle \psi (x,0)}

.
It is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states.[note 3] Instead, we can consider a Gaussian wave packet:
which has Fourier transform, and therefore momentum distribution
We see that as we make 



a


{\displaystyle a}

 smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making 



a


{\displaystyle a}

 larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.
As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.[27]
The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region.[19]:7778 For the one-dimensional case in the 



x


{\displaystyle x}

 direction, the time-independent Schrdinger equation may be written
With the differential operator defined by
the previous equation is evocative of the classic kinetic energy analogue,
with state 






{\displaystyle \psi }

 in this case having energy 



E


{\displaystyle E}

 coincident with the kinetic energy of the particle.
The general solutions of the Schrdinger equation for the particle in a box are
or, from Euler's formula,
The infinite potential walls of the box determine the values of 



C
,
D
,


{\displaystyle C,D,}

 and 



k


{\displaystyle k}

 at 



x
=
0


{\displaystyle x=0}

 and 



x
=
L


{\displaystyle x=L}

 where 






{\displaystyle \psi }

 must be zero. Thus, at 



x
=
0


{\displaystyle x=0}

,
and 



D
=
0


{\displaystyle D=0}

. At 



x
=
L


{\displaystyle x=L}

,
in which 



C


{\displaystyle C}

 cannot be zero as this would conflict with the postulate that 






{\displaystyle \psi }

 has norm 1. Therefore, since 



sin

(
k
L
)
=
0


{\displaystyle \sin(kL)=0}

, 



k
L


{\displaystyle kL}

 must be an integer multiple of 






{\displaystyle \pi }

,
This constraint on 



k


{\displaystyle k}

 implies a constraint on the energy levels, yielding





E

n


=






2





2



n

2




2
m

L

2





=




n

2



h

2




8
m

L

2





.


{\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.}


A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy.
As in the classical case, the potential for the quantum harmonic oscillator is given by
This problem can either be treated by directly solving the Schrdinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given by
where Hn are the Hermite polynomials
and the corresponding energy levels are
This is another example illustrating the discretization of energy for bound states.
The MachZehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the ElitzurVaidman bomb tester, and in studies of quantum entanglement.[28][29]
We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the "lower" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the "upper" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector 







C


2




{\displaystyle \psi \in \mathbb {C} ^{2}}

 that is a superposition of the "lower" path 






l


=


(



1




0



)




{\displaystyle \psi _{l}={\begin{pmatrix}1\\0\end{pmatrix}}}

 and the "upper" path 






u


=


(



0




1



)




{\displaystyle \psi _{u}={\begin{pmatrix}0\\1\end{pmatrix}}}

, that is, 




=




l


+




u




{\displaystyle \psi =\alpha \psi _{l}+\beta \psi _{u}}

 for complex 




,



{\displaystyle \alpha ,\beta }

. In order to respect the postulate that 





,


=
1


{\displaystyle \langle \psi ,\psi \rangle =1}

 we require that 




|




|


2


+

|




|


2


=
1


{\displaystyle |\alpha |^{2}+|\beta |^{2}=1}

.
Both beam splitters are modelled as the unitary matrix 



B
=


1

2





(



1


i




i


1



)




{\displaystyle B={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&i\\i&1\end{pmatrix}}}

, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of 



1

/



2




{\displaystyle 1/{\sqrt {2}}}

, or be reflected to the other path with a probability amplitude of 



i

/



2




{\displaystyle i/{\sqrt {2}}}

. The phase shifter on the upper arm is modelled as the unitary matrix 



P
=


(



1


0




0



e

i







)




{\displaystyle P={\begin{pmatrix}1&0\\0&e^{i\Delta \Phi }\end{pmatrix}}}

, which means that if the photon is on the "upper" path it will gain a relative phase of 







{\displaystyle \Delta \Phi }

, and it will stay unchanged if it is in the lower path.
A photon that enters the interferometer from the left will then be acted upon with a beam splitter 



B


{\displaystyle B}

, a phase shifter 



P


{\displaystyle P}

, and another beam splitter 



B


{\displaystyle B}

, and so end up in the state 
and the probabilities that it will be detected at the right or at the top are given respectively by
One can therefore use the MachZehnder interferometer to estimate the phase shift by estimating these probabilities.
It is interesting to consider what would happen if the photon were definitely in either the "lower" or "upper" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases there will be no interference between the paths anymore, and the probabilities are given by 



p
(
u
)
=
p
(
l
)
=
1

/

2


{\displaystyle p(u)=p(l)=1/2}

, independently of the phase 







{\displaystyle \Delta \Phi }

. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.[30]
Quantum mechanics has had enormous success in explaining many of the features of our universe, with regards to small-scale and discrete quantities and interactions which cannot be explained by classical methods.[note 4] Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.[31]
In many aspects modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.[32] Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.
The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers.[33] One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.
When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.
Complications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.
Quantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations. Quantum coherence is not typically evident at macroscopic scales, except maybe at temperatures approaching absolute zero at which quantum behavior may manifest macroscopically.[note 5]
Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.[34]
Early attempts to merge quantum mechanics with special relativity involved the replacement of the Schrdinger equation with a covariant equation such as the KleinGordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.[35][36]
The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical 






e

2



/

(
4







0




r
)



{\displaystyle \textstyle -e^{2}/(4\pi \epsilon _{_{0}}r)}

 Coulomb potential. This "semi-classical" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.
Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg.[37]
Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.
One proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.[38][39]
Another popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric "woven" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.6161035 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[40]
Is there a preferred interpretation of quantum mechanics? How does the quantum description of reality, which includes elements such as the "superposition of states" and "wave function collapse", give rise to the reality we perceive?Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics."[41] According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[42]
The views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the "Copenhagen interpretation".[43][44] According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of "causality". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations remain popular in the 21st century.[45]
Albert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the BohrEinstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the EinsteinPodolskyRosen paradox.[note 6] In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles.[50] Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.[13][14]
Bohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrdinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.[51]
Everett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[52] This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we don't observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule,[53][54] with no consensus on whether they have been successful.[55][56][57]
Relational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas,[58] and QBism was developed some years later.[59]
Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[60] In 1803 English polymath Thomas Young described the famous double-slit experiment.[61] This experiment played a major role in the general acceptance of the wave theory of light.
During the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics.[62] While the early conception of atoms from Greek philosophy had been that they were indivisible units  the word "atom" deriving from the Greek for "uncuttable"  the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Plcker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.[63][64]
The black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation.[65] The word quantum derives from the Latin, meaning "how great" or "how much".[66] According to Planck, quantities of energy could be thought of as divided into "elements" whose size (E) would be proportional to their frequency ():
where h is Planck's constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation.[67] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[68] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen.[69] Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency.[70]  In his paper "On the Quantum Theory of Radiation," Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation,[71] which became the basis of the laser.
This phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics.[72] The theory is now understood as a semi-classical approximation[73] to modern quantum mechanics.[74] Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.
In the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan[75][76] developed matrix mechanics and the Austrian physicist Erwin Schrdinger invented wave mechanics. Born introduced the probabilistic interpretation of Schrdinger's wave function in July 1926.[77] Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[78]
By 1930 quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann[79] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors[80] and superfluids.[81]
The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.
More technical:
On Wikibooks

The following outline is provided as an overview of and topical guide to physics:
Physics  natural science that involves the study of matter[1] and its motion through spacetime, along with related concepts such as energy and force.[2] More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.[3][4][5]
Physics can be described as all of the following:
History of physics  history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force
Physics  branch of science that studies matter[9] and its motion through space and time, along with related concepts such as energy and force.[10]   Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology  etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:
Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment
Theoretical concepts:
Massenergy equivalence, elementary particle, physical law, fundamental force, physical constant
This is a list of the primary theories in physics, major subtopics, and concepts.
Index of physics articles


Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions, in addition to the study of other forms of nuclear matter.
Nuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.
Discoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.
Particle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.
The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896,[1] made while investigating phosphorescence in uranium salts.[2] The discovery of the electron by J. J. Thomson[3] a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.
In the years that followed, radioactivity was extensively investigated, notably by Marie Curie, Pierre Curie, Ernest Rutherford and others. By the turn of the century, physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.
The 1903 Nobel Prize in Physics was awarded jointly to Becquerel, for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".
In 1905, Albert Einstein formulated the idea of massenergy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.
In 1906, Ernest Rutherford published "Retardation of the  Particle from Radium in passing through matter."[4] Hans Geiger expanded on this work in a communication to the Royal Society[5] with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Ernest Marsden,[6] and further greatly expanded work was published in 1910 by Geiger.[7] In 19111912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.
Published in 1909,[8] with the eventual classical analysis by Rutherford published May 1911,[9][10][11][12] the key preemptive experiment was performed during 1909,[9][13][14][15][16] at the University of Manchester. Ernest Rutherford's assistant, Professor [15][16] Johannes [14] "Hans" Geiger, and an undergraduate, Marsden,[15][16] performed an experiment in which Geiger and Marsden under Rutherford's supervision fired alpha particles (helium 4 nuclei[17]) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.
Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars.[18][19] At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation E = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.
The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons each had a spin of +12. In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of 12. Rasetti discovered, however, that nitrogen-14 had a spin of 1.
In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irne and Frdric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle).[20] In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus  only protons and neutrons  and that neutrons were spin 12 particles, which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of 12 in the same direction, giving a final total spin of 1.
With the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein's calculation of the equivalence of mass and energy to within 1% as of 1934.
Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli[21] who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Frhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.[22][23][24][25][26]
In 1935 Hideki Yukawa[27] proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.
With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high-energy photons (gamma decay).
The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics, which describes the strong, weak, and electromagnetic forces.
A heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model,[28] the nucleus has an energy that arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.
Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer[29] and J. Hans D. Jensen.[30] Nuclei with certain "magic" numbers of neutrons and protons are particularly stable, because their shells are filled.
Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons.
Ab initio methods try to solve the nuclear many-body problem from the ground up, starting from the nucleons and their interactions.[31]
Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quarkgluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.
Eighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 252 stable nuclides. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years. Plotted on a chart as a function of atomic and neutron numbers, the binding energy of the nuclides forms what is known as the valley of stability. Stable nuclides lie along the bottom of this energy valley, while increasingly unstable nuclides lie up the valley walls, that is, have weaker binding energy.
The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay, a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons)[32] within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.
In alpha decay, which typically occurs in the heaviest nuclei, the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.
In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).
Other more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons but is not beta decay and (unlike beta decay) does not transmute one element to another.
In nuclear fusion, two low-mass nuclei come into very close contact with each other so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nuclei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction. Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.
Nuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.
The process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.
From several of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission-type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.
For a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago.[33] Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.[citation needed]
According to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).
Some relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the protonproton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.
Since the binding energy per nucleon peaks around iron (56 nucleons), energy is only released in fusion processes involving smaller atoms than that. Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a slow neutron capture process (the so-called s-process) or the rapid, or r-process. The s process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The r-process is thought to occur in supernova explosions, which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).


Physics is a branch of science whose primary objects of study are matter and energy. Discoveries of physics find applications throughout the natural sciences and in technology. Physics today may be divided loosely into classical physics and modern physics.
Elements of what became physics were drawn primarily from the fields of astronomy, optics, and mechanics, which were methodologically united through the study of geometry. These mathematical disciplines began in antiquity with the Babylonians and with Hellenistic writers such as Archimedes and Ptolemy. Ancient philosophy, meanwhile including what was called "physics"
The move towards a rational understanding of nature began at least since the Archaic period in Greece (650480 BCE) with the Pre-Socratic philosophers. The philosopher Thales of Miletus (7th and 6th centuries BCE), dubbed "the Father of Science" for refusing to accept various supernatural, religious or mythological explanations for natural phenomena, proclaimed that every event had a natural cause.[1] Thales also made advancements in 580 BCE by suggesting that water is the basic element, experimenting with the attraction between magnets and rubbed amber and formulating the first recorded cosmologies. Anaximander, famous for his proto-evolutionary theory, ., a substance called apeiron was the building block of all matter. Around 500 BCE, Heraclitus proposed that the only basic law governing the Universe was the principle of change and that nothing remains in the same state indefinitely. This observation made him one of the first scholars in ancient physics to address the role of time in the universe, a key and sometimes contentious concept in modern and present-day physics.[citation needed]
During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy slowly developed into an exciting and contentious field of study. Aristotle (Greek: , Aristotls) (384  322 BCE), a student of Plato, promoted the concept that observation of physical phenomena could ultimately lead to the discovery of the natural laws governing them.[citation needed] Aristotle's writings cover physics, metaphysics, poetry, theater, music, logic, rhetoric, linguistics, politics, government, ethics, biology and zoology. He wrote the first work which refers to that line of study as "Physics" in the 4th century BCE, Aristotle founded the system known as Aristotelian physics. He attempted to explain ideas such as motion (and gravity) with the theory of four elements. Aristotle believed that all matter was made up of aether, or some combination of four elements: earth, water, air, and fire. According to Aristotle, these four terrestrial elements are capable of inter-transformation and move toward their natural place, so a stone falls downward toward the center of the cosmos, but flames rise upward toward the circumference. Eventually, Aristotelian physics became enormously popular for many centuries in Europe, informing the scientific and scholastic developments of the Middle Ages. It remained the mainstream scientific paradigm in Europe until the time of Galileo Galilei and Isaac Newton.
Early in Classical Greece, knowledge that the Earth is spherical ("round") was common. Around 240 BCE, as the result of a seminal experiment, Eratosthenes (276194 BCE) accurately estimated its circumference. In contrast to Aristotle's geocentric views, Aristarchus of Samos (Greek: ; c.310 c.230 BCE) presented an explicit argument for a heliocentric model of the Solar System, i.e. for placing the Sun, not the Earth, at its centre. Seleucus of Seleucia, a follower of Aristarchus' heliocentric theory, stated that the Earth rotated around its own axis, which, in turn, revolved around the Sun. Though the arguments he used were lost, Plutarch stated that Seleucus was the first to prove the heliocentric system through reasoning.
In the 3rd century BCE, the Greek mathematician Archimedes of Syracuse (Greek:  (287212 BCE) generally considered to be the greatest mathematician of antiquity and one of the greatest of all time laid the foundations of hydrostatics, statics and calculated the underlying mathematics of the lever. A leading scientist of classical antiquity, Archimedes also developed elaborate systems of pulleys to move large objects with a minimum of effort. The Archimedes' screw underpins modern hydroengineering, and his machines of war helped to hold back the armies of Rome in the First Punic War. Archimedes even tore apart the arguments of Aristotle and his metaphysics, pointing out that it was impossible to separate mathematics and nature and proved it by converting mathematical theories into practical inventions. Furthermore, in his work On Floating Bodies, around 250 BCE, Archimedes developed the law of buoyancy, also known as Archimedes' principle. In mathematics, Archimedes used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers. He also developed the principles of equilibrium states and centers of gravity, ideas that would influence the well known scholars, Galileo, and Newton.
Hipparchus (190120 BCE), focusing on astronomy and mathematics, used sophisticated geometrical techniques to map the motion of the stars and planets, even predicting the times that Solar eclipses would happen. In addition, he added calculations of the distance of the Sun and Moon from the Earth, based upon his improvements to the observational instruments used at that time. Another of the most famous of the early physicists was Ptolemy (90168 CE), one of the leading minds during the time of the Roman Empire. Ptolemy was the author of several scientific treatises, at least three of which were of continuing importance to later Islamic and European science. The first is the astronomical treatise now known as the Almagest (in Greek,   , "The Great Treatise", originally  , "Mathematical Treatise"). The second is the Geography, which is a thorough discussion of the geographic knowledge of the Greco-Roman world.
Much of the accumulated knowledge of the ancient world was lost. Even of the works of the better known thinkers, few fragments survived. Although he wrote at least fourteen books, almost nothing of Hipparchus' direct work survived. Of the 150 reputed Aristotelian works, only 30 exist, and some of those are "little more than lecture notes"[according to whom?].
Important physical and mathematical traditions also existed in ancient Chinese and Indian sciences.
In Indian philosophy, Maharishi Kanada was the first to systematically develop a theory of atomism around 200 BCE[2] though some authors have allotted him an earlier era in the 6th century BCE.[3][4] It was further elaborated by the Buddhist atomists Dharmakirti and Dignga during the 1st millennium CE.[5] Pakudha Kaccayana, a 6th-century BCE Indian philosopher and contemporary of Gautama Buddha, had also propounded ideas about the atomic constitution of the material world. These philosophers believed that other elements (except ether) were physically palpable and hence comprised minuscule particles of matter. The last minuscule particle of matter that could not be subdivided further was termed Parmanu. These philosophers considered the atom to be indestructible and hence eternal. The Buddhists thought atoms to be minute objects unable to be seen to the naked eye that come into being and vanish in an instant. The Vaisheshika school of philosophers believed that an atom was a mere point in space. It was also first to depict relations between motion and force applied. Indian theories about the atom are greatly abstract and enmeshed in philosophy as they were based on logic and not on personal experience or experimentation. In Indian astronomy, Aryabhata's Aryabhatiya (499 CE) proposed the Earth's rotation, while Nilakantha Somayaji (14441544) of the Kerala school of astronomy and mathematics proposed a semi-heliocentric model resembling the Tychonic system.
The study of magnetism in Ancient China dates back to the 4th century BCE. (in the Book of the Devil Valley Master),[6] A main contributor to this field was Shen Kuo (10311095), a polymath and statesman who was the first to describe the magnetic-needle compass used for navigation, as well as establishing the concept of true north. In optics, Shen Kuo independently developed a camera obscura.[7]
In the 7th to 15th centuries, scientific progress occurred in the Muslim world. Many classic works in Indian, Assyrian, Sassanian (Persian) and Greek, including the works of Aristotle, were translated into Arabic.[8] Important contributions were made by Ibn al-Haytham (9651040), an Arab scientist, considered to be a founder of modern optics. Ptolemy and Aristotle theorised that light either shone from the eye to illuminate objects or that "forms" emanated from objects themselves, whereas al-Haytham (known by the Latin name "Alhazen") suggested that light travels to the eye in rays from different points on an object. The works of Ibn al-Haytham and Ab Rayhn Brn (9731050), a Persian scientist, eventually passed on to Western Europe where they were studied by scholars such as Roger Bacon and Witelo.[9]
Ibn al-Haytham and Biruni were early proponents of the scientific method. Ibn al-Haytham is considered to be the "father of the modern scientific method" due to his emphasis on experimental data and reproducibility of its results.[10][11] The earliest methodical approach to experiments in the modern sense is visible in the works of Ibn al-Haytham, who introduced an inductive-experimental method for achieving results.[12] Brn introduced early scientific methods for several different fields of inquiry during the 1020s and 1030s,[13] including an early experimental method for mechanics.[note 2] Biruni's methodology resembled the modern scientific method, particularly in his emphasis on repeated experimentation.[14]
Ibn Sn (9801037), known as "Avicenna", was a polymath from Bukhara (in present-day Uzbekistan) responsible for important contributions to physics, optics, philosophy and medicine. He published his theory of motion in Book of Healing (1020), where he argued that an impetus is imparted to a projectile by the thrower, and believed that it was a temporary virtue that would decline even in a vacuum. He viewed it as persistent, requiring external forces such as air resistance to dissipate it.[15][16][17] Ibn Sina made a distinction between 'force' and 'inclination' (called "mayl"), and argued that an object gained mayl when the object is in opposition to its natural motion. He concluded that continuation of motion is attributed to the inclination that is transferred to the object, and that object will be in motion until the mayl is spent. He also claimed that projectile in a vacuum would not stop unless it is acted upon. This conception of motion is consistent with Newton's first law of motion, inertia, which states that an object in motion will stay in motion unless it is acted on by an external force.[15] This idea which dissented from the Aristotelian view was later described as "impetus" by John Buridan, who was influenced by Ibn Sina's Book of Healing.[18]
Hibat Allah Abu'l-Barakat al-Baghdaadi (c. 1080-1165) adopted and modified Ibn Sina's theory on projectile motion. In his Kitab al-Mu'tabar, Abu'l-Barakat stated that the mover imparts a violent inclination (mayl qasri) on the moved and that this diminishes as the moving object distances itself from the mover.[19] He also proposed an explanation of the acceleration of falling bodies by the accumulation of successive increments of power with successive increments of velocity.[20] According to Shlomo Pines, al-Baghdaadi's theory of motion was "the oldest negation of Aristotle's fundamental dynamic law [namely, that a constant force produces a uniform motion], [and is thus an] anticipation in a vague fashion of the fundamental law of classical mechanics [namely, that a force applied continuously produces acceleration]."[21] Jean Buridan and Albert of Saxony later referred to Abu'l-Barakat in explaining that the acceleration of a falling body is a result of its increasing impetus.[19]
Ibn Bajjah (c. 10851138), known as "Avempace" in Europe, proposed that for every force there is always a reaction force. Ibn Bajjah was a critic of Ptolemy and he worked on creating a new theory of velocity to replace the one theorized by Aristotle. Two future philosophers supported the theories Avempace created, known as Avempacean dynamics. These philosophers were Thomas Aquinas, a Catholic priest, and John Duns Scotus.[22] Galileo went on to adopt Avempace's formula "that the velocity of a given object is the difference of the motive power of that object and the resistance of the medium of motion".[22]
Nasir al-Din al-Tusi (12011274), a Persian astronomer and mathematician who died in Baghdad introduced the Tusi couple. Copernicus later drew heavily on the work of al-Din al-Tusi and his students, but without acknowledgment.[23]
Awareness of ancient works re-entered the West through translations from Arabic to Latin. Their re-introduction, combined with Judeo-Islamic theological commentaries, had a great influence on Medieval philosophers such as Thomas Aquinas. Scholastic European scholars, who sought to reconcile the philosophy of the ancient classical philosophers with Christian theology, proclaimed Aristotle the greatest thinker of the ancient world. In cases where they didn't directly contradict the Bible, Aristotelian physics became the foundation for the physical explanations of the European Churches. Quantification became a core element of medieval physics.[24]

Based on Aristotelian physics, Scholastic physics described things as moving according to their essential nature. Celestial objects were described as moving in circles, because perfect circular motion was considered an innate property of objects that existed in the uncorrupted realm of the celestial spheres. The theory of impetus, the ancestor to the concepts of inertia and momentum, was developed along similar lines by medieval philosophers such as John Philoponus and Jean Buridan. Motions below the lunar sphere were seen as imperfect, and thus could not be expected to exhibit consistent motion. More idealized motion in the "sublunary" realm could only be achieved through artifice, and prior to the 17th century, many did not view artificial experiments as a valid means of learning about the natural world. Physical explanations in the sublunary realm revolved around tendencies. Stones contained the element earth, and earthly objects tended to move in a straight line toward the centre of the earth (and the universe in the Aristotelian geocentric view) unless otherwise prevented from doing so.[25]
During the 16th and 17th centuries, a large advancement of scientific progress known as the Scientific revolution took place in Europe. Dissatisfaction with older philosophical approaches had begun earlier and had produced other changes in society, such as the Protestant Reformation, but the revolution in science began when natural philosophers began to mount a sustained attack on the Scholastic philosophical programme and supposed that mathematical descriptive schemes adopted from such fields as mechanics and astronomy could actually yield universally valid characterizations of motion and other concepts.
A breakthrough in astronomy was made by Polish astronomer Nicolaus Copernicus (14731543) when, in 1543, he gave strong arguments for the heliocentric model of the Solar System, ostensibly as a means to render tables charting planetary motion more accurate and to simplify their production. In heliocentric models of the Solar system, the Earth orbits the Sun along with other bodies in Earth's galaxy, a contradiction according to the Greek-Egyptian astronomer Ptolemy (2nd century CE; see above), whose system placed the Earth at the center of the Universe and had been accepted for over 1,400 years. The Greek astronomer Aristarchus of Samos (c.310 c.230 BCE) had suggested that the Earth revolves around the Sun, but Copernicus' reasoning led to lasting general acceptance of this "revolutionary" idea. Copernicus' book presenting the theory (De revolutionibus orbium coelestium, "On the Revolutions of the Celestial Spheres") was published just before his death in 1543 and, as it is now generally considered to mark the beginning of modern astronomy, is also considered to mark the beginning of the Scientific revolution.[citation needed] Copernicus' new perspective, along with the accurate observations made by Tycho Brahe, enabled German astronomer Johannes Kepler (15711630) to formulate his laws regarding planetary motion that remain in use today.
The Italian mathematician, astronomer, and physicist Galileo Galilei (15641642) was famous for his support for Copernicanism, his astronomical discoveries, empirical experiments and his improvement of the telescope. As a mathematician, Galileo's role in the university culture of his era was subordinated to the three major topics of study: law, medicine, and theology (which was closely allied to philosophy). Galileo, however, felt that the descriptive content of the technical disciplines warranted philosophical interest, particularly because mathematical analysis of astronomical observations notably, Copernicus' analysis of the relative motions of the Sun, Earth, Moon, and planets indicated that philosophers' statements about the nature of the universe could be shown to be in error. Galileo also performed mechanical experiments, insisting that motion itself regardless of whether it was produced "naturally" or "artificially" (i.e. deliberately) had universally consistent characteristics that could be described mathematically.
Galileo's early studies at the University of Pisa were in medicine, but he was soon drawn to mathematics and physics. At 19, he discovered (and, subsequently, verified) the isochronal nature of the pendulum when, using his pulse, he timed the oscillations of a swinging lamp in Pisa's cathedral and found that it remained the same for each swing regardless of the swing's amplitude. He soon became known through his invention of a hydrostatic balance and for his treatise on the center of gravity of solid bodies. While teaching at the University of Pisa (158992), he initiated his experiments concerning the laws of bodies in motion that brought results so contradictory to the accepted teachings of Aristotle that strong antagonism was aroused. He found that bodies do not fall with velocities proportional to their weights. The famous story in which Galileo is said to have dropped weights from the Leaning Tower of Pisa is apocryphal, but he did find that the path of a projectile is a parabola and is credited with conclusions that anticipated Newton's laws of motion (e.g. the notion of inertia). Among these is what is now called Galilean relativity, the first precisely formulated statement about properties of space and time outside three-dimensional geometry.[citation needed]
Galileo has been called the "father of modern observational astronomy",[26] the "father of modern physics",[27] the "father of science",[27] and "the father of modern science".[28] According to Stephen Hawking, "Galileo, perhaps more than any other single person, was responsible for the birth of modern science."[29] As religious orthodoxy decreed a geocentric or Tychonic understanding of the Solar system, Galileo's support for heliocentrism provoked controversy and he was tried by the Inquisition. Found "vehemently suspect of heresy", he was forced to recant and spent the rest of his life under house arrest.
The contributions that Galileo made to observational astronomy include the telescopic confirmation of the phases of Venus; his discovery, in 1609, of Jupiter's four largest moons (subsequently given the collective name of the "Galilean moons"); and the observation and analysis of sunspots. Galileo also pursued applied science and technology, inventing, among other instruments, a military compass. His discovery of the Jovian moons was published in 1610 and enabled him to obtain the position of mathematician and philosopher to the Medici court. As such, he was expected to engage in debates with philosophers in the Aristotelian tradition and received a large audience for his own publications such as the Discourses and Mathematical Demonstrations Concerning Two New Sciences (published abroad following his arrest for the publication of Dialogue Concerning the Two Chief World Systems) and The Assayer.[30][31] Galileo's interest in experimenting with and formulating mathematical descriptions of motion established experimentation as an integral part of natural philosophy. This tradition, combining with the non-mathematical emphasis on the collection of "experimental histories" by philosophical reformists such as William Gilbert and Francis Bacon, drew a significant following in the years leading up to and following Galileo's death, including Evangelista Torricelli and the participants in the Accademia del Cimento in Italy; Marin Mersenne and Blaise Pascal in France; Christiaan Huygens in the Netherlands; and Robert Hooke and Robert Boyle in England.
The French philosopher Ren Descartes (15961650) was well-connected to, and influential within, the experimental philosophy networks of the day. Descartes had a more ambitious agenda, however, which was geared toward replacing the Scholastic philosophical tradition altogether. Questioning the reality interpreted through the senses, Descartes sought to re-establish philosophical explanatory schemes by reducing all perceived phenomena to being attributable to the motion of an invisible sea of "corpuscles". (Notably, he reserved human thought and God from his scheme, holding these to be separate from the physical universe). In proposing this philosophical framework, Descartes supposed that different kinds of motion, such as that of planets versus that of terrestrial objects, were not fundamentally different, but were merely different manifestations of an endless chain of corpuscular motions obeying universal principles. Particularly influential were his explanations for circular astronomical motions in terms of the vortex motion of corpuscles in space (Descartes argued, in accord with the beliefs, if not the methods, of the Scholastics, that a vacuum could not exist), and his explanation of gravity in terms of corpuscles pushing objects downward.[32][33][34]
Descartes, like Galileo, was convinced of the importance of mathematical explanation, and he and his followers were key figures in the development of mathematics and geometry in the 17th century. Cartesian mathematical descriptions of motion held that all mathematical formulations had to be justifiable in terms of direct physical action, a position held by Huygens and the German philosopher Gottfried Leibniz, who, while following in the Cartesian tradition, developed his own philosophical alternative to Scholasticism, which he outlined in his 1714 work, The Monadology. Descartes has been dubbed the 'Father of Modern Philosophy', and much subsequent Western philosophy is a response to his writings, which are studied closely to this day. In particular, his Meditations on First Philosophy continues to be a standard text at most university philosophy departments. Descartes' influence in mathematics is equally apparent; the Cartesian coordinate system  allowing algebraic equations to be expressed as geometric shapes in a two-dimensional coordinate system  was named after him. He is credited as the father of analytical geometry, the bridge between algebra and geometry, important to the discovery of calculus and analysis.
The late 17th and early 18th centuries saw the achievements of Cambridge University physicist and mathematician Sir Isaac Newton (1642-1727). Newton, a fellow of the Royal Society of England, combined his own discoveries in mechanics and astronomy to earlier ones to create a single system for describing the workings of the universe. Newton formulated three laws of motion which formulated the relationship between motion and objects and also the law of universal gravitation, the latter of which could be used to explain the behavior not only of falling bodies on the earth but also planets and other celestial bodies. To arrive at his results, Newton invented one form of an entirely new branch of mathematics: calculus (also invented independently by Gottfried Leibniz), which was to become an essential tool in much of the later development in most branches of physics. Newton's findings were set forth in his Philosophi Naturalis Principia Mathematica ("Mathematical Principles of Natural Philosophy"), the publication of which in 1687 marked the beginning of the modern period of mechanics and astronomy.
Newton was able to refute the Cartesian mechanical tradition that all motions should be explained with respect to the immediate force exerted by corpuscles. Using his three laws of motion and law of universal gravitation, Newton removed the idea that objects followed paths determined by natural shapes and instead demonstrated that not only regularly observed paths, but all the future motions of any body could be deduced mathematically based on knowledge of their existing motion, their mass, and the forces acting upon them. However, observed celestial motions did not precisely conform to a Newtonian treatment, and Newton, who was also deeply interested in theology, imagined that God intervened to ensure the continued stability of the solar system.
Newton's principles (but not his mathematical treatments) proved controversial with Continental philosophers, who found his lack of metaphysical explanation for movement and gravitation philosophically unacceptable. Beginning around 1700, a bitter rift opened between the Continental and British philosophical traditions, which were stoked by heated, ongoing, and viciously personal disputes between the followers of Newton and Leibniz concerning priority over the analytical techniques of calculus, which each had developed independently. Initially, the Cartesian and Leibnizian traditions prevailed on the Continent (leading to the dominance of the Leibnizian calculus notation everywhere except Britain). Newton himself remained privately disturbed at the lack of a philosophical understanding of gravitation while insisting in his writings that none was necessary to infer its reality. As the 18th century progressed, Continental natural philosophers increasingly accepted the Newtonians' willingness to forgo ontological metaphysical explanations for mathematically described motions.[35][36][37]
Newton built the first functioning reflecting telescope[38] and developed a theory of color, published in Opticks, based on the observation that a prism decomposes white light into the many colours forming the visible spectrum. While Newton explained light as being composed of tiny particles, a rival theory of light which explained its behavior in terms of waves was presented in 1690 by Christiaan Huygens. However, the belief in the mechanistic philosophy coupled with Newton's reputation meant that the wave theory saw relatively little support until the 19th century. Newton also formulated an empirical law of cooling, studied the speed of sound, investigated power series, demonstrated the generalised binomial theorem and developed a method for approximating the roots of a function. His work on infinite series was inspired by Simon Stevin's decimals.[39] Most importantly, Newton showed that the motions of objects on Earth and of celestial bodies are governed by the same set of natural laws, which were neither capricious nor malevolent. By demonstrating the consistency between Kepler's laws of planetary motion and his own theory of gravitation, Newton also removed the last doubts about heliocentrism. By bringing together all the ideas set forth during the Scientific revolution, Newton effectively established the foundation for modern society in mathematics and science.
Other branches of physics also received attention during the period of the Scientific revolution. William Gilbert, court physician to Queen Elizabeth I, published an important work on magnetism in 1600, describing how the earth itself behaves like a giant magnet. Robert Boyle (162791) studied the behavior of gases enclosed in a chamber and formulated the gas law named for him; he also contributed to physiology and to the founding of modern chemistry. Another important factor in the scientific revolution was the rise of learned societies and academies in various countries. The earliest of these were in Italy and Germany and were short-lived. More influential were the Royal Society of England (1660) and the Academy of Sciences in France (1666). The former was a private institution in London and included such scientists as John Wallis, William Brouncker, Thomas Sydenham, John Mayow, and Christopher Wren (who contributed not only to architecture but also to astronomy and anatomy); the latter, in Paris, was a government institution and included as a foreign member the Dutchman Huygens. In the 18th century, important royal academies were established at Berlin (1700) and at St. Petersburg (1724). The societies and academies provided the principal opportunities for the publication and discussion of scientific results during and after the scientific revolution. In 1690, James Bernoulli showed that the cycloid is the solution to the tautochrone problem; and the following year, in 1691, Johann Bernoulli showed that a chain freely suspended from two points will form a catenary, the curve with the lowest possible center of gravity available to any chain hung between two fixed points. He then showed, in 1696, that the cycloid is the solution to the brachistochrone problem.
A precursor of the engine was designed by the German scientist Otto von Guericke who, in 1650, designed and built the world's first vacuum pump and created the world's first ever vacuum known as the Magdeburg hemispheres experiment. He was driven to make a vacuum to disprove Aristotle's long-held supposition that 'Nature abhors a vacuum'. Shortly thereafter, Irish physicist and chemist Boyle had learned of Guericke's designs and in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed the pressure-volume correlation for a gas: PV = k, where P is pressure, V is volume and k is a constant: this relationship is known as Boyle's Law. In that time, air was assumed to be a system of motionless particles, and not interpreted as a system of moving molecules. The concept of thermal motion came two centuries later. Therefore, Boyle's publication in 1660 speaks about a mechanical concept: the air spring.[40] Later, after the invention of the thermometer, the property temperature could be quantified. This tool gave Gay-Lussac the opportunity to derive his law, which led shortly later to the ideal gas law. But, already before the establishment of the ideal gas law, an associate of Boyle's named Denis Papin built in 1679 a bone digester, which is a closed vessel with a tightly fitting lid that confines steam until a high pressure is generated.
Later designs implemented a steam release valve to keep the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and cylinder engine. He did not however follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. Hence, prior to 1698 and the invention of the Savery Engine, horses were used to power pulleys, attached to buckets, which lifted water out of flooded salt mines in England. In the years to follow, more variations of steam engines were built, such as the Newcomen Engine, and later the Watt Engine. In time, these early engines would eventually be utilized in place of horses. Thus, each engine began to be associated with a certain amount of "horse power" depending upon how many horses it had replaced. The main problem with these first engines was that they were slow and clumsy, converting less than 2% of the input fuel into useful work. In other words, large quantities of coal (or wood) had to be burned to yield only a small fraction of work output. Hence the need for a new science of engine dynamics was born.
During the 18th century, the mechanics founded by Newton was developed by several scientists as more mathematicians learned calculus and elaborated upon its initial formulation. The application of mathematical analysis to problems of motion was known as rational mechanics, or mixed mathematics (and was later termed classical mechanics).
In 1714, Brook Taylor derived the fundamental frequency of a stretched vibrating string in terms of its tension and mass per unit length by solving a differential equation. The Swiss mathematician Daniel Bernoulli (17001782) made important mathematical studies of the behavior of gases, anticipating the kinetic theory of gases developed more than a century later, and has been referred to as the first mathematical physicist.[41] In 1733, Daniel Bernoulli derived the fundamental frequency and harmonics of a hanging chain by solving a differential equation. In 1734, Bernoulli solved the differential equation for the vibrations of an elastic bar clamped at one end. Bernoulli's treatment of fluid dynamics and his examination of fluid flow was introduced in his 1738 work Hydrodynamica.
Rational mechanics dealt primarily with the development of elaborate mathematical treatments of observed motions, using Newtonian principles as a basis, and emphasized improving the tractability of complex calculations and developing of legitimate means of analytical approximation. A representative contemporary textbook was published by Johann Baptiste Horvath. By the end of the century analytical treatments were rigorous enough to verify the stability of the Solar System solely on the basis of Newton's laws without reference to divine interventioneven as deterministic treatments of systems as simple as the three body problem in gravitation remained intractable.[42] In 1705, Edmond Halley predicted the periodicity of Halley's Comet, William Herschel discovered Uranus in 1781, and Henry Cavendish measured the gravitational constant and determined the mass of the Earth in 1798. In 1783, John Michell suggested that some objects might be so massive that not even light could escape from them.
In 1739, Leonhard Euler solved the ordinary differential equation for a forced harmonic oscillator and noticed the resonance phenomenon. In 1742, Colin Maclaurin discovered his uniformly rotating self-gravitating spheroids. In 1742, Benjamin Robins published his New Principles in Gunnery, establishing the science of aerodynamics. British work, carried on by mathematicians such as Taylor and Maclaurin, fell behind Continental developments as the century progressed. Meanwhile, work flourished at scientific academies on the Continent, led by such mathematicians as Bernoulli, Euler, Lagrange, Laplace, and Legendre. In 1743, Jean le Rond d'Alembert published his Traite de Dynamique, in which he introduced the concept of generalized forces for accelerating systems and systems with constraints, and applied the new idea of virtual work to solve dynamical problem, now known as D'Alembert's principle, as a rival to Newton's second law of motion. In 1747, Pierre Louis Maupertuis applied minimum principles to mechanics. In 1759, Euler solved the partial differential equation for the vibration of a rectangular drum. In 1764, Euler examined the partial differential equation for the vibration of a circular drum and found one of the Bessel function solutions. In 1776, John Smeaton published a paper on experiments relating power, work, momentum and kinetic energy, and supporting the conservation of energy. In 1788, Joseph Louis Lagrange presented Lagrange's equations of motion in Mcanique Analytique, in which the whole of mechanics was organized around the principle of virtual work. In 1789, Antoine Lavoisier states the law of conservation of mass. The rational mechanics developed in the 18th century received a brilliant exposition in both Lagrange's 1788 work and the Celestial Mechanics (17991825) of Pierre-Simon Laplace.
During the 18th century, thermodynamics was developed through the theories of weightless "imponderable fluids", such as heat ("caloric"), electricity, and phlogiston (which was rapidly overthrown as a concept following Lavoisier's identification of oxygen gas late in the century). Assuming that these concepts were real fluids, their flow could be traced through a mechanical apparatus or chemical reactions. This tradition of experimentation led to the development of new kinds of experimental apparatus, such as the Leyden Jar; and new kinds of measuring instruments, such as the calorimeter, and improved versions of old ones, such as the thermometer. Experiments also produced new concepts, such as the University of Glasgow experimenter Joseph Black's notion of latent heat and Philadelphia intellectual Benjamin Franklin's characterization of electrical fluid as flowing between places of excess and deficit (a concept later reinterpreted in terms of positive and negative charges). Franklin also showed that lightning is electricity in 1752.
The accepted theory of heat in the 18th century viewed it as a kind of fluid, called caloric; although this theory was later shown to be erroneous, a number of scientists adhering to it nevertheless made important discoveries useful in developing the modern theory, including Joseph Black (172899) and Henry Cavendish (17311810). Opposed to this caloric theory, which had been developed mainly by the chemists, was the less accepted theory dating from Newton's time that heat is due to the motions of the particles of a substance. This mechanical theory gained support in 1798 from the cannon-boring experiments of Count Rumford (Benjamin Thompson), who found a direct relationship between heat and mechanical energy.
While it was recognized early in the 18th century that finding absolute theories of electrostatic and magnetic force akin to Newton's principles of motion would be an important achievement, none were forthcoming. This impossibility only slowly disappeared as experimental practice became more widespread and more refined in the early years of the 19th century in places such as the newly established Royal Institution in London. Meanwhile, the analytical methods of rational mechanics began to be applied to experimental phenomena, most influentially with the French mathematician Joseph Fourier's analytical treatment of the flow of heat, as published in 1822.[43][44][45] Joseph Priestley proposed an electrical inverse-square law in 1767, and Charles-Augustin de Coulomb introduced the inverse-square law of electrostatics in 1798.
At the end of the century, the members of the French Academy of Sciences had attained clear dominance in the field.[37][46][47][48] At the same time, the experimental tradition established by Galileo and his followers persisted. The Royal Society and the French Academy of Sciences were major centers for the performance and reporting of experimental work. Experiments in mechanics, optics, magnetism, static electricity, chemistry, and physiology were not clearly distinguished from each other during the 18th century, but significant differences in explanatory schemes and, thus, experiment design were emerging. Chemical experimenters, for instance, defied attempts to enforce a scheme of abstract Newtonian forces onto chemical affiliations, and instead focused on the isolation and classification of chemical substances and reactions.[49]
In 1821, William Hamilton began his analysis of Hamilton's characteristic function. 
In 1835, he stated Hamilton's canonical equations of motion. 
In 1813, Peter Ewart supported the idea of the conservation of energy in his paper On the measure of moving force. 
In 1829, Gaspard Coriolis introduced the terms of work (force times distance) and kinetic energy with the meanings they have today. 
In 1841, Julius Robert von Mayer, an amateur scientist, wrote a paper on the conservation of energy, although his lack of academic training led to its rejection. 
In 1847, Hermann von Helmholtz formally stated the law of conservation of energy.
In 1800, Alessandro Volta invented the electric battery (known as the voltaic pile) and thus improved the way electric currents could also be studied. A year later, Thomas Young demonstrated the wave nature of lightwhich received strong experimental support from the work of Augustin-Jean Fresneland the principle of interference. 
In 1820, Hans Christian rsted found that a current-carrying conductor gives rise to a magnetic force surrounding it, and within a week after rsted's discovery reached France, Andr-Marie Ampre discovered that two parallel electric currents will exert forces on each other. 
In 1821, Michael Faraday built an electricity-powered motor, while Georg Ohm stated his law of electrical resistance in 1826, expressing the relationship between voltage, current, and resistance in an electric circuit.
In 1831, Faraday (and independently Joseph Henry) discovered the reverse effect, the production of an electric potential or current through magnetism known as electromagnetic induction; these two discoveries are the basis of the electric motor and the electric generator, respectively.
In the 19th century, the connection between heat and mechanical energy was established quantitatively by Julius Robert von Mayer and James Prescott Joule, who measured the mechanical equivalent of heat in the 1840s. In 1849, Joule published results from his series of experiments (including the paddlewheel experiment) which show that heat is a form of energy, a fact that was accepted in the 1850s. The relation between heat and energy was important for the development of steam engines, and in 1824 the experimental and theoretical work of Sadi Carnot was published. Carnot captured some of the ideas of thermodynamics in his discussion of the efficiency of an idealized engine. Sadi Carnot's work provided a basis for the formulation of the first law of thermodynamicsa restatement of the law of conservation of energywhich was stated around 1850 by William Thomson, later known as Lord Kelvin, and Rudolf Clausius. Lord Kelvin, who had extended the concept of absolute zero from gases to all substances in 1848, drew upon the engineering theory of Lazare Carnot, Sadi Carnot, and mile Clapeyronas well as the experimentation of James Prescott Joule on the interchangeability of mechanical, chemical, thermal, and electrical forms of workto formulate the first law.
Kelvin and Clausius also stated the second law of thermodynamics, which was originally formulated in terms of the fact that heat does not spontaneously flow from a colder body to a hotter. Other formulations followed quickly (for example, the second law was expounded in Thomson and Peter Guthrie Tait's influential work Treatise on Natural Philosophy) and Kelvin in particular understood some of the law's general implications. The second Law was the idea that gases consist of molecules in motion had been discussed in some detail by Daniel Bernoulli in 1738, but had fallen out of favor, and was revived by Clausius in 1857. In 1850, Hippolyte Fizeau and Lon Foucault measured the speed of light in water and find that it is slower than in air, in support of the wave model of light. In 1852, Joule and Thomson demonstrated that a rapidly expanding gas cools, later named the JouleThomson effect or JouleKelvin effect. Hermann von Helmholtz puts forward the idea of the heat death of the universe in 1854, the same year that Clausius established the importance of dQ/T (Clausius's theorem) (though he did not yet name the quantity).
In 1859, James Clerk Maxwell discovered the distribution law of molecular velocities. Maxwell showed that electric and magnetic fields are propagated outward from their source at a speed equal to that of light and that light is one of several kinds of electromagnetic radiation, differing only in frequency and wavelength from the others. In 1859, Maxwell worked out the mathematics of the distribution of velocities of the molecules of a gas. The wave theory of light was widely accepted by the time of Maxwell's work on the electromagnetic field, and afterward the study of light and that of electricity and magnetism were closely related. In 1864 James Maxwell published his papers on a dynamical theory of the electromagnetic field, and stated that light is an electromagnetic phenomenon in the 1873 publication of Maxwell's Treatise on Electricity and Magnetism. This work drew upon theoretical work by German theoreticians such as Carl Friedrich Gauss and Wilhelm Weber. The encapsulation of heat in particulate motion, and the addition of electromagnetic forces to Newtonian dynamics established an enormously robust theoretical underpinning to physical observations.
The prediction that light represented a transmission of energy in wave form through a "luminiferous ether", and the seeming confirmation of that prediction with Helmholtz student Heinrich Hertz's 1888 detection of electromagnetic radiation, was a major triumph for physical theory and raised the possibility that even more fundamental theories based on the field could soon be developed.[50][51][52][53] Experimental confirmation of Maxwell's theory was provided by Hertz, who generated and detected electric waves in 1886 and verified their properties, at the same time foreshadowing their application in radio, television, and other devices. In 1887, Heinrich Hertz discovered the photoelectric effect. Research on the electromagnetic waves began soon after, with many scientists and inventors conducting experiments on their properties. In the mid to late 1890s Guglielmo Marconi developed a radio wave based wireless telegraphy system
[54] (see invention of radio).
The atomic theory of matter had been proposed again in the early 19th century by the chemist John Dalton and became one of the hypotheses of the kinetic-molecular theory of gases developed by Clausius and James Clerk Maxwell to explain the laws of thermodynamics.
The kinetic theory in turn led to a revolutionary approach to science, the statistical mechanics of Ludwig Boltzmann (18441906) and Josiah Willard Gibbs (18391903), which studies the statistics of microstates of a system and uses statistics to determine the state of a physical system. Interrelating the statistical likelihood of certain states of organization of these particles with the energy of those states, Clausius reinterpreted the dissipation of energy to be the statistical tendency of molecular configurations to pass toward increasingly likely, increasingly disorganized states (coining the term "entropy" to describe the disorganization of a state). The statistical versus absolute interpretations of the second law of thermodynamics set up a dispute that would last for several decades (producing arguments such as "Maxwell's demon"), and that would not be held to be definitively resolved until the behavior of atoms was firmly established in the early 20th century.[55][56] In 1902, James Jeans found the length scale required for gravitational perturbations to grow in a static nearly homogeneous medium.
In 1822, botanist Robert Brown discovered Brownian motion: pollen grains in water undergoing movement resulting from their bombardment by the fast-moving atoms or molecules in the liquid. 
In 1834, Carl Jacobi discovered his uniformly rotating self-gravitating ellipsoids (the Jacobi ellipsoid). 
In 1834, John Russell observed a nondecaying solitary water wave (soliton) in the Union Canal near Edinburgh and used a water tank to study the dependence of solitary water wave velocities on wave amplitude and water depth. 
In 1835, Gaspard Coriolis examined theoretically the mechanical efficiency of waterwheels, and deduced the Coriolis effect. 
In 1842, Christian Doppler proposed the Doppler effect. 
In 1851, Lon Foucault showed the Earth's rotation with a huge pendulum (Foucault pendulum).
There were important advances in continuum mechanics in the first half of the century, namely formulation of laws of elasticity for solids and discovery of NavierStokes equations for fluids.
At the end of the 19th century, physics had evolved to the point at which classical mechanics could cope with highly complex problems involving macroscopic situations; thermodynamics and kinetic theory were well established; geometrical and physical optics could be understood in terms of electromagnetic waves; and the conservation laws for energy and momentum (and mass) were widely accepted. So profound were these and other developments that it was generally accepted that all the important laws of physics had been discovered and that, henceforth, research would be concerned with clearing up minor problems and particularly with improvements of method and measurement. However, around 1900 serious doubts arose about the completeness of the classical theoriesthe triumph of Maxwell's theories, for example, was undermined by inadequacies that had already begun to appearand their inability to explain certain physical phenomena, such as the energy distribution in blackbody radiation and the photoelectric effect, while some of the theoretical formulations led to paradoxes when pushed to the limit. Prominent physicists such as Hendrik Lorentz, Emil Cohn, Ernst Wiechert and Wilhelm Wien believed that some modification of Maxwell's equations might provide the basis for all physical laws. These shortcomings of classical physics were never to be resolved and new ideas were required. At the beginning of the 20th century a major revolution shook the world of physics, which led to a new era, generally referred to as modern physics.[57]
In the 19th century, experimenters began to detect unexpected forms of radiation: Wilhelm Rntgen caused a sensation with his discovery of X-rays in 1895; in 1896 Henri Becquerel discovered that certain kinds of matter emit radiation on their own accord. In 1897, J. J. Thomson discovered the electron, and new radioactive elements found by Marie and Pierre Curie raised questions about the supposedly indestructible atom and the nature of matter. Marie and Pierre coined the term "radioactivity" to describe this property of matter, and isolated the radioactive elements radium and polonium. Ernest Rutherford and Frederick Soddy identified two of Becquerel's forms of radiation with electrons and the element helium. Rutherford identified and named two types of radioactivity and in 1911 interpreted experimental evidence as showing that the atom consists of a dense, positively charged nucleus surrounded by negatively charged electrons. Classical theory, however, predicted that this structure should be unstable. Classical theory had also failed to explain successfully two other experimental results that appeared in the late 19th century. One of these was the demonstration by Albert A. Michelson and Edward W. Morleyknown as the MichelsonMorley experimentwhich showed there did not seem to be a preferred frame of reference, at rest with respect to the hypothetical luminiferous ether, for describing electromagnetic phenomena. Studies of radiation and radioactive decay continued to be a preeminent focus for physical and chemical research through the 1930s, when the discovery of nuclear fission by Lise Meitner and Otto Frisch opened the way to the practical exploitation of what came to be called "atomic" energy.
In 1905, a 26-year-old German physicist named Albert Einstein (then a patent clerk in Bern, Switzerland) showed how measurements of time and space are affected by motion between an observer and what is being observed. Einstein's radical theory of relativity revolutionized science. Although Einstein made many other important contributions to science, the theory of relativity alone represents one of the greatest intellectual achievements of all time. Although the concept of relativity was not introduced by Einstein, his major contribution was the recognition that the speed of light in a vacuum is constant, i.e. the same for all observers, and an absolute physical boundary for motion. This does not impact a person's day-to-day life since most objects travel at speeds much slower than light speed. For objects travelling near light speed, however, the theory of relativity shows that clocks associated with those objects will run more slowly and that the objects shorten in length according to measurements of an observer on Earth. Einstein also derived the famous equation, E = mc2, which expresses the equivalence of mass and energy.
Einstein argued that the speed of light was a constant in all inertial reference frames and that electromagnetic laws should remain valid independent of reference frameassertions which rendered the ether "superfluous" to physical theory, and that held that observations of time and length varied relative to how the observer was moving with respect to the object being measured (what came to be called the "special theory of relativity"). It also followed that mass and energy were interchangeable quantities according to the equation E=mc2. In another paper published the same year, Einstein asserted that electromagnetic radiation was transmitted in discrete quantities ("quanta"), according to a constant that the theoretical physicist Max Planck had posited in 1900 to arrive at an accurate theory for the distribution of blackbody radiationan assumption that explained the strange properties of the photoelectric effect.
The special theory of relativity is a formulation of the relationship between physical observations and the concepts of space and time. The theory arose out of contradictions between electromagnetism and Newtonian mechanics and had great impact on both those areas. The original historical issue was whether it was meaningful to discuss the electromagnetic wave-carrying "ether" and motion relative to it and also whether one could detect such motion, as was unsuccessfully attempted in the MichelsonMorley experiment. Einstein demolished these questions and the ether concept in his special theory of relativity. However, his basic formulation does not involve detailed electromagnetic theory. It arises out of the question: "What is time?" Newton, in the Principia (1686), had given an unambiguous answer: "Absolute, true, and mathematical time, of itself, and from its own nature, flows equably without relation to anything external, and by another name is called duration." This definition is basic to all classical physics.
Einstein had the genius to question it, and found that it was incomplete. Instead, each "observer" necessarily makes use of his or her own scale of time, and for two observers in relative motion, their time-scales will differ. This induces a related effect on position measurements. Space and time become intertwined concepts, fundamentally dependent on the observer. Each observer presides over his or her own space-time framework or coordinate system. There being no absolute frame of reference, all observers of given events make different but equally valid (and reconcilable) measurements. What remains absolute is stated in Einstein's relativity postulate: "The basic laws of physics are identical for two observers who have a constant relative velocity with respect to each other."
Special relativity had a profound effect on physics: started as a rethinking of the theory of electromagnetism, it found a new symmetry law of nature, now called Poincar symmetry, that replaced the old Galilean symmetry.

Special relativity exerted another long-lasting effect on dynamics. Although initially it was credited with the "unification of mass and energy", it became evident that relativistic dynamics established a firm distinction between rest mass, which is an invariant (observer independent) property of a particle or system of particles, and the energy and momentum of a system. The latter two are separately conserved in all situations but not invariant with respect to different observers. The term mass in particle physics underwent a semantic change, and since the late 20th century it almost exclusively denotes the rest (or invariant) mass. By 1916, Einstein was able to generalize this further, to deal with all states of motion including non-uniform acceleration, which became the general theory of relativity. In this theory Einstein also specified a new concept, the curvature of space-time, which described the gravitational effect at every point in space. In fact, the curvature of space-time completely replaced Newton's universal law of gravitation. According to Einstein, gravitational force in the normal sense is a kind of illusion caused by the geometry of space. The presence of a mass causes a curvature of space-time in the vicinity of the mass, and this curvature dictates the space-time path that all freely-moving objects must follow. It was also predicted from this theory that light should be subject to gravity - all of which was verified experimentally. This aspect of relativity explained the phenomena of light bending around the sun, predicted black holes as well as properties of the Cosmic microwave background radiation  a discovery rendering fundamental anomalies in the classic Steady-State hypothesis. For his work on relativity, the photoelectric effect and blackbody radiation, Einstein received the Nobel Prize in 1921.
The gradual acceptance of Einstein's theories of relativity and the quantized nature of light transmission, and of Niels Bohr's model of the atom created as many problems as they solved, leading to a full-scale effort to reestablish physics on new fundamental principles. Expanding relativity to cases of accelerating reference frames (the "general theory of relativity") in the 1910s, Einstein posited an equivalence between the inertial force of acceleration and the force of gravity, leading to the conclusion that space is curved and finite in size, and the prediction of such phenomena as gravitational lensing and the distortion of time in gravitational fields.
Although relativity resolved the electromagnetic phenomena conflict demonstrated by Michelson and Morley, a second theoretical problem was the explanation of the distribution of electromagnetic radiation emitted by a black body; experiment showed that at shorter wavelengths, toward the ultraviolet end of the spectrum, the energy approached zero, but classical theory predicted it should become infinite. This glaring discrepancy, known as the ultraviolet catastrophe, was solved by the new theory of quantum mechanics. Quantum mechanics is the theory of atoms and subatomic systems. Approximately the first 30 years of the 20th century represent the time of the conception and evolution of the theory. The basic ideas of quantum theory were introduced in 1900 by Max Planck (18581947), who was awarded the Nobel Prize for Physics in 1918 for his discovery of the quantified nature of energy. The quantum theory (which previously relied in the "correspondence" at large scales between the quantized world of the atom and the continuities of the "classical" world) was accepted when the Compton Effect established that light carries momentum and can scatter off particles, and when Louis de Broglie asserted that matter can be seen as behaving as a wave in much the same way as electromagnetic waves behave like particles (waveparticle duality).
In 1905, Einstein used the quantum theory to explain the photoelectric effect, and in 1913 the Danish physicist Niels Bohr used the same constant to explain the stability of Rutherford's atom as well as the frequencies of light emitted by hydrogen gas. The quantized theory of the atom gave way to a full-scale quantum mechanics in the 1920s. New principles of a "quantum" rather than a "classical" mechanics, formulated in matrix-form by Werner Heisenberg, Max Born, and Pascual Jordan in 1925, were based on the probabilistic relationship between discrete "states" and denied the possibility of causality. Quantum mechanics was extensively developed by Heisenberg, Wolfgang Pauli, Paul Dirac, and Erwin Schrdinger, who established an equivalent theory based on waves in 1926; but Heisenberg's 1927 "uncertainty principle" (indicating the impossibility of precisely and simultaneously measuring position and momentum) and the "Copenhagen interpretation" of quantum mechanics (named after Bohr's home city) continued to deny the possibility of fundamental causality, though opponents such as Einstein would metaphorically assert that "God does not play dice with the universe".[58] The new quantum mechanics became an indispensable tool in the investigation and explanation of phenomena at the atomic level. Also in the 1920s, the Indian scientist Satyendra Nath Bose's work on photons and quantum mechanics provided the foundation for BoseEinstein statistics, the theory of the BoseEinstein condensate.
 The spinstatistics theorem established that any particle in quantum mechanics may be either a boson (statistically BoseEinstein) or a fermion (statistically FermiDirac). It was later found that all fundamental bosons transmit forces, such as the photon that transmits electromagnetism.
Fermions are particles "like electrons and nucleons" and are the usual constituents of matter. FermiDirac statistics later found numerous other uses, from astrophysics (see Degenerate matter) to semiconductor design.
As the philosophically inclined continued to debate the fundamental nature of the universe, quantum theories continued to be produced, beginning with Paul Dirac's formulation of a relativistic quantum theory in 1928. However, attempts to quantize electromagnetic theory entirely were stymied throughout the 1930s by theoretical formulations yielding infinite energies. This situation was not considered adequately resolved until after World WarII ended, when Julian Schwinger, Richard Feynman and Sin-Itiro Tomonaga independently posited the technique of renormalization, which allowed for an establishment of a robust quantum electrodynamics (QED).[59]
Meanwhile, new theories of fundamental particles proliferated with the rise of the idea of the quantization of fields through "exchange forces" regulated by an exchange of short-lived "virtual" particles, which were allowed to exist according to the laws governing the uncertainties inherent in the quantum world. Notably, Hideki Yukawa proposed that the positive charges of the nucleus were kept together courtesy of a powerful but short-range force mediated by a particle with a mass between that of the electron and proton. This particle, the "pion", was identified in 1947 as part of what became a slew of particles discovered after World War II. Initially, such particles were found as ionizing radiation left by cosmic rays, but increasingly came to be produced in newer and more powerful particle accelerators.[60]
Outside particle physics, significant advances of the time were:
Einstein deemed that all fundamental interactions in nature can be explained in a single theory. Unified field theories were numerous attempts to "merge" several interactions. One of many formulations of such theories (as well as field theories in general) is a gauge theory, a generalization of the idea of symmetry. Eventually the Standard Model (see below) succeeded in unification of strong, weak, and electromagnetic interactions. All attempts to unify gravitation with something else failed.
When parity was broken in weak interactions by Chien-Shiung Wu in her experiment, a series of discoveries were created thereafter.[62] The interaction of these particles by scattering and decay provided a key to new fundamental quantum theories. Murray Gell-Mann and Yuval Ne'eman brought some order to these new particles by classifying them according to certain qualities, beginning with what Gell-Mann referred to as the "Eightfold Way". While its further development, the quark model, at first seemed inadequate to describe strong nuclear forces, allowing the temporary rise of competing theories such as the S-Matrix, the establishment of quantum chromodynamics in the 1970s finalized a set of fundamental and exchange particles, which allowed for the establishment of a "standard model" based on the mathematics of gauge invariance, which successfully described all forces except for gravitation, and which remains generally accepted within its domain of application.[58]
The Standard Model, based on the Yang-Mills Theory[63] groups the electroweak interaction theory and quantum chromodynamics into a structure denoted by the gauge group SU(3)SU(2)U(1). The formulation of the unification of the electromagnetic and weak interactions in the standard model is due to Abdus Salam, Steven Weinberg and, subsequently, Sheldon Glashow. Electroweak theory was later confirmed experimentally (by observation of neutral weak currents),[64][65][66][67] and distinguished by the 1979 Nobel Prize in Physics.[68]
Since the 1970s, fundamental particle physics has provided insights into early universe cosmology, particularly the Big Bang theory proposed as a consequence of Einstein's general theory of relativity. However, starting in the 1990s, astronomical observations have also provided new challenges, such as the need for new explanations of galactic stability ("dark matter") and the apparent acceleration in the expansion of the universe ("dark energy").
While accelerators have confirmed most aspects of the Standard Model by detecting expected particle interactions at various collision energies, no theory reconciling general relativity with the Standard Model has yet been found, although supersymmetry and string theory were believed by many theorists to be a promising avenue forward. The Large Hadron Collider, however, which began operating in 2008, has failed to find any evidence whatsoever that is supportive of supersymmetry and string theory.[69]
Cosmology may be said to have become a serious research question with the publication of Einstein's General Theory of Relativity in 1915 although it did not enter the scientific mainstream until the period known as the "Golden age of general relativity".
About a decade later, in the midst of what was dubbed the "Great Debate", Hubble and Slipher discovered the expansion of universe in the 1920s measuring the redshifts of Doppler spectra from galactic nebulae. Using Einstein's general relativity, Lematre and Gamow formulated what would become known as the big bang theory. A rival, called the steady state theory was devised by Hoyle, Gold, Narlikar and Bondi.
Cosmic background radiation was verified in the 1960s by Penzias and Wilson, and this discovery favoured the big bang at the expense of the steady state scenario. Later work was by Smoot et al. (1989), among other contributors, using data from the Cosmic Background explorer (CoBE) and the Wilkinson Microwave Anisotropy Probe (WMAP) satellites that refined these observations. The 1980s (the same decade of the COBE measurements) also saw the proposal of inflation theory by Alan Guth.
Recently the problems of dark matter and dark energy have risen to the top of the cosmology agenda.
On July 4, 2012, physicists working at CERN's Large Hadron Collider announced that they had discovered a new subatomic particle greatly resembling the Higgs boson, a potential key to an understanding of why elementary particles have mass and indeed to the existence of diversity and life in the universe.[70] For now, some physicists are calling it a "Higgslike" particle.[70] Joe Incandela, of the University of California, Santa Barbara, said, "It's something that may, in the end, be one of the biggest observations of any new phenomena in our field in the last 30 or 40 years, going way back to the discovery of quarks, for example."[70] Michael Turner, a cosmologist at the University of Chicago and the chairman of the physics center board, said:
"This is a big moment for particle physics and a crossroads  will this be the high water mark or will it be the first of many discoveries that point us toward solving the really big questions that we have posed?"Peter Higgs was one of six physicists, working in three independent groups, who, in 1964, invented the notion of the Higgs field ("cosmic molasses"). The others were Tom Kibble of Imperial College, London; Carl Hagen of the University of Rochester; Gerald Guralnik of Brown University; and Franois Englert and Robert Brout, both of Universit libre de Bruxelles.[70]
Although they have never been seen, Higgslike fields play an important role in theories of the universe and in string theory. Under certain conditions, according to the strange accounting of Einsteinian physics, they can become suffused with energy that exerts an antigravitational force. Such fields have been proposed as the source of an enormous burst of expansion, known as inflation, early in the universe and, possibly, as the secret of the dark energy that now seems to be speeding up the expansion of the universe.[70]
With increased accessibility to and elaboration upon advanced analytical techniques in the 19th century, physics was defined as much, if not more, by those techniques than by the search for universal principles of motion and energy, and the fundamental nature of matter. Fields such as acoustics, geophysics, astrophysics, aerodynamics, plasma physics, low-temperature physics, and solid-state physics joined optics, fluid dynamics, electromagnetism, and mechanics as areas of physical research. In the 20th century, physics also became closely allied with such fields as electrical, aerospace and materials engineering, and physicists began to work in government and industrial laboratories as much as in academic settings. Following World WarII, the population of physicists increased dramatically, and came to be centered on the United States, while, in more recent decades, physics has become a more international pursuit than at any time in its previous history.
"Using a whole body of mathematical methods (not only those inherited from the antique theory of ratios and infinitesimal techniques, but also the methods of the contemporary algebra and fine calculation techniques), Islamic scientists raised statics to a new, higher level. The classical results of Archimedes in the theory of the centre of gravity were generalized and applied to three-dimensional bodies, the theory of ponderable lever was founded and the 'science of gravity' was created and later further developed in medieval Europe. The phenomena of statics were studied by using the dynamic approach so that two trends  statics and dynamics  turned out to be inter-related within a single science, mechanics.""The combination of the dynamic approach with Archimedean hydrostatics gave birth to a direction in science which may be called medieval hydrodynamics.""Archimedean statics formed the basis for creating the fundamentals of the science on specific weight. Numerous fine experimental methods were developed for determining the specific weight, which were based, in particular, on the theory of balances and weighing. The classical works of al-Biruni and al-Khazini can by right be considered as the beginning of the application of experimental methods in medieval science.""Arabic statics was an essential link in the progress of world science. It played an important part in the prehistory of classical mechanics in medieval Europe. Without it classical mechanics proper could probably not have been created."

Electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use.
Electrical engineering is now divided into a wide range of different fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, photovoltaic cells, electronics, and optics and photonics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics/control, and electrical materials science.[a]
Electrical engineers typically hold a degree in electrical engineering or electronic engineering.  Practising engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) (formerly the IEE).
Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager.  The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.
Electricity has been a subject of scientific interest since at least the early-17th-century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term "electricity".[1] He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery.
In the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Hans Christian rsted who discovered in 1820 that an electric current produces a magnetic field that will deflect a compass needle, of William Sturgeon who, in 1825 invented the electromagnet, of Joseph Henry and Edward Davy who invented the electrical relay in 1835, of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor,[2] of Michael Faraday (the discoverer of electromagnetic induction in 1831), and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise Electricity and Magnetism.[3]
In 1782, Georges-Louis Le Sage developed and presented in Berlin probably the world's first form of electric telegraphy, using 24 different wires, one for each letter of the alphabet. This telegraph connected two rooms. It was an electrostatic telegraph that moved gold leaf through electrical conduction.
In 1795, Francisco Salva Campillo proposed an electrostatic telegraph system. Between 1803 and 1804, he worked on electrical telegraphy and in 1804, he presented his report at the Royal Academy of Natural Sciences and Arts of Barcelona. Salva's electrolyte telegraph system was very innovative though it was greatly influenced by and based upon two new discoveries made in Europe in 1800  Alessandro Volta's electric battery for generating an electric current and William Nicholson and Anthony Carlyle's electrolysis of water.[4] Electrical telegraphy may be considered the first example of electrical engineering.[5] Electrical engineering became a profession in the later 19th century.  Practitioners had created a global electric telegraph network, and the first professional electrical engineering institutions were founded in the UK and USA to support the new discipline. Francis Ronalds created an electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity.[6][7] Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort.[8] By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.
Practical applications and advances in such fields created an increasing need for standardised units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893.[9] The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.[10]
During these years, the study of electricity was largely considered to be a subfield of physics since the early electrical technology was considered electromechanical in nature. The Technische Universitt Darmstadt founded the world's first department of electrical engineering in 1882 and introduced the first degree course in electrical engineering in 1883.[11] The first electrical engineering degree program in the United States was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, [12] though it was Cornell University to produce the world's first electrical engineering graduates in 1885.[13] The first course in electrical engineering was taught in 1883 in Cornell's Sibley College of Mechanical Engineering and Mechanic Arts.[14] It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States.[15] In the same year, University College London founded the first chair of electrical engineering in Great Britain.[16] Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886.[17] Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.
During these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts  direct current (DC)  to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Kroly Zipernowsky, Ott Blthy and Miksa Dri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.[18] Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering.[19][20] The spread in the use of AC set off in the United States what has been called the war of the currents between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.[21]
During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including the possibility of invisible airborne waves (later called "radio waves"). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these "Hertzian waves" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of 2,100 miles (3,400km).[22]
Millimetre wave communication was first investigated by Jagadish Chandra Bose during 18941896, when he reached an extremely high frequency of up to 60GHz in his experiments.[23] He also introduced the use of semiconductor junctions to detect radio waves,[24] when he patented the radio crystal detector in 1901.[25][26]
In 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television.[27] John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.[28]
In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer.[29][30] In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.[31]
In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts.  In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer.[32][33] In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives.[34]
In 1948 Claude Shannon publishes "A Mathematical Theory of Communication" which mathematically describes the passage of information with uncertainty (electrical noise).
The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947.[35] They then invented the bipolar junction transistor in 1948.[36] While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis,[37] they opened the door for more compact devices.[38]
The surface passivation process, which electrically stabilized silicon surfaces via thermal oxidation, was developed by Mohamed M. Atalla at BTL in 1957. This led to the development of the monolithic integrated circuit chip.[39][40][41] The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959.[42]
The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959.[43][44][45] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[37] It revolutionized the electronics industry,[46][47] becoming the most widely used electronic device in the world.[44][48][49] The MOSFET is the basic element in most modern electronic equipment,[50][51] and has been central to the electronics revolution,[52] the microelectronics revolution,[53] and the Digital Revolution.[45][54][55] The MOSFET has thus been credited as the birth of modern electronics,[56][57] and possibly the most important invention in electronics.[58]
The MOSFET made it possible to build high-density integrated circuit chips.[44] Atalla first proposed the concept of the MOS integrated circuit (MOS IC) chip in 1960, followed by Kahng in 1961.[37][59] The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962.[60] MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965.[61] Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968.[62] Since then, the MOSFET has been the basic building block of modern electronics.[45][63][50] The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking.[64]
The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP)[65][66] and silicon integrated circuit chips in the Apollo Guidance Computer (AGC).[67]
The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s.[68][51] The first single-chip microprocessor was the Intel 4004, released in 1971.[68] It began with the "Busicom Project"[69] as Masatoshi Shima's three-chip CPU design in 1968,[70][69] before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968.[71] The Intel 4004 was then designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology,[68] along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima.[69] The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.
One of the properties of electricity is that it is very useful for energy transmission as well as for information transmission. These were also the first areas in which electrical engineering was developed. Today electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered disciplines in their own right.
Power & Energy engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices.[72] These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it.[73] Such systems are called on-grid power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called off-grid power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.
Telecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space.[74] Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation.[75] The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.
Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength.[76][77] Typically, if the power of the transmitted signal is insufficient once the signal arrives at the receiver's antenna(s), the information contained in the signal will be corrupted by noise, specifically static.
Control engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner.[78] To implement such controllers, electronics control engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles.[79] It also plays an important role in industrial automation.
Control engineers often use feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly.[80] Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.
Control engineers also work in robotics to design autonomous systems using control algorithms which interpret sensory feedback to control actuators that move robots such as autonomous vehicles, autonomous drones and others used in a variety of industries.[81]
Electronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality.[73] The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.
Prior to the Second World War, the subject was commonly known as radio engineering and basically was restricted to aspects of communications and radar, commercial radio, and early television.[73] Later, in post-war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term radio engineering gradually gave way to the name electronic engineering.
Before the invention of the integrated circuit in 1959,[82] electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large numberoften millionsof tiny electrical components, mainly transistors,[83] into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.
Microelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component.[84] The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level.
Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100nm processing having been standard since around 2002.[85]
Microelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.[86]
Signal processing deals with the analysis and manipulation of signals.[87] Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.[88]
Signal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.
DSP processor ICs are found in many types of modern electronic devices, such as digital television sets,[89] radios, Hi-Fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating positions using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.[90]
Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature.[91] The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.[92]
Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant.[93] For this reason, instrumentation engineering is often viewed as the counterpart of control.
Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant.[94] Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.[95] Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players. Computer engineers are involved in many hardware and software aspects of computing.[96]
Optics and photonics deals with the generation, transmission, amplification, modulation, detection, and analysis of electromagnetic radiation. The application of optics deals with design of optical instruments such as lenses, microscopes, telescopes, and other equipment that uses the properties of electromagnetic radiation. Other prominent applications of optics include electro-optical sensors and measurement systems, lasers, fiber optic communication systems, and optical disc systems (e.g. CD and DVD). Photonics builds heavily on optical technology, supplemented with modern developments such as optoelectronics (mostly involving semiconductors), laser systems, optical amplifiers and novel materials (e.g. metamaterials).
Mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems,[97] heating, ventilation and air-conditioning systems,[98] and various subsystems of aircraft and automobiles.
[99]
Electronic systems design is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.[100]
The term mechatronics is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy,[101] in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.[102]
Biomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners,[103] and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.
Aerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.
Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology,[104] or electrical and electronic engineering.[105][106] The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science, depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering.[107] Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.
At many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others, electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.[108]
Some electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (MEng/MSc), a Master of Engineering Management, a Doctor of Philosophy (PhD) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than a standalone postgraduate degree.[109]
In most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body.[110] After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).
The advantages of licensure vary depending upon location. For example, in the United States and Canada "only a licensed engineer may seal engineering work for public and private clients".[111] This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act.[112] In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion.[113] In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.
Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET).  The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually.[114] The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe.[115][116] Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.[117]
In Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force.[b]
From the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.[121]
Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.
Although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.[122]
A wide range of instrumentation is used by electrical engineers.  For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice.  Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument.  In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used.  In some disciplines, safety can be a particular concern with instrumentation.  For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids.[123] Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different.[124] Many disciplines of electrical engineering use tests specific to their discipline.  Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise.  Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.
For many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules.[125] Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.
The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, on board a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.[126]
Electrical engineering has an intimate relationship with the physical sciences.  For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable.[127] Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables.[128] Electrical engineers are often required on major science projects.  For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project including the power distribution, the instrumentation, and the manufacture and installation of the superconducting electromagnets.[129][130]

The following outline is provided as an overview of and topical guide to electrical engineering.
Electrical engineering  field of engineering that generally deals with the study and application of electricity, electronics and electromagnetism. The field first became an identifiable occupation in the late nineteenth century after commercialization of the electric telegraph and electrical power supply. It now covers a range of subtopics including power, electronics, control systems, signal processing and telecommunications.
Electrical engineering can be described as all of the following:
History of electrical engineering
Electromagnetism
Physical laws
Control engineering
Electronics
Power engineering
Electric vehicles
Signal processing
Telecommunication

A Bachelor of Engineering (abbreviated as B.E., B.Eng. or B.A.I. in Latin form) is a first professional undergraduate academic degree awarded to a student after three to five years of studying engineering at an accredited university. In the UK, a B.Eng. degree will be accredited by one of the Engineering Council's professional engineering institutions as suitable for registration as an incorporated engineer or chartered engineer with further study to masters level. In Canada, the degree from a Canadian university can be accredited by the Canadian Engineering Accreditation Board (CEAB). Alternatively, it might be accredited directly by another professional engineering institution, such as the US-based Institute of Electrical and Electronics Engineers (IEEE). The B.Eng. contributes to the route to chartered engineer (UK), registered engineer or licensed professional engineer and has been approved by representatives of the profession.
A B.E. has a greater emphasis on math and science, to allow the engineers to move from one discipline to another. Multi-discipline is required in certain fields, like Marine Engineering. The marine engineer is required to know mechanical, chemical and electric engineering. If an engineer is strictly staying in a single discipline, he/she would probably be better served with a B.Sc. A typical B.Sc. is 128 credits. SUNY Maritime B.E. is 172 credits.
Most universities in the United States and Europe award the Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Engineering (B.Eng.), Bachelor of Engineering Science (B.Eng.Sc.), Bachelor of Science in Engineering (B.S.E.) or Bachelor of Applied Science (B.A.Sc.) degree to undergraduate students of engineering study. For example, Canada is the only country that awards the B.A.Sc. degree for graduating engineers. Other institutions award engineering degrees specific to the area of study, such as B.S.E.E. (Bachelor of Science in Electrical Engineering) [1] and B.S.M.E. (Bachelor of Science in Mechanical Engineering).[2]
In French-speaking Canada, mainly Qubec, the Bachelor of Engineering is referred to as B.Ing (Baccalaurat en ingnierie).[3]
A less common and possibly the oldest variety of the degree in the English-speaking world, is Baccalaureus in Arte Ingeniaria (B.A.I.), a Latin name meaning Bachelor in the Art of Engineering.[4] Here Baccalaureus in Arte Ingeniaria implies an excellence in carrying out the 'art' or 'function' of an engineer. The degree is awarded by the University of Dublin (its Trinity College Dublin has had a School of Engineering since 1841) and also by the constituent universities of the National University of Ireland (N.U.I.)[citation needed], but in everyday speech it is more commonly referred to as Bachelor of Engineering and the N.U.I. graduates also use the post-nominals translated into English, B.E., even though the actual degree and its parchment is in Latin[citation needed].
Some South African Universities refer to their engineering degrees as B.Ing. (Baccalaureus Ingenieurswese, in Afrikaans).
A Bachelor of Engineering degree will usually be undertaken in one field of engineering, which is sometimes noted in the degree postnominals, as in BE (Aero) or BEng (Elec). Common fields for the Bachelor of Engineering degree include the following fields:
In Australia, the Bachelor of Engineering (BE or BEng - depending on institution) is a four-year undergraduate degree course and a professional qualification. It is also available as a six-year sandwich course (where students are required to undertake a period of professional placement as a part of the degree) or an eight-year part-time course through some universities. The Institution of Engineers, Australia (Engineers Australia) accredits degree courses and graduates of accredited courses are eligible for membership of the Institution. Bachelor of Engineering graduates may commence work as a graduate professional engineer upon graduation, although some may elect to undertake further study such as a Master's or Doctoral degree. 
The title of engineer is not protected in Australia, therefore anyone can claim to be an engineer and practice without the necessary competencies, understanding of standards or in compliance with a code of ethics.[11] The industry has attempted to overcome the lack of title protection through chartership (CPEng), national registration (NER) and various state registration (RPEQ) programs which are usually obtained after a few years of professional practice.
In Bangladesh, the Bachelor of Science in Engineering (B.Sc. Engineering) is a four-year undergraduate university degree. Universities offer engineering degree in many branches like Computer Science and Engineering, Civil Engineering, Mechanical Engineering, Mechatronics Engineering, Electrical and Electronics Engineering, Industrial and Production Engineering, Information and Communication Engineering, Material Science and Engineering, Nuclear Engineering, Chemical Engineering, Textile Engineering, Aeronautical Engineering, Electronics and Communication Engineering, Biomedical Engineering, Food engineering, Leather Engineering, Agricultural Engineering etc. The Ministry of Education and University Grants Commission are responsible bodies for recognition and accreditation of educational institutions.
In Canada, degrees awarded for undergraduate engineering studies include: the Bachelor of Engineering (B.Eng. or B.E., depending on the institution); the Baccalaurat en gnie (B.Ing., the French equivalent of a B.Eng.; sometimes referred to as a Baccalaurat en ingnierie); the Bachelor of Applied Science (B.A.Sc.); and the Bachelor of Science in Engineering (B.Sc.Eng.).
The Canadian Engineering Accreditation Board (CEAB), a division of the Engineers Canada, sets out and maintains the standards of accreditation among Canadian undergraduate engineering programs. Graduates of those programs are deemed by the profession to have the required academic qualifications to be licensed as professional engineers in Canada.[12] This practice is intended to maintain standards of education and allow mobility of engineers in different provinces of Canada.[13]
A CEAB-accredited degree is the minimum academic requirement for registration as a professional engineer anywhere in the country and the standard against which all other engineering academic qualifications are measured.[14] Graduation from an accredited program, which normally involves four years of study, is a required first step to becoming a professional engineer. Regulation and accreditation are accomplished through a self-governing body (the name of which varies from province to province), which is given the power by statute to register and discipline engineers, as well as regulate the field of engineering in the individual provinces.
Graduates of non-CEAB-accredited programs must demonstrate that their education is at least equivalent to that of a graduate of a CEAB-accredited program.[14]
In Finland, Universities of applied sciences (ammattikorkeakoulu) grant professional bachelor's degrees (insinri (amk)). The degree does not traditionally prepare for further study, but due to the Bologna process, a completely new degree of ylempi insinri (yamk) has been introduced for engineers who wish to continue studying after some work experience. Before 2005, academic universities (see Education in Finland) did not make an administrative distinction between studies on the Bachelor's and Master's level and the Master's level diplomi-insinri was the first degree to be received. Due to the Bologna process an intermediate "Bachelor of Science in Engineering" (tekniikan kandidaatti) has been introduced.
In German, the traditional engineer's degree is called Diplom-Ingenieur (Dipl.-Ing., in Austria DI is also used). This supersedes "Ing. (Grad)", the old form in Germany. This degree is generally equivalent to a Master's degree, which is not to be confused with the old Magister degree. Most programs that used to lead to a Dipl.-Ing. degree lead to master's degrees today, as the Diplom-Ingenieur as an academic title is phased out because of the Bologna Process. However, some universities continue to hand out so-called equivalence certificates that certify the equivalence of a Dipl.-Ing. with the newly introduced M.Sc. degrees. German technical universities award a Bachelor of Science (B.Sc.) in engineering while German Universities of Applied Sciences offer the B.Eng. degree.
In India, the Bachelor of Engineering (BE) is a professional undergraduate degree awarded after completion of four years of engineering study. Many Indian universities offer degree in engineering disciplines under the name of Bachelor of Technology (B.Tech) instead of BE. Institutes of National Importance like Indian Institute of Technology (IIT) and National Institute of Technology (NIT) also use B.Tech nomenclature for engineering degrees instead of using BE but whichever name is used, degree course follows the standard curriculum as laid down by competent authorities like University Grants Commission of India (UGC), All India Council for Technical Education (AICTE) and National Board of Accreditation (NBA). There is no difference in program objectives and learning outcomes of B.Tech. and BE except nomenclature. Generally the first year (first two semesters) is common to all branches and has the same subjects of study. Courses divert after first year. common branches are Civil Engineering, Mechanical Engineering, Electrical and Electronics Engineering, Electronics and Communications Engineering, Instrumentation and Control Engineering, Computer Science and Engineering, Information Technology, Petroleum Engineering, Chemical Engineering, Biotechnology, Metallurgical Engineering, aeronautical engineering, Aerospace Engineering, Production Engineering, Biochemical Engineering, Biomedical Engineering, Agricultural Engineering etc.
In Nepal, the Bachelor of Engineering (B.E.) is a four-year undergraduate course. Institute of Engineering under the umbrella of Tribhuvan University offers B.E. degree in several disciplines such as Electronics and Communication Engineering, Electrical Engineering, Computer Engineering, Civil Engineering, Chemical Engineering, Mechanical Engineering, Automobile Engineering, Aerospace Engineering, Geomatics Engineering, Industrial Engineering, Agricultural Engineering etc. Likewise other universities in Nepal such as Kathmandu University, Pokhara University and Purbanchal University, Mid Western University offers B.E. degree in most of aforementioned engineering disciplines. The normal duration for the completion of the course is 4 years. However, the maximum time to complete the course is 8 years from the time of registration or 4 years after the normal duration. Graduates of B.E. degrees can apply for the Engineer title (Er.) or certification as registered engineer to a governing body called Nepal Engineering Council (NEC). After the scrutiny examination of the application/applicants, Nepal Engineering Council provides the Certificate of Registration as a General Engineer which is a must for practicing engineering profession in Nepal. For example: If a student called XYZ graduate with B.E. degree in Electronics and Communication Engineering from one of the accredited universities, he/she is registered as a General Electronics and Communication Engineer with Er. title written in front of name as Er. XYZ.
In Netherlands, the Bachelor of Engineering was also introduced as part of implementation of the Bologna Process, the same as in Germany. The degree is only offered by Dutch Hogeschool-institutions and is equivalent to the Dutch engineer's degree "ingenieur" (ing.). A Dutch BEng involves a study of four years and is only awarded in the field of aeronautical engineering, mechanical engineering, software engineering, industrial engineering or electrical engineering. Completion of a Dutch engineer's study in the field biochemical engineering, biomedical engineering, chemical engineering, environmental engineering, material engineering is however awarded with a Bachelor of Applied Science degree. Dutch technical universities award a Bachelor of Science in Engineering (BScEng) instead of the BEng degree.
In Pakistan, Bachelor of Engineering (BE) or Bachelor of science in Engineering (BS/BSc Engineering) is a four years undergraduate professional university degree. Pakistan Engineering Council (PEC) is the responsible government body for accreditation of undergraduate engineering degrees, registration of engineers and regulation of engineering profession in Pakistan. PEC is a full signatory of Washington Accord and International Professional Engineer (IntPE) Agreement (IPEA).

The Institute of Electrical and Electronics Engineers (IEEE) is a professional association for electronic engineering and electrical engineering (and associated disciplines) with its corporate office in New York City[4] and its operations center in Piscataway, New Jersey. It was formed in 1963 from the amalgamation of the American Institute of Electrical Engineers and the Institute of Radio Engineers.[5]
Due to its expansion of scope into so many related fields, it is simply referred to by the letters I-E-E-E (pronounced I-triple-E), except on legal business documents. As of 2018[update], it is the world's largest association of technical professionals[6] with more than 423,000 members in over 160 countries around the world.[7] Its objectives are the educational and technical advancement of electrical and electronic engineering, telecommunications, computer engineering and similar disciplines.[4][8]
The IEEE traces its founding to 1884 and the American Institute of Electrical Engineers.  In 1912, the rival Institute of Radio Engineers was formed. Although the AIEE was initially larger, the IRE attracted more students and was larger by the mid-1950s.[9] The AIEE and IRE merged in 1963.
The IEEE headquarters is in New York City at 3 Park Ave, but most business is done at the IEEE Operations Center[10] in Piscataway, NJ, first occupied in 1975.
The AIEE and the IRE merged to create the IEEE on January 1, 1963. At that time, the combined group had 150,000 members, 93%  in the United States. By 1984 there were 250,000 members, 20% of whom were outside the U.S.[citation needed]
The Australian Section of the IEEE existed between 1972 and 1985. After this date, it split into state- and territory-based sections.[11]
As of 2021[update], IEEE has over 400,000 members in 160 countries, with the U.S. based membership no longer constituting a majority.[12]
In May 2019, IEEE restricted Huawei employees from peer reviewing papers or handling papers as editors due to the "severe legal implications" of U.S. government sanctions against Huawei.[13] As members of its standard-setting body, Huawei employees could continue to exercise their voting rights, attend standards development meetings, submit proposals and comment in public discussions on new standards.[14][15] The ban sparked outrage among Chinese scientists on social media. Some professors in China decided to cancel their memberships.[16][17]
On June 3, 2019, IEEE lifted restrictions on Huawei's editorial and peer review activities after receiving clearance from the United States government.[18][19][20]
IEEE produces over 30% of the world's literature in the electrical and electronics engineering and computer science fields, publishing approximately 200 peer-reviewed journals[21] and magazines. IEEE publishes more than 1,200 conference proceedings every year.
The published content in these journals as well as the content from several hundred annual conferences sponsored by the IEEE are available in the IEEE Electronic Library (IEL)[22] available through IEEE Xplore[23] platform, for subscription-based access and individual publication purchases.[24]
In addition to journals and conference proceedings, the IEEE also publishes tutorials and standards that are produced by its standardization committees. The organization also has its own IEEE format paper. In writing IEEE papers, it is not just a matter of mentioning the author's name or the page number or the date an article was published. The most important aspect is referring to the source by indicating its number in a square bracket and ensure it corresponds with the full citation as mentioned in the reference list.[25]
The IEEE provides learning opportunities within the engineering sciences, research, and technology.
IEEE offers educational opportunities such as IEEE e Learning Library,[26]  the Education Partners Program,[27] Standards in Education[28] and Continuing Education Units (CEUs).[29]
IEEE eLearning Library is a collection of online educational courses designed for self-paced learning. Education Partners, exclusive for IEEE members, offers on-line degree programs, certifications and courses at a 10% discount.  The Standards in Education website explains what standards are and the importance of developing and using them. The site includes tutorial modules and case illustrations to introduce the history of standards, the basic terminology, their applications and impact on products, as well as news related to standards, book reviews and links to other sites that contain information on standards. Currently, forty states in the United States require Professional Development Hours (PDH) to maintain a Professional Engineering license,[30][31][32] encouraging engineers to seek Continuing Education Units (CEUs) for their participation in continuing education programs. CEUs readily translate into Professional Development Hours (PDHs), with 1 CEU being equivalent to 10 PDHs. Countries outside the United States, such as South Africa, similarly require continuing professional development (CPD) credits, and it is anticipated that IEEE Expert Now courses will feature in the CPD listing for South Africa.
IEEE also sponsors a website  designed to help young people better understand engineering, and how an engineering career can be made part of their future. Students of age 818, parents, and teachers can explore the site to prepare for an engineering career, ask experts engineering-related questions, play interactive games, explore curriculum links, and review lesson plans. This website also allows students to search for accredited engineering degree programs in Canada and the United States; visitors are able to search by state/province/territory, country, degree field, tuition ranges, room and board ranges, size of student body, and location (rural, suburban, or urban).
Through the Student Activities Committee, IEEE facilitates partnership between student activities and all other IEEE entities.[33]
Most IEEE members are electrical and electronics engineers, but the organization's wide scope of interests has attracted people in other disciplines as well (e.g., computer science, software engineering, mechanical engineering, civil engineering, biology, physics, and mathematics).
An individual can join the IEEE as a student member, professional member, or associate member. In order to qualify for membership, the individual must fulfill certain academic or professional criteria and abide to the code of ethics and bylaws of the organization. There are several categories and levels of IEEE membership and affiliation:
Various technical areas are addressed by IEEE's 39 societies, each one focused on a certain knowledge area. They provide specialized publications, conferences, business networking and sometimes other services.[36]
The IEEE Technology Engineering and Management Society (TEMS) has its origins in the IRE Professional Group on Engineering Management had been established under the Institute of Radio Engineers (IRE) in 1951.  This group had become the Engineering Management Society (EMS) of the IEEE in 1955.  In 2007, this group became the IEEE Technology Management Council (TMC); and in January 2015 is finally acquired the current name.[37][38]  TEMS was formerly one of seven councils of the IEEE, established 1 January 2015.[39]
The society's field of interest encompasses the management sciences and practices required for defining, implementing, and managing engineering and technology.[40]
The society publishes two peer-reviewed journals: the IEEE Transactions on Engineering Management[41][42] and the IEEE Engineering Management Review.[43] The society also publishes a newsletter, the IEEE Leader, which covers issues ranging from education to entrepreneurship.[44]
The society is governed by a board of governors consisting of officers and members-at-large,[45] most of whom are elected by the membership. In addition, the society has Technical Committees dealing with the major management issues.[46]
The society sponsors and co-sponsors a number of internationally held conferences and events on subjects relevant to its field of interest[47][48] the flagships being TEMSCON[49][50] and ICTE[51][52])
IEEE technical councils are collaborations of several IEEE societies on a broader knowledge area. There are currently seven technical councils:[53]
To allow a quick response to new innovations, IEEE can also organize technical committees on top of their technical societies and technical councils. There are currently over twenty such technical committees.[55]
IEEE develops communities around particular technology challenges or cutting-edge subject areas.[56] Their areas of coverage may be general in nature, which include multi-disciplinary topics or emerging concepts.
IEEE Geographically consists of Regions and Sections, which include local Subunits (Chapters and Affinity Groups).[57]
The IEEE Foundation is a charitable foundation established in 1973[58] to support and promote technology education, innovation and excellence.[59] It is incorporated separately from the IEEE, although it has a close relationship to it. Members of the Board of Directors of the foundation are required to be active members of IEEE, and one third of them must be current or former members of the IEEE Board of Directors.
Initially, the role of the IEEE Foundation was to accept and administer donations for the IEEE Awards program, but donations increased beyond what was necessary for this purpose, and the scope was broadened. In addition to soliciting and administering unrestricted funds, the foundation also administers donor-designated funds supporting particular educational, humanitarian, historical preservation, and peer recognition programs of the IEEE.[59] As of the end of 2014, the foundation's total assets were nearly $45 million, split equally between unrestricted and donor-designated funds.[60]


Computer engineering (CoE or CpE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software.[1] Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work but also how they integrate into the larger picture.[2]
Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
In many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.[3][4][5][6]
Computer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world's first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took 5 years to complete.[7]
While the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.[8]
The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947,[9] the silicon surface passivation process (via thermal oxidation) by Mohamed Atalla at Bell Labs in 1957,[10][11][12] the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959,[13] the metal-oxide-semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959,[14][15][16] and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.[17]
The first computer engineering degree program in the United States was established in 1971 at Case Western Reserve University in Cleveland, Ohio.[18] As of 2015[update], there were 250 ABET-accredited computer engineering programs in the U.S.[19] In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor's degree generally called computer engineering.  Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.
Computer engineering is referred to as computer science and engineering at some universities. Most entry-level computer engineering jobs require at least a bachelor's degree in computer engineering (or computer science and engineering). Typically one must learn an array of mathematics such as calculus, algebra and trigonometry and some computer science classes.[citation needed] Sometimes a degree in electronic engineering is accepted, due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a strong background in computer programming is necessary. According to BLS, "a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum".[20] Some large firms or specialized jobs require a master's degree.
It is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers. This can be helpful, especially when it comes to learning new skills or improving existing ones. For example, as the relative cost of fixing a bug increases the further along it is in the software development cycle, there can be greater cost savings attributed to developing and testing for quality code as soon as possible in the process, and particularly before release.[21]
A person with a profession in computer engineering is called a computer engineer.
Institution: Educational Institution (such as College, University)
Institution: Private institution (such as computer engineering organizations and private companies)
Institution: Public Institution (such as Country's Regulatory Board)
Institution: Public or Private Institution
There are two major focuses in computer engineering: hardware and software.
According to the BLS, Job Outlook employment for computer hardware engineers, the expected ten-year growth from 2019 to 2029 for computer hardware engineering was an estimated 2% and a total of 71,100 jobs. ("Slower than average" in their own words when compared to other occupations)".[22] This is a decrease from the 2014 to 2024 BLS computer hardware engineering estimate of 3% and  a total of 77,700 jobs. "[22] and is down from 7% for the 2012 to 2022 BLS estimate[22] and is further down from 9% in the BLS 2010 to 2020 estimate[22]." Today, computer hardware is somehow equal[clarification needed] to electronic and computer engineering (ECE) and has been divided into many subcategories; the most significant[citation needed] is embedded system design.[20]
According to the U.S. Bureau of Labor Statistics (BLS), "computer applications software engineers and computer systems software engineers are projected to be among the faster than average growing occupations" The expected ten-year growth as of 2014 for computer software engineering was an estimated seventeen percent and there was a total of 1,114,000 jobs that same year.[23] This is down from the 2012 to 2022 BLS estimate of 22% for software developers.[24][23] And, further down from the 30% 2010 to 2020 BLS estimate.[25] In addition, growing concerns over cybersecurity add up to put computer software engineering high above the average rate of increase for all fields. However, some of the work will be outsourced in foreign countries.[26] Due to this, job growth will not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead go to computer software engineers in countries such as India.[27] In addition, the BLS Job Outlook for Computer Programmers, 201424 has an 8% (a decline, in their words)[27] and a Job Outlook, 2019-29 	-9% (Decline)[28] for those who program computers (i.e. embedded systems) who are not computer application developers.[29][30] Furthermore, women in software fields has been declining over the years even faster than other engineering fields.[31]
Computer engineering is generally practiced within larger product development firms, and such practice may not be subject to licensing.[32][33]  However, independent consultants who advertise computer engineering, just like any form of engineering, may be subject to state laws which restrict professional engineer practice to only those who have received the appropriate License.[34][35] National Council of Examiners for Engineering and Surveying (NCEES) first offered a Principles and Practice of Engineering Examination for computer engineering[36] in 2003. 
There are many specialty areas in the field of computer engineering.
Processor design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture, which might be described in e.g. VHDL or Verilog. CPU design is divided into design of the following components: datapaths (such as ALUs and pipelines), control unit: logic which controls the datapaths, memory components such as register files, caches, clock circuitry such as clock drivers, PLLs, clock distribution networks, pad transceiver circuitry, logic gate cell library which is used to implement the logic.
Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.[37]
Those focusing on communications and wireless networks, work advancements in telecommunications systems and networks (especially wireless networks), modulation and error-control coding, and information theory. High-speed network design, interference suppression and modulation, design, and analysis of fault-tolerant system, and storage and transmission schemes are all a part of this specialty.[37]
This specialty focuses on compilers and operating systems design and development. Engineers in this field develop new operating system architecture, program analysis techniques, and new techniques to assure quality. Examples of work in this field include post-link-time code transformation algorithm development and new operating system development.[37]
Computational science and engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, "computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more".[37]
In this specialty, engineers build integrated environments for computing, communications, and information access. Examples include shared-channel wireless networks, adaptive resource management in various systems, and improving the quality of service in mobile and ATM environments. Some other examples include work on wireless network systems and fast Ethernet cluster wired systems.[37]
Engineers working in computer systems work on research projects that allow for reliable, secure, and high-performance computer systems. Projects such as designing processors for multi-threading and parallel processing are included in this field. Other examples of work in this field include the development of new theories, algorithms, and other tools that add performance to computer systems.[37]
Computer architecture includes CPU design, cache hierarchy layout, memory organization and load balancing.
In this specialty, computer engineers focus on developing visual sensing technology to sense an environment, representation of an environment, and manipulation of the environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.[37]
Individuals working in this area design technology for enhancing the speed, reliability, and performance of systems. Embedded systems are found in many devices from a small FM radio to the space shuttle. According to the Sloan Cornerstone Career Center, ongoing developments in embedded systems include "automated vehicles and equipment to conduct search and rescue, automated transportation systems, and human-robot coordination to repair equipment in space."[37] As of 2018[update], computer embedded computer engineering specializations include system-on-chip design, architecture of edge computing and the Internet of things.
This specialty of computer engineering requires adequate knowledge of electronics and electrical systems. Engineers working in this area work on enhancing the speed, reliability, and energy efficiency of next-generation very-large-scale integrated (VLSI) circuits and microsystems. An example of this specialty is work done on reducing the power consumption of VLSI algorithms and architecture.[37]
Computer engineers in this area develop improvements in human-computer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.[37]

Electronic engineering (also called electronics and communications engineering) is an electrical engineering discipline which utilizes nonlinear and active electrical components (such as semiconductor devices, especially transistors and diodes) to design electronic circuits, devices, integrated circuits and their systems. The discipline typically also designs passive electrical components, usually based on printed circuit boards. 
Electronics is a subfield within the wider electrical engineering academic subject but denotes a broad engineering field that covers subfields such as analog electronics, digital electronics, consumer electronics, embedded systems and power electronics. Electronics engineering deals with implementation of applications, principles and algorithms developed within many related fields, for example solid-state physics, radio engineering, telecommunications, control systems, signal processing, systems engineering, computer engineering, instrumentation engineering, electric power control, robotics, and many others.
The Institute of Electrical and Electronics Engineers (IEEE) is one of the most important and influential organizations for electronics engineers based in the US. On an international level, the International Electrotechnical Commission (IEC) prepares standards for electronic engineering, developed through consensus and thanks to the work of 20,000 experts from 172 countries worldwide.
Electronics is a subfield within the wider electrical engineering academic subject. An academic degree with a major in electronics engineering can be acquired from some universities, while other universities use electrical engineering as the subject. The term electrical engineer is still used in the academic world to include electronic engineers.[1] However, some believe the term electrical engineer should be reserved for those having specialized in power and heavy current or high voltage engineering, while others consider that power is just one subset of electrical engineering similar to electric power distribution engineering. The term power engineering is used as a descriptor in that industry.  Again, in recent years there has been a growth of new separate-entry degree courses such as systems engineering and communication systems engineering, often followed by academic departments of similar name, which are typically not considered as subfields of electronics engineering but of electrical engineering.[2][3]
Electronic engineering as a profession sprang from technological improvements in the telegraph industry in the late 19th century and the radio and the telephone industries in the early 20th century. People were attracted to radio by the technical fascination it inspired, first in receiving and then in transmitting. Many who went into broadcasting in the 1920s were only 'amateurs' in the period before World War I.[4]
To a large extent, the modern discipline of electronic engineering was born out of telephone, radio, and television equipment development and the large amount of electronic systems development during World War II of radar, sonar, communication systems, and advanced munitions and weapon systems. In the interwar years, the subject was known as radio engineering and it was only in the late 1950s that the term electronic engineering started to emerge.[5]
The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947.[6] The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was later invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[7][8][9] The MOSFET was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[10]  The MOSFET revolutionized the electronics industry,[11][12] becoming the most widely used electronic device in the world.[8][13][14] The MOSFET is the basic element in most modern electronic equipment.[15][16]
In the field of electronic engineering, engineers design and test circuits that use the electromagnetic properties of electrical components such as resistors, capacitors, inductors, diodes and transistors to achieve a particular functionality. The tuner circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit.
In designing an integrated circuit, electronics engineers first construct circuit schematics that specify the electrical components and describe the interconnections between them. When completed, VLSI engineers convert the schematics into actual layouts, which map the layers of various conductor and semiconductor materials needed to construct the circuit. The conversion from schematics to layouts can be done by software (see electronic design automation) but very often requires human fine-tuning to decrease space and power consumption. Once the layout is complete, it can be sent to a fabrication plant for manufacturing.
For systems of intermediate complexity, engineers may use VHDL modeling for programmable logic devices and FPGAs.
Integrated circuits, FPGAs and other electrical components can then be assembled on printed circuit boards to form more complicated circuits. Today, printed circuit boards are found in most electronic devices including televisions, computers and audio players.[17]
Electronic engineering has many subfields. This section describes some of the most popular subfields in electronic engineering; although there are engineers who focus exclusively on one subfield, there are also many who focus on a combination of subfields.
Signal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information.
For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error checking and error detection of digital signals.
Telecommunications engineering deals with the transmission of information across a channel such as a co-axial cable, optical fiber or free space.
Transmissions across free space require information to be encoded in a carrier wave in order to shift the information to a carrier frequency suitable for transmission, this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.
Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. If the signal strength of a transmitter is insufficient the signal's information will be corrupted by noise.
Aviation-Electronic Engineering and Aviation-Telecommunications Engineering, They research and work on electronics and aerospace topics. Aviation-telecommunication engineers include a group of specialists who have a lot of information about the flight (such as meteorological data, some specific information, etc.) through the available platforms (such as AFTN, etc.) at the disposal of the aircraft itself or parts Others are stationed at airports. Specialists in this field mainly need knowledge of computer, networking, IT and physics.Their works are in organizations and companies in the air and space. Before, during and after the flight, the aircraft needs equipment and platforms that meet many of its needs such as navigation information, communication and monitoring systems (CNS). Of course, such equipment requires installation, commissioning, maintenance and repair, which is a serious task for an aviation electronics specialist at the airports. These courses are offered at different universities like Civil Aviation Technology College[18].[19]
Electromagnetics is an in-depth study about the signals that are transmitted in a channel (Wired or Wireless). This includes Basics of Electromagnetic waves, Transmission Lines and Waveguides, Antennas, its types and applications with Radio-Frequency (RF) and Microwaves. Its applications are seen widely in other sub-fields like Telecommunication, Control and Instrumentation Engineering.
Control engineering has a wide range of applications from the flight and propulsion systems of commercial airplanes to the cruise control present in many modern cars. It also plays an important role in industrial automation.
Control engineers often utilize feedback when designing control systems. For example, in a car with cruise control, the vehicle's speed is continuously monitored and fed back to the system which adjusts the engine's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.
Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow and temperature. These devices are known as instrumentation.
The design of such instrumentation requires a good understanding of physics that often extends beyond electromagnetic theory. For example, radar guns use the Doppler effect to measure the speed of oncoming vehicles. Similarly, thermocouples use the PeltierSeebeck effect to measure the temperature difference between two points.
Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control engineering.[20]
Computer engineering deals with the design of computers and computer systems. This may involve the design of new computer hardware, the design of PDAs or the use of computers to control an industrial plant. Development of embedded systemssystems made for specific tasks (e.g., mobile phones)is also included in this field.  This field includes the micro controller and its applications.
Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.
VLSI design engineering VLSI stands for very large scale integration. It deals with fabrication of ICs and various electronic components.
Electronics engineers typically possess an academic degree with a major in electronic engineering. The length of study for such a degree is usually three or four years and the completed degree may be designated as a Bachelor of Engineering, Bachelor of Science, Bachelor of Applied Science, or Bachelor of Technology depending upon the university.  Many UK universities also offer Master of Engineering (MEng) degrees at the graduate level.
Some electronics engineers also choose to pursue a postgraduate degree such as a Master of Science, Doctor of Philosophy in Engineering, or an Engineering Doctorate. The master's degree is being introduced in some European and American Universities as a first degree and the differentiation of an engineer with graduate and postgraduate studies is often difficult. In these cases, experience is taken into account. The master's degree may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy consists of a significant research component and is often viewed as the entry point to academia.
In most countries, a bachelor's degree in engineering represents the first step towards certification and the degree program itself is certified by a professional body. Certification allows engineers to legally sign off on plans for projects affecting public safety.[21] After completing a certified degree program, the engineer must satisfy a range of requirements, including work experience requirements, before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada, and South Africa), Chartered Engineer or Incorporated Engineer (in the United Kingdom, Ireland, India, and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).
A degree in electronics generally includes units covering physics, chemistry, mathematics, project management and specific topics in electrical engineering. Initially, such topics cover most, if not all, of the subfields of electronic engineering. Students then choose to specialize in one or more subfields towards the end of the degree.
Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design and simulation software programs when designing electronic systems. Although most electronic engineers will understand basic circuit theory, the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI but are largely irrelevant to engineers working with embedded systems.
Apart from electromagnetics and network theory, other items in the syllabus are particular to electronics engineering course. Electrical engineering courses have other specialisms such as machines, power generation and distribution. This list does not include the extensive engineering mathematics curriculum that is a prerequisite to a degree.[22][23]
Elements of vector calculus: divergence and curl; Gauss' and Stokes' theorems, Maxwell's equations: differential and integral forms. Wave equation, Poynting vector. Plane waves: propagation through various media; reflection and refraction; phase and group velocity; skin depth. Transmission lines: characteristic impedance; impedance transformation; Smith chart; impedance matching; pulse excitation. Waveguides: modes in rectangular waveguides; boundary conditions; cut-off frequencies; dispersion relations. Antennas: Dipole antennas; antenna arrays; radiation pattern; reciprocity theorem, antenna gain.[24][25]
Network graphs: matrices associated with graphs; incidence, fundamental cut set, and fundamental circuit matrices. Solution methods: nodal and mesh analysis. Network theorems: superposition, Thevenin and Norton's maximum power transfer, Wye-Delta transformation.[26] Steady state sinusoidal analysis using phasors. Linear constant coefficient differential equations; time domain analysis of simple RLC circuits, Solution of network equations using Laplace transform: frequency domain analysis of RLC circuits. 2-port network parameters: driving point and transfer functions. State equations for networks.[27]
Electronic devices: Energy bands in silicon, intrinsic and extrinsic silicon. Carrier transport in silicon: diffusion current, drift current, mobility, resistivity. Generation and recombination of carriers. p-n junction diode, Zener diode, tunnel diode, BJT, JFET, MOS capacitor, MOSFET, LED, p-i-n and avalanche photo diode, LASERs. Device technology: integrated circuit fabrication process, oxidation, diffusion, ion implantation, photolithography, n-tub, p-tub and twin-tub CMOS process.[28][29]
Analog circuits: Equivalent circuits (large and small-signal) of diodes, BJT, JFETs, and MOSFETs. Simple diode circuits, clipping, clamping, rectifier. Biasing and bias stability of transistor and FET amplifiers. Amplifiers: single-and multi-stage, differential, operational, feedback and power. Analysis of amplifiers; frequency response of amplifiers. Simple op-amp circuits. Filters. Sinusoidal oscillators; criterion for oscillation; single-transistor and op-amp configurations. Function generators and wave-shaping circuits, Power supplies.[30]
Digital circuits: Boolean functions (NOT, AND, OR, XOR,...).  Logic gates digital IC families (DTL, TTL, ECL, MOS, CMOS). Combinational circuits: arithmetic circuits, code converters, multiplexers and decoders. Sequential circuits: latches and flip-flops, counters and shift-registers. Sample and hold circuits, ADCs, DACs. Semiconductor memories. Microprocessor 8086: architecture, programming, memory and I/O interfacing.[31][32]
Definitions and properties of Laplace transform, continuous-time and discrete-time Fourier series, continuous-time and discrete-time Fourier Transform, z-transform. Sampling theorems. Linear Time-Invariant (LTI) Systems: definitions and properties; causality, stability, impulse response, convolution, poles and zeros frequency response, group delay, phase delay. Signal transmission through LTI systems. Random signals and noise: probability, random variables, probability density function, autocorrelation, power spectral density, function analogy between vectors & functions.[33][34]
Basic control system components; block diagrammatic description, reduction of block diagrams Mason's rule. Open loop and closed loop (negative unity feedback) systems and stability analysis of these systems. Signal flow graphs and their use in determining transfer functions of systems; transient and steady-state analysis of LTI control systems and frequency response. Analysis of steady-state disturbance rejection and noise sensitivity.
Tools and techniques for LTI control system analysis and design: root loci, RouthHurwitz stability criterion, Bode and Nyquist plots. Control system compensators: elements of lead and lag compensation, elements of proportionalintegralderivative (PID) control. Discretization of continuous-time systems using zero-order hold and ADCs for digital controller implementation. Limitations of digital controllers: aliasing. State variable representation and solution of state equation of LTI control systems. Linearization of Nonlinear dynamical systems with state-space realizations in both frequency and time domains. Fundamental concepts of controllability and observability for MIMO LTI systems. State space realizations: observable and controllable canonical form. Ackermann's formula for state-feedback pole placement. Design of full order and reduced order estimators.[35][36]
Analog communication systems: amplitude and angle modulation and demodulation systems, spectral analysis of these operations, superheterodyne noise conditions.
Digital communication systems: pulse-code modulation (PCM), differential pulse-code modulation (DPCM), delta modulation (DM), digital modulation  amplitude, phase- and frequency-shift keying schemes (ASK, PSK, FSK), matched-filter receivers, bandwidth consideration and probability of error calculations for these schemes, GSM, TDMA.[37][38]
Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Electrical Engineers (IEE) (now renamed the Institution of Engineering and Technology or IET). Members of the Institution of Engineering and Technology (MIET) are recognized professionally in Europe, as Electrical and computer (technology) engineers. The IEEE claims to produce 30 percent of the world's literature in electrical/electronic engineering, has over 430,000 members, and holds more than 450 IEEE sponsored or cosponsored conferences worldwide each year. SMIEEE is a recognised professional designation in the United States.
For most engineers not involved at the cutting edge of system design and development, technical work accounts for only a fraction of the work they do. A lot of time is also spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason, project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.
The workplaces of electronics engineers are just as varied as the types of work they do. Electronics engineers may be found in the pristine laboratory environment of a fabrication plant, the offices of a consulting firm or in a research laboratory. During their working life, electronics engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers and other engineers.
Obsolescence of technical skills is a serious concern for electronics engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. And these are mostly used in the field of consumer electronics products.[39]



Computer Science & Engineering (CSE) is an academic program at many universities which comprises scientific and engineering aspects of computing. CSE is also a term often used in Europe to translate the name of engineering informatics academic programs. It is offered in both Undergraduate as well Postgraduate with specializations.
Academic programs vary between colleges. Undergraduate Courses usually include programming, algorithms and data structures, computer architecture, operating systems, computer networks, parallel computing, embedded systems, algorithms design, circuit analysis and electronics, digital logic and processor design, computer graphics, scientific computing, software engineering, database systems, digital signal processing, virtualization, computer simulations and games programming. CSE programs also include core subjects of theoretical computer science such as theory of computation, numerical methods, machine learning, programming theory and paradigms.[1] Modern academic programs also cover emerging computing fields like image processing, data science, robotics, bio-inspired computing, computational biology, autonomic computing and artificial intelligence.[2] Most of the above CSE areas require initial mathematical knowledge, hence the first year of study is dominated by mathematical courses, primarily discrete mathematics, mathematical analysis, linear algebra, Probability, and statistics, as well as the basics of Electrical and electronic engineering, physics - field theory, and electromagnetism.

This article details the history of electrical engineering.
Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the "Thunderer of the Nile", and described them as the "protectors" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians.[1] Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects.[2] Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them.[3] Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning raad () applied to the electric ray.[4]
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus, an ancient Greek philosopher, writing at around 600 BCE, described a form of static electricity, noting that rubbing fur on various substances, such as amber, would cause a particular attraction between the two. He noted that the amber buttons could attract light objects such as hair and that if they rubbed the amber for long enough they could even get a spark to jump.
At around 450 BCE Democritus, a later Greek philosopher, developed an atomic theory that was similar to modern atomic theory. His mentor, Leucippus, is credited with this same theory. The hypothesis of Leucippus and Democritus held everything to be composed of atoms. But these atoms, called "atomos", were  indivisible, and indestructible. He presciently stated that between atoms lies empty space, and that atoms are constantly in motion. He was incorrect only in stating that atoms come in different sizes and shapes, and that each object had its own shaped and sized atom.[5][6]
An object found in Iraq in 1938, dated to about 250 BCE and called the Baghdad Battery, resembles a galvanic cell and is claimed by some to have been used for electroplating in Mesopotamia, although there is no evidence for this.
Electricity would remain little more than an intellectual curiosity for millennia. In 1600, the English scientist, William Gilbert extended the study of Cardano on electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber.[7] He coined the New Latin word electricus ("of amber" or "like amber", from  [elektron], the Greek word for "amber") to refer to the property of attracting small objects after being rubbed.[8] This association gave rise to the English words "electric" and "electricity", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.[9]
Further work was conducted by Otto von Guericke who showed electrostatic repulsion. Robert Boyle also published work.[10]
Though electrical phenomena had been known for centuries, in the 18th century, the systematic study of electricity became known as "the youngest of the sciences", and the public became electrified by the newest discoveries in the field.[11]
By 1705, Francis Hauksbee had discovered that if he placed a small amount of mercury in the glass of his modified version of Otto von Guericke's generator, evacuated the air from it to create a mild vacuum and rubbed the ball in order to build up a charge, a glow was visible if he placed his hand on the outside of the ball. This glow was bright enough to read by. It seemed to be similar to St. Elmo's Fire. This effect later became the basis of the gas-discharge lamp, which led to neon lighting and mercury vapor lamps. In 1706 he produced an 'Influence machine' to generate this effect.[12] He was elected a Fellow of the Royal Society the same year.[13]
Hauksbee continued to experiment with electricity, making numerous observations and developing machines to generate and demonstrate various electrical phenomena. In 1709 he published Physico-Mechanical Experiments on Various Subjects which summarized much of his scientific work.
Stephen Gray discovered the importance of insulators and conductors. C. F. du Fay seeing his work, developed a "two-fluid" theory of electricity.
[10]
In the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky.[14] A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature.[15] He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge, by coming up with the single fluid, two states theory of electricity.
In 1791, Italian Luigi Galvani published his discovery of bioelectricity, demonstrating that electricity was the medium by which nerve cells passed signals to the muscles.[10][16][17] Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used.[16][17]
Electrical engineering became a profession in the late 19th century. Practitioners had created a global electric telegraph network and the first electrical engineering institutions to support the new discipline were founded in the UK and US. Although it is impossible to precisely pinpoint a first electrical engineer, Francis Ronalds stands ahead of the field, who created a working electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity.[18][19] Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort.[20] The donation of his extensive electrical library was a considerable boon for the fledgling Society.
Development of the scientific basis for electrical engineering, with the tools of modern research techniques, intensified during the 19th century. Notable developments early in this century include the work of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, Michael Faraday, the discoverer of electromagnetic induction in 1831.[22] In the 1830s, Georg Ohm also constructed an early electrostatic machine. The homopolar generator was developed first by Michael Faraday during his memorable experiments in 1831. It was the beginning of modern dynamos  that is, electrical generators which operate using a magnetic field.  The invention of the industrial generator, which didn't need external magnetic power in 1866 by Werner von Siemens made a large series of other inventions in the wake possible.
In 1873 James Clerk Maxwell published a unified treatment of electricity and magnetism in A Treatise on Electricity and Magnetism which stimulated several theorists to think in terms of fields described by Maxwell's equations. In 1878, the British inventor James Wimshurst developed an apparatus that had two glass disks mounted on two shafts. It was not till 1883 that the Wimshurst machine was more fully reported to the scientific community.
During the latter part of the 1800s, the study of electricity was largely considered to be a subfield of physics. It was not until the late 19th century that universities started to offer degrees in electrical engineering. 
In 1882, Darmstadt University of Technology founded the first chair and the first faculty of electrical engineering worldwide. In the same year, under Professor Charles Cross, the Massachusetts Institute of Technology began offering the first option of Electrical Engineering within a physics department.[23] In 1883, Darmstadt University of Technology and Cornell University introduced the world's first courses of study in electrical engineering and in 1885 the University College London founded the first chair of electrical engineering in the United Kingdom. The University of Missouri subsequently established the first department of electrical engineering in the United States in 1886.[24]
During this period commercial use of electricity increased dramatically. Starting in the late 1870s cities started installing large scale electric street lighting systems based on arc lamps.[25] After the development of a practical incandescent lamp for indoor lighting, Thomas Edison switched on the world's first public electric supply utility in 1882, using what was considered a relatively safe 110 volts direct current system to supply customers. Engineering advances in the 1880s, including the invention of the transformer, led to electric utilities starting to adopting alternating current, up till then used primarily in arc lighting systems, as a distribution standard for outdoor and indoor lighting (eventually replacing direct current for such purposes). In the US there was a rivalry, primarily between a Westinghouse AC and the Edison DC system known as the "war of the currents".[26]
"By the mid-1890s the four "Maxwell equations" were recognized as the foundation of one of the strongest and most successful theories in all of physics; they had taken their place as companions, even rivals, to Newton's laws of mechanics. The equations were by then also being put to practical use, most dramatically in the emerging new technology of radio communications, but also in the telegraph, telephone, and electric power industries."[27] By the end of the 19th century, figures in the progress of electrical engineering were beginning to emerge.[28]
Charles Proteus Steinmetz helped foster the development of alternating current that made possible the expansion of the electric power industry in the United States, formulating mathematical theories for engineers.
During the development of radio, many scientists and inventors contributed to radio technology and electronics. In his classic UHF experiments of 1888, Heinrich Hertz demonstrated the existence of electromagnetic waves (radio waves) leading many inventors and scientists to try to adapt them to commercial applications, such as Guglielmo Marconi (1895) and Alexander Popov (1896).
Millimetre wave communication was first investigated by Jagadish Chandra Bose during 18941896, when he reached an extremely high frequency of up to 60GHz in his experiments.[29] He also introduced the use of semiconductor junctions to detect radio waves,[30] when he patented the radio crystal detector in 1901.[31][32]
John Fleming invented the first radio tube, the diode, in 1904.
Reginald Fessenden recognized that a continuous wave needed to be generated to make speech transmission possible, and by the end of 1906 he sent the first radio broadcast of voice. Also in 1906, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.[33] Edwin Howard Armstrong enabling technology for electronic television, in 1931.[34]
In the early 1920s, there was a growing interest in the development of domestic applications for electricity.[35] Public interest led to exhibitions such featuring "homes of the future" and in the UK, the Electrical Association for Women was established with Caroline Haslett as its director in 1924 to encourage women to become involved in electrical engineering.[36]
The second world war saw tremendous advances in the field of electronics; especially in radar and with the invention of the magnetron by Randall and Boot at the University of Birmingham in 1940. Radio location, radio communication and radio guidance of aircraft were all developed at this time. An early electronic computing device, Colossus was built by Tommy Flowers of the GPO to decipher the coded messages of the German Lorenz cipher machine. Also developed at this time were advanced clandestine radio transmitters and receivers for use by secret agents.
An American invention at the time was a device to scramble the telephone calls between Winston Churchill and Franklin D. Roosevelt. This was called the Green Hornet system and worked by inserting noise into the signal. The noise was then extracted at the receiving end. This system was never broken by the Germans.
A great amount of work was undertaken in the United States as part of the War Training Program in the areas of radio direction finding, pulsed linear networks, frequency modulation, vacuum tube circuits, transmission line theory and fundamentals of electromagnetic engineering. These studies were published shortly after the war in what became known as the 'Radio Communication Series' published by McGraw-Hill in 1946.
In 1941 Konrad Zuse presented the Z3, the world's first fully functional and programmable computer.[37]
Prior to the Second World War, the subject was commonly known as 'radio engineering' and was primarily restricted to aspects of communications and radar, commercial radio and early television. At this time, the study of radio engineering at universities could only be undertaken as part of a physics degree.
Later, in post war years, as consumer devices began to be developed, the field broadened to include modern TV, audio systems, Hi-Fi and latterly computers and microprocessors. 
In 1946 the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives, including the Apollo missions and the NASA moon landing.[38]
In the mid-to-late 1950s, the term radio engineering gradually gave way to the name electronics engineering, which then became a stand-alone university degree subject, usually taught alongside electrical engineering with which it had become associated due to some similarities.
The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947.[39] They then invented the bipolar junction transistor in 1948.[40] While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis,[41] they opened the door for more compact devices.[42]
The surface passivation process, which electrically stabilized silicon surfaces via thermal oxidation, was developed by Mohamed M. Atalla at BTL in 1957. This led to the development of the monolithic integrated circuit chip.[43][44][45] The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959.[46]
The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959.[47][48][49] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[41] It revolutionized the electronics industry,[50][51] becoming the most widely used electronic device in the world.[48][52][53] The MOSFET is the basic element in most modern electronic equipment,[54][55] and has been central to the electronics revolution,[56] the microelectronics revolution,[57] and the Digital Revolution.[49][58][59] The MOSFET has thus been credited as the birth of modern electronics,[60][61] and possibly the most important invention in electronics.[62]
John Bardeen, William Shockley, Walter Brattain  transistor (1947)
Mohamed M. Atalla  silicon passivation (1957) and MOSFET transistor (1959)
Robert Noyce  monolithic integrated circuit chip (1959)
Dawon Kahng  MOSFET transistor (1959)
Gordon Moore  Moore's law (1965)
Federico Faggin  silicon-gate MOSFET (1968) and microprocessor (1971)
Marcian Hoff  microprocessor (1971)
Masatoshi Shima, Stanley Mazor  microprocessor (1971)
The MOSFET made it possible to build high-density integrated circuit chips.[48] Atalla first proposed the concept of the MOS integrated circuit (MOS IC) chip in 1960, followed by Kahng in 1961.[41][63] The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962.[64] MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965.[65] Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968.[66] Since then, the MOSFET has been the basic building block of modern electronics.[49][67][68] The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking.[69]
The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP)[70][71] and silicon integrated circuit chips in the Apollo Guidance Computer (AGC).[72]
The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s.[73][55] The first single-chip microprocessor was the Intel 4004, released in 1971.[73] It began with the "Busicom Project"[74] as Masatoshi Shima's three-chip CPU design in 1968,[75][74] before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968.[76] The Intel 4004 was then designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology,[73] along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima.[74] This ignited the development of the personal computer. The 4004, a 4-bit processor, was followed in 1973 by the Intel 8080, an 8-bit processor, which made possible the building of the first personal computer, the Altair 8800.[77]

Electrical/Electronics engineering technology (EET) is an engineering technology field that implements and applies the principles of electrical engineering.[1] Like electrical engineering, EET deals with the "design, application, installation, manufacturing, operation or maintenance of electrical/electronic(s) systems."[2] However, EET is a specialized discipline that has more focus on application, theory, and applied design, and implementation, while electrical engineering may focus more of a generalized emphasis on theory and conceptual design.[3] Electrical/Electronic engineering technology is the largest branch of engineering technology and includes a diverse range of sub-disciplines, such as applied design, electronics, embedded systems, control systems, instrumentation, telecommunications, and power systems.
The Accreditation Board for Engineering and Technology (ABET) is the recognized[4] organization for accrediting both undergraduate engineering and engineering technology programs in the United States.[5]
EET curricula can vary widely by institution type, degree type, program objective, and expected student outcome. Each year after, however, ABET publishes a set of minimum criteria that a given EET program (either associate degree or bachelor's degree) must meet in order to maintain its ABET accreditation. These criteria may be classified as either general criteria, which apply to all ABET accredited programs, or as program criteria, which apply to discipline-specific criteria.[6]
Associate degree programs emphasize the practical field knowledge that is needed to maintain or troubleshoot existing electrical/electronic systems or to build and test new design prototypes.
Discipline-specific program outcomes include the application of circuit analysis and design, analog and digital electronics, computer programming, associated software, and relevant engineering standards
Coursework must be at a minimum algebra and trigonometry based.[2]
Bachelor's degree programs emphasize the analysis, design, and implementation of electrical/electronic systems. Some programs may focus on a specific sub-discipline, such as control systems or communications systems, while others may take a broader approach, introducing the student to several different sub-disciplines.[2]
Math to differential equations is a minimum requirement for ABET accredited bachelor's level EET degrees. In addition, graduates must demonstrate an understanding of basic project management skills.[2]
The United States Department of Commerce classifies the bachelor of science in electrical engineering technology (BSEET) as a STEM undergraduate engineering degree field.[7]
In many states, recent graduates and students who are close to finishing an undergraduate BSEET degree are qualified to sit-in for the Fundamentals of Engineering exam[8] while those BSEETs who have already gained at least four years post-college experience are qualified to sit-in for the Professional Engineer exam[9] for their licensure in the United States. The importance of the licensing board requirements[10] depend upon location, level of education, required years of experience, and the BSEETs sub-discipline are the passageways for becoming a licensed engineer. The knowledge obtained by a TAC/ABET accredited program is one pathway that may help students prepare for and pass the FE/PE exam. For example, in the United States and Canada, "only a licensed engineer may seal engineering work for public and private clients".[11]
Graduates of electrical/electronics engineering technology programs work in a wide range of career fields. Some examples include:[12]
Electrical/electronic engineering technicians may have a two-year associate degree [13] and considered craftsman technicians. Eventually, with additional experience and certifications obtained then the craftsman technicians may advance to master craftsman technicians.
Electrical/electronic engineering technologists are broad specialists, rather than central technicians. EETs have a bachelor's degree and are considered applied electrical or electronic engineers because they have electrical engineering concepts to use in their work.[13] Entry-level jobs in electrical or electronics engineering generally require a bachelor's degree in electrical engineering, electronics engineering, or electrical engineering technology.[14]

Engineering is the discipline and profession that applies scientific theories, mathematical methods, and empirical evidence to design, create, and analyze technological solutions cognizant of safety, human factors, physical laws, regulations, practicality, and cost. In the contemporary era, engineering is generally considered to consist of the major primary branches of chemical engineering, civil engineering, electrical engineering, and mechanical engineering.[1] There are numerous other engineering sub-disciplines and interdisciplinary subjects that may or may not be part of these major engineering branches.
Chemical engineering is the application of chemical, physical and biological sciences to the process of converting raw materials or chemicals into more useful or valuable forms.
Civil engineering comprises the design, construction, and maintenance of the physical and natural built environments.
Electrical engineering comprises the study and application of electricity, electronics and electromagnetism.
Mechanical engineering comprises the design and analysis of heat and mechanical power for the operation of machines and mechanical systems.[4]
Field of engineering that designs, construct and maintains different types of power plants. Serves as the prime mover to produce electricity.
Field of engineering that designs, construct and maintains different types of Industrial Machines and Equipment. 
With the process of mineral extraction, some amount of waste material and other byproducts are generated which are the primary source of pollution in the vicinity of mines. Mining activities by their nature cause a disturbance of the natural environment in and around which the minerals are located. Mining engineers must, therefore, be concerned not only with the production and processing of mineral commodities but also with the mitigation of damage to the environment both during and after mining as a result of the change in the mining area.



Astronomy (from Greek: , literally meaning the science that studies the laws of the stars) is a natural science that studies celestial objects and phenomena. It uses mathematics, physics, and chemistry in order to explain their origin and evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates beyond Earth's atmosphere. Cosmology is a branch of astronomy that studies the universe as a whole.[1]
Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Babylonians, Greeks, Indians, Egyptians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Nowadays, professional astronomy is often said to be the same as astrophysics.[2]
Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results.
Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets.
Astronomy (from the Greek  from  astron, "star" and - -nomia from  nomos, "law" or "culture") means "law of the stars" (or "culture of the stars" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects.[4] Although the two fields share a common origin, they are now entirely distinct.[5]
"Astronomy" and "astrophysics" are synonyms.[6][7][8] Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties,"[9] while "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena".[10] In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject.[11] However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics.[6] Some fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics", partly depending on whether the department is historically affiliated with a physics department,[7] and many professional astronomers have physics rather than astronomy degrees.[8] Some titles of the leading scientific journals in this field include The Astronomical Journal, The Astrophysical Journal, and Astronomy & Astrophysics.
In early historic times, astronomy only consisted of the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops and in understanding the length of the year.[12]
Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled and ideas on the nature of the Universe began to develop. Most early astronomy consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy.[13]
A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations.[15] The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.[16]
Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena.[17] In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model.[18] In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe.[19] Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy.[20] The Antikythera mechanism (c. 15080 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.[21]
Medieval Europe housed a number of important astronomers. Richard of Wallingford (12921336) made major contributions to astronomy and horology, including the invention of the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes and could predict eclipses. Nicole Oresme (13201382) and Jean Buridan (13001361) first discussed evidence for the rotation of the Earth, furthermore, Buridan also developed the theory of impetus (predecessor of the modern scientific theory of inertia) which was able to show planets were capable of motion without the intervention of angels.[22] Georg von Peuerbach (14231461) and Regiomontanus (14361476) helped make astronomical progress instrumental to Copernicus's development of the heliocentric model decades later.
Astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century.[23][24][25] In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars.[26] The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Abd al-Rahman al-Sufi, Biruni, Ab Ishq Ibrhm al-Zarql, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars.[27][28]
It is also believed that the ruins at Great Zimbabwe and Timbuktu[29] may have housed astronomical observatories.[30] In Post-classical West Africa, Astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens as well as precise diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in August 1583.[31][32]
Europeans had previously believed that there had been no astronomical observation in sub-Saharan Africa during the pre-colonial Middle Ages, but modern discoveries show otherwise.[33][34][35][36]
For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter.[37]
During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended by Galileo Galilei and expanded upon by Johannes Kepler. Kepler was the first to devise a system that correctly described the details of the motion of the planets around the Sun. However, Kepler did not succeed in formulating a theory behind the laws he wrote down.[38] It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope.[39]
Improvements in the size and quality of the telescope led to further discoveries. The English astronomer John Flamsteed catalogued over 3000 stars,[40] More extensive star catalogues were produced by Nicolas Louis de Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found.[41]
During the 1819th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.[42]
Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Joseph von Fraunhofer discovered about 600 bands in the spectrum of the Sun in 181415, which, in 1859, Gustav Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.[27]
The existence of the Earth's galaxy, the Milky Way, as its own group of stars was only proved in the 20th century, along with the existence of "external" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe.[43] Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have been used to explain such observed phenomena as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century. In the early 1900s the model of the Big Bang theory was formulated, heavily evidenced by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere.[citation needed] In February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves in the previous September.[44][45]
The main source of information about celestial bodies and other objects is visible light, or more generally electromagnetic radiation.[46] Observational astronomy may be categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.
Radio astronomy uses radiation with wavelengths greater than approximately one millimeter, outside the visible range.[47] Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.[47]
Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields.[47] Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21cm, are observable at radio wavelengths.[11][47]
A wide variety of other objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.[11][47]
Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous galactic protostars and their host star clusters.[49][50]
With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space.[51] Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.[52]
Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy.[53] Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000  to 7000  (400 nm to 700nm),[53] that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.
Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 (10 to 320nm).[47] Light at those wavelengths is absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei.[47] However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.[47]
X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10million) kelvins, and thermal emission from thick gases above 107 Kelvin.[47] Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.[47]
Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes.[47] The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.[54]
Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.[47]
In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth.
In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A.[47] Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories.[55] Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.[47]
Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole.[56] A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.[57][58]
The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.[59][60]
One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars.
Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.[61]
The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allow astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.[62]
During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.[63]
Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.[64][65]
Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and the model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to the total abandonment of a model.
Phenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves.
Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, dark matter and fundamental theories of physics.
A few examples of this process:
Along with Cosmic inflation, dark matter and dark energy are the current leading topics in astronomy,[66] as their discovery and controversy originated during the study of the galaxies.
Astrophysics is the branch of astronomy that employs the principles of physics and chemistry "to ascertain the nature of the astronomical objects, rather than their positions or motions in space".[67][68] Among the objects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background.[69][70] Their emissions are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, and black holes; whether or not time travel is possible, wormholes can form, or the multiverse exists; and the origin and ultimate fate of the universe.[69] Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics.
Astrochemistry is the study of the abundance and reactions of molecules in the Universe, and their interaction with radiation.[71] The discipline is an overlap of astronomy and chemistry. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form.
Studies in this field contribute to the understanding of the formation of the Solar System, Earth's origin and geology, abiogenesis, and the origin of climate and oceans.
Astrobiology is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and how humans can detect it if it does.[72] The term exobiology is similar.[73]
Astrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth.[74] The origin and early evolution of life is an inseparable part of the discipline of astrobiology.[75] Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories.
This interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.[76][77][78]
Cosmology (from the Greek  (kosmos) "world, universe" and  (logos) "word, study" or literally "logic") could be considered the study of the Universe as a whole.
Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years[79] to its present condition.[80] The concept of the Big Bang can be traced back to the discovery of the microwave background radiation in 1965.[80]
In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe.[80] (See also nucleocosmochronology.)
When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.[81]
A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.[82]
Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.[83]
Various fields of physics are crucial to studying the universe. Interdisciplinary studies involve the fields of quantum mechanics, particle physics, plasma physics, condensed matter physics, statistical mechanics, optics, and nuclear physics.
Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.[84]
The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos.
Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.[85]
As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies.
A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies.
Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction.
An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a supermassive black hole that is emitting radiation from in-falling material.
A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.[86]
The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.[87]
The Solar System orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view.
In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.[88]
Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.[89]
As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.[90]
Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.[91]
The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior.[92] Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.[89]
Almost all elements heavier than hydrogen and helium were created inside the cores of stars.[92]
The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.[93]
The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae;[94] while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebula.[95] The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole.[96] Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova.[97] Planetary nebulae and supernovae distribute the "metals" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.[98]
At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.[99]
The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth.[100] The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.[101]
The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona.
At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.[99]
A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth. The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines then descend into the atmosphere.[102]
Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made.[103]
The Solar System is divided into the inner Solar System (subdivided into the inner planets and the asteroid belt), the outer Solar System (subdivided into the outer planets and centaurs), comets, the trans-Neptunian region (subdivided into the Kuiper belt, and the scattered disc) and the farthest regions (e.g., boundaries of the heliosphere, and the Oort Cloud, which may extend as far as a light-year). The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer giant planets are the gas giants (Jupiter and Saturn) and the ice giants (Uranus and Neptune).[104]
The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.[105]
Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.[106]
A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.[107]
Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of a vast amount of observational astrophysical data.
The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low-temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.
Astronomy is one of the sciences to which amateurs can contribute the most.[108]
Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events that interest them.[109][110]
Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).[111][112]
Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.[113][114][115]
Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics.
Solar System Local Interstellar Cloud Local Bubble Gould Belt Orion Arm Milky Way Milky Way subgroup Local Group  Local Sheet  Virgo Supercluster  Laniakea Supercluster Observable universe UniverseEach arrow () may be read as "within" or "part of".


Astronomy is the scientific study of celestial objects.
Astronomy may also refer to:

Astronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of prehistory: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy. It was not completely separated in Europe (see astrology and astronomy) during the Copernican Revolution starting in 1543. In some cultures, astronomical data was used for astrological prognostication. The study of astronomy has received financial and social support from many institutions, especially the Church, which was its largest source of support between the 12th century to the Enlightenment.[1]
Ancient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.
Early cultures identified celestial objects with gods and spirits.[2] They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests, and that they understood celestial objects and events to be manifestations of the divine, hence early astronomy's connection to what is now called astrology. A 32,500 year old carved ivory Mammoth tusk could contain the oldest known star chart (resembling the constellation Orion).[3] It has also been suggested that drawing on the wall of the Lascaux caves in France dating from 33,000 to 10,000 years ago could be a graphical representation of the Pleiades, the Summer Triangle, and the Northern Crown.[4][5] Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions.
Calendars of the world have often been set by observations of the Sun and Moon (marking the day, month and year), and were important to agricultural societies, in which the harvest depended on planting at the correct time of year, and for which the nearly full moon was the only lighting for night-time travel into city markets.[6]
The common modern calendar is based on the Roman calendar. Although originally a lunar calendar, it broke the traditional link of the month to the phases of the Moon and divided the year into twelve almost-equal months, that mostly alternated between thirty and thirty-one days. Julius Caesar instigated calendar reform in 46BCE and introduced what is now called the Julian calendar, based upon the 36514 day year length originally proposed by the 4thcenturyBCE Greek astronomer Callippus.
Since 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy.
Among the discoveries are:
The origins of Western astronomy can be found in Mesopotamia, the "land between the rivers" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 35003000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, or an hour into 60 minutes, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics.
Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination.
The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the Enma Anu Enlil. The oldest significant astronomical text that we possess is Tablet 63 of the Enma Anu Enlil, the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.[21]
A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time.
The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (32360 BC). In the 3rd century BC, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model.
Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.[22]
Astronomy in the Indian subcontinent dates back to the period of Indus Valley Civilization during 3rd millennium BCE, when it was used to create calendars.[23] As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period.[24] Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. During the 6th century, astronomy was influenced by the Greek and Byzantine astronomical traditions.[23][25]
Aryabhata (476550), in his magnum opus Aryabhatiya (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon.[26][27][pageneeded] Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II.
Astronomy was advanced during the Shunga Empire and many star catalogues were produced during this time. The Shunga period is known[according to whom?] as the "Golden age of astronomy in India".
It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses.
Indian astronomers by the 6th century believed that comets were celestial bodies that re-appeared periodically. This was the view expressed in the 6th century by the astronomers Varahamihira and Bhadrabahu, and the 10th-century astronomer Bhattotpala listed the names and estimated periods of certain comets, but it is unfortunately not known how these figures were calculated or how accurate they were.[28]
Bhskara II (11141185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the Siddhantasiromani which consists of two parts: Goladhyaya (sphere) and Grahaganita (mathematics of the planets). He also calculated the time taken for the Earth to orbit the Sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies.
Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his Aryabhatiyabhasya, a commentary on Aryabhata's Aryabhatiya, developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more efficient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.[29][30]
The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis.
A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his Timaeus, Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul.[31] Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth.[32] This basic cosmological model prevailed, in various forms, until the 16th century.
In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive.[33] Eratosthenes estimated the circumference of the Earth with great accuracy.[34]
Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes.
The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.
Depending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the Megale Syntaxis (Great Synthesis), better known by its Arabic title Almagest, which had a lasting effect on astronomy up to the Renaissance. In his Planetary Hypotheses, Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.
The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco.[36] Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter Sun.[37] The length of the corridor down which sunlight would travel would have limited illumination at other times of the year. The Egyptians also found the position of Sirius (the dog star) who they believed was Anubis their Jackal headed god moving through the heavens. Its position was critical to their civilisation as when it rose heliacal in the east before sunrise it foretold the flooding of the Nile. It is also where we get the phrase 'dog days of summer' from.
Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar.
Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites:
And after the Singer advances the Astrologer (), with a horologium () in his hand, and a palm (), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the Sun and Moon and five planets; one on the conjunctions and phases of the Sun and Moon; and one concerns their risings.[38]The Astrologer's instruments (horologium and palm) are a plumb line and sighting instrument[clarification needed]. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arm's length. The "Hermetic" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.[39]
From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.
The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia.
Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses.
Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose.
Astrological divination was also an important part of astronomy. Astronomers took careful note of "guest stars"(Chinese: ; pinyin: kxng; lit.: 'guest star') which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 AD. Also, the supernova that created the Crab Nebula in 1054 is an example of a "guest star" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies.
The world's first star catalogue was made by Gan De, a Chinese astronomer, in the 4th century BC.
Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology.[40] A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.[41]
Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar.[42] Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.
The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy.[43] This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century.[44][45] Zij star catalogues were produced at these observatories.
In the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his Book of Fixed Stars. He also gave the first descriptions and pictures of "A Little Cloud" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This "cloud" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD.[46] The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi.[47][48] In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star.
In the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing.[49][50] In 11th-century Persia, Omar Khayym compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian.
Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel,[51] the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Ms ibn Shkir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth,[52] the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques,[53] and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.[54]
Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century,[55] and Qushji in the 15th century, leading to the development of an astronomical physics.[56]
After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages.[57]  Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.[58]
Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius.[59]  In the 6th century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.[60]
In the 7th century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the computus. This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.[61]
The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne.[62]  By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.[63]
Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.[64]
By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home.[65]  Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.[66]
In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the Earth moves, and not the heavens. However, he concluded "everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved."[67] In the 15th century, Cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun.
During the renaissance period, astronomy began to undergo a revolution in thought known as the Copernican Revolution, which gets the name from the astronomer Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His De revolutionibus orbium coelestium was published in 1543.[68] While in the long term this was a very controversial claim, in the very beginning it only brought minor controversy.[68] The theory became the dominant view because many figures, most notably Galileo Galilei, Johannes Kepler and Isaac Newton championed and improved upon the work. Other figures also aided this new model despite not believing the overall theory, like Tycho Brahe, with his well-known observations.[69]
Brahe, a Danish noble, was an essential astronomer in this period.[69] He came on the astronomical scene with the publication of De nova stella, in which he disproved conventional wisdom on the supernova SN 1572[69] (As bright as Venus at its peak, SN 1572 later became invisible to the naked eye, disproving  the Aristotelian doctrine of the immutability of the heavens.)[70][71] He also created the Tychonic system, where the Sun and Moon and the stars revolve around the Earth, but the other five planets revolve around the Sun. This system blended the mathematical benefits of the Copernican system with the "physical benefits" of the Ptolemaic system.[72] This was one of the systems people believed in when they did not accept heliocentrism, but could no longer accept the Ptolemaic system.[72] He is most known for his highly accurate observations of the stars and the solar system. Later he moved to Prague and continued his work. In Prague he was at work on the Rudolphine Tables, that were not finished until after his death.[73] The Rudolphine Tables was a star map designed to be more accurate than either the Alfonsine tables, made in the 1300s, and the Prutenic Tables, which were inaccurate.[73] He was assisted at this time by his assistant Johannes Kepler, who would later use his observations to finish Brahe's works and for his theories as well.[73]
After the death of Brahe, Kepler was deemed his successor and was given the job of completing Brahe's uncompleted works, like the Rudolphine Tables.[73] He completed the Rudolphine Tables in 1624, although it was not published for several years.[73] Like many other figures of this era, he was subject to religious and political troubles, like the Thirty Years' War, which led to chaos that almost destroyed some of his works. Kepler was, however, the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. He discovered the three Kepler's laws of planetary motion that now carry his name, those laws being as follows:

With these laws, he managed to improve upon the existing heliocentric model. The first two were published in 1609. Kepler's contributions improved upon the overall system, giving it more credibility because it adequately explained events and could cause more reliable predictions. Before this, the Copernican model was just as unreliable as the Ptolemaic model.  This improvement came because Kepler realized the orbits were not perfect circles, but ellipses.Galileo Galilei was among the first to use a telescope to observe the sky, and after constructing a 20x refractor telescope.[75] He discovered the four largest moons of Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor.[76] This discovery was the first known observation of satellites orbiting another planet.[76] He also found that our Moon had craters and observed, and correctly explained, sunspots, and that Venus exhibited a full set of phases resembling lunar phases.[77][78] Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it.[77] With the moons it demonstrated that the Earth does not have to have everything orbiting it and that other parts of the Solar System could orbit another object, such as the Earth orbiting the Sun.[76] In the Ptolemaic system the celestial bodies were supposed to be perfect so such objects should not have craters or sunspots.[79] The phases of Venus could only happen in the event that Venus' orbit is insides Earth's orbit, which could not happen if the Earth was the center. He, as the most famous example, had to face challenges from church officials, more specifically the Roman Inquisition.[80] They accused him of heresy because these beliefs went against the teachings of the Roman Catholic Church and were challenging the Catholic church's authority when it was at its weakest.[80] While he was able to avoid punishment for a little while he was eventually tried and pled guilty to heresy in 1633.[80] Although this came at some expense, his book was banned, and he was put under house arrest until he died in 1642.[81]Sir Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realizing that the same force that attracts objects to the surface of the Earth held the Moon in orbit around the Earth, Newton was able to explain  in one theoretical framework  all known gravitational phenomena. In his Philosophi Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Those first principles are as follows:
Thus while Kepler explained how the planets moved, Newton accurately managed to explain why the planets moved the way they do. Newton's theoretical developments laid many of the foundations of modern physics.
Outside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibniz and Cassini accepted only parts of Newton's system, preferring their own philosophies. Voltaire published a popular account in 1738.[83] In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets, publishing from 1798 to 1825. The early origins of the solar nebular model of planetary formation had begun.
Edmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the TitiusBode law was filled by the discovery of the asteroids Ceres and 2 Pallas Pallas in 1801 and 1802 with many more following.
At first, astronomical thought in America was based on Aristotelian philosophy,[84] but interest in the new astronomy began to appear in Almanacs as early as 1659.[85]
Cosmic pluralism is the name given to the idea that the stars are distant suns, perhaps with their own planetary systems.
Ideas in this direction were expressed in antiquity, by Anaxagoras and  by Aristarchus of Samos, but did not find mainstream acceptance. The first astronomer of the European Renaissance to suggest that the stars were distant suns was Giordano Bruno in his De l'infinito universo et mondi (1584). This idea was among the charges, albeit not in a prominent position,  brought against him by the Inquisition.
The idea became mainstream in the later 17th century, especially following the publication of Conversations on the Plurality of Worlds by Bernard Le Bovier de Fontenelle (1686), and by the early 18th century it was the default working assumptions in stellar astronomy.
The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby "fixed" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus.
William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction.[86] In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems.[87]
In the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to the Sun, but with a range of temperatures, masses and sizes.
The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption linesthe dark lines in stellar spectra caused by the atmosphere's absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types.[88]
The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens.[citation needed] Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827.[89]
The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s.[90]
The twentieth century saw increasingly rapid advances in the scientific study of stars. 
The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory.[91]
Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. 
In Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence.
At Princeton University, Henry Norris Russell plotted the spectral types of these stars against their absolute magnitude, and found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy. 
Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 doctoral thesis.[92] The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined.[93]
As evolutionary models of stars were developed during the 1930s, Bengt Strmgren introduced the term HertzsprungRussell diagram to denote a luminosity-spectral class diagram.
A refined scheme for stellar classification was published in 1943 by William Wilson Morgan and Philip Childs Keenan.
The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us. The "Great Debate" between Harlow Shapley and Heber Curtis, in the 1920s, concerned the nature of the Milky Way, spiral nebulae, and the dimensions of the universe.[94]

During the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, which was necessary to understand the observations.
In the 20th century, with the help of the use of photography, fainter objects were observed. The Sun was found to be part of a galaxy made up of more than 1010 stars (10 billion stars). The existence of other galaxies, one of the matters of the great debate, was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy.
Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot Big Bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.
Although in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human "computers",  who performed the tedious calculations while scientists performed research requiring more background knowledge.[95] A number of discoveries in this period were originally noted by the women "computers" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of the Solar System.
Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, "in only 4 years discovered and catalogued more stars than all the men in history put together."[96]
Most of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.[citation needed]


Indian astronomy has a long history stretching from pre-historic to modern times. Some of the earliest roots of Indian astronomy can be dated to the period of Indus Valley Civilization or earlier.[1][2] Astronomy later developed as a discipline of Vedanga or one of the "auxiliary disciplines" associated with the study of the Vedas,[3] dating 1500 BCE or older.[4] The oldest known text is the Vedanga Jyotisha, dated to 14001200 BCE (with the extant form possibly from 700 to 600 BCE).[5]
Indian astronomy was influenced by Greek astronomy beginning in the 4th century BCE[6][7][8] and through the early centuries of the Common Era, for example by the Yavanajataka[6] and the Romaka Siddhanta, a Sanskrit translation of a Greek text disseminated from the 2nd century.[9]
Indian astronomy flowered in the 5th6th century, with Aryabhata, whose Aryabhatiya represented the pinnacle of astronomical knowledge at the time. Later the Indian astronomy significantly influenced Muslim astronomy, Chinese astronomy, European astronomy,[10] and others. Other astronomers of the classical era who further elaborated on Aryabhata's work include Brahmagupta, Varahamihira and Lalla.
An identifiable native Indian astronomical tradition remained active throughout the medieval period and into the 16th or 17th century, especially within the Kerala school of astronomy and mathematics.
Some of the earliest forms of astronomy can be dated to the period of Indus Valley Civilization, or earlier.[1][2] Some cosmological concepts are present in the Vedas, as are notions of the movement of heavenly bodies and the course of the year.[3]
As in other traditions, there is a close association of astronomy and religion during the early history of the science, astronomical observation being necessitated by spatial and temporal requirements of correct performance of religious ritual. Thus, the Shulba Sutras, texts dedicated to altar construction, discusses advanced mathematics and basic astronomy.[11] Vedanga Jyotisha is another of the earliest known Indian texts on astronomy,[12] it includes the details about the Sun, Moon, nakshatras, lunisolar calendar.[13][14]
Greek astronomical ideas began to enter India in the 4th century BCE following the conquests of Alexander the Great.[6][7][8][9] By the early centuries of the Common Era, Indo-Greek influence on the astronomical tradition is visible, with texts such as the Yavanajataka[6] and Romaka Siddhanta.[9]
Later astronomers mention the existence of various siddhantas during this period, among them a text known as the
Surya Siddhanta. These were not fixed texts but rather an oral tradition of knowledge, and their content is not extant. The text today known as Surya Siddhanta dates to the Gupta period and was received by Aryabhata.
The classical era of Indian astronomy begins in the late Gupta era, in the 5th to 6th centuries.
The Pacasiddhntik by Varhamihira (505 CE) approximates the method for determination of the meridian direction from any three positions of the shadow using a gnomon.[11] By the time of Aryabhata the motion of planets was treated to be elliptical rather than circular.[15] Other topics included definitions of different units of time, eccentric models of planetary motion, epicyclic models of planetary motion, and planetary longitude corrections for various terrestrial locations.[15]
The divisions of the year were on the basis of religious rites and seasons (Rtu).[16] The duration from mid Marchmid May was taken to be spring (vasanta), mid Maymid July: summer (grishma), mid Julymid September: rains (varsha), mid Septembermid November: autumn (sharad), mid Novembermid January: winter (hemanta), mid Januarymid March: the dews (shishir).[16]
In the Vednga Jyotia, the year begins with the winter solstice.[17] Hindu calendars have several eras:
J.A.B. van Buitenen (2008) reports on the calendars in India:
The oldest system, in many respects the basis of the classical one, is known from texts of about 1000 BCE. It divides an approximate solar year of 360 days into 12 lunar months of 27 (according to the early Vedic text Taittirya Sahit 4.4.10.13) or 28 (according to the Atharvaveda, the fourth of the Vedas, 19.7.1.) days. The resulting discrepancy was resolved by the intercalation of a leap month every 60 months. Time was reckoned by the position marked off in constellations on the ecliptic in which the Moon rises daily in the course of one lunation (the period from New Moon to New Moon) and the Sun rises monthly in the course of one year. These constellations (nakatra) each measure an arc of 13 20 of the ecliptic circle. The positions of the Moon were directly observable, and those of the Sun inferred from the Moon's position at Full Moon, when the Sun is on the opposite side of the Moon. The position of the Sun at midnight was calculated from the nakatra that culminated on the meridian at that time, the Sun then being in opposition to that nakatra.[16]Among the devices used for astronomy was gnomon, known as Sanku, in which the shadow of a vertical rod is applied on a horizontal plane in order to ascertain the cardinal directions, the latitude of the point of observation, and the time of observation.[36] This device finds mention in the works of Varhamihira, ryabhata, Bhskara, Brahmagupta, among others.[11] The Cross-staff, known as Yasti-yantra, was used by the time of Bhaskara II (11141185 CE).[36] This device could vary from a simple stick to V-shaped staffs designed specifically for determining angles with the help of a calibrated scale.[36] The clepsydra (Ghat-yantra) was used in India for astronomical purposes until recent times.[36] hashi (2008) notes that: "Several astronomers also described water-driven instruments such as the model of fighting sheep."[36]
The armillary sphere was used for observation in India since early times, and finds mention in the works of ryabhata (476 CE).[37] The Goladpika detailed treatise dealing with globes and the armillary sphere was composed between 1380 and 1460 CE by Paramevara.[37] On the subject of the usage of the armillary sphere in India, hashi (2008) writes: "The Indian armillary sphere (gola-yantra) was based on equatorial coordinates, unlike the Greek armillary sphere, which was based on ecliptical coordinates, although the Indian armillary sphere also had an ecliptical hoop. Probably, the celestial coordinates of the junction stars of the lunar mansions were determined by the armillary sphere since the seventh century or so. There was also a celestial globe rotated by flowing water."[36]
An instrument invented by the mathematician and astronomer Bhaskara II (11141185 CE) consisted of a rectangular board with a pin and an index arm.[36] This devicecalled the Phalaka-yantrawas used to determine time from the sun's altitude.[36] The Kaplayantra was an equatorial sundial instrument used to determine the sun's azimuth.[36] Kartar-yantra combined two semicircular board instruments to give rise to a 'scissors instrument'.[36] Introduced from the Islamic world and first finding mention in the works of Mahendra Srithe court astronomer of Firuz Shah Tughluq (13091388 CE)the astrolabe was further mentioned by Padmanbha (1423 CE) and Rmacandra (1428 CE) as its use grew in India.[36]
Invented by Padmanbha, a nocturnal polar rotation instrument consisted of a rectangular board with a slit and a set of pointers with concentric graduated circles.[36] Time and other astronomical quantities could be calculated by adjusting the slit to the directions of  and  Ursa Minor.[36] hashi (2008) further explains that: "Its backside was made as a quadrant with a plumb and an index arm. Thirty parallel lines were drawn inside the quadrant, and trigonometrical calculations were done graphically. After determining the sun's altitude with the help of the plumb, time was calculated graphically with the help of the index arm."[36]
hashi (2008) reports on the observatories constructed by Jai Singh II of Amber:
The Mahrja of Jaipur, Sawai Jai Singh (16881743 CE), constructed five astronomical observatories at the beginning of the eighteenth century. The observatory in Mathura is not extant, but those in Delhi, Jaipur, Ujjain, and Banaras are. There are several huge instruments based on Hindu and Islamic astronomy. For example, the samrt.-yantra (emperor instrument) is a huge sundial which consists of a triangular gnomon wall and a pair of quadrants toward the east and west of the gnomon wall. Time has been graduated on the quadrants.[36]The seamless celestial globe invented in Mughal India, specifically Lahore and Kashmir, is considered to be one of the most impressive astronomical instruments and remarkable feats in metallurgy and engineering. All globes before and after this were seamed, and in the 20th century, it was believed by metallurgists to be technically impossible to create a metal globe without any seams, even with modern technology. It was in the 1980s, however, that Emilie Savage-Smith discovered several celestial globes without any seams in Lahore and Kashmir. The earliest was invented in Kashmir by Ali Kashmiri ibn Luqman in 158990 CE during Akbar the Great's reign; another was produced in 165960 CE by Muhammad Salih Tahtawi with Arabic and Sanskrit inscriptions; and the last was produced in Lahore by a Hindu metallurgist Lala Balhumal Lahuri in 1842 during Jagatjit Singh Bahadur's reign. 21 such globes were produced, and these remain the only examples of seamless metal globes. These Mughal metallurgists developed the method of lost-wax casting in order to produce these globes.[38]
According to David Pingree, there are a number of Indian astronomical texts that are dated to the sixth century CE or later with a high degree of certainty. There is substantial similarity between these and pre-Ptolomaic Greek astronomy.[39] Pingree believes that these similarities suggest a Greek origin for certain aspects of Indian astronomy. One of the direct proofs for this approach is the fact quoted that many Sanskrit words related to astronomy, astrology and calendar are either direct phonetical borrowings from the Greek language, or translations, assuming complex ideas, like the names of the days of the week which presuppose a relation between those days, planets (including Sun and Moon) and gods.
With the rise of Greek culture in the east, Hellenistic astronomy filtered eastwards to India, where it profoundly influenced the local astronomical tradition.[6][7][8][9][40] For example, Hellenistic astronomy is known to have been practiced near India in the Greco-Bactrian city of Ai-Khanoum from the 3rd century BCE. Various sun-dials, including an equatorial sundial adjusted to the latitude of Ujjain have been found in archaeological excavations there.[41] 
Numerous interactions with the Mauryan Empire, and the later expansion of the Indo-Greeks into India suggest that transmission of Greek astronomical ideas to India occurred during this period.[42] 
The Greek concept of a spherical earth surrounded by the spheres of planets, further influenced the astronomers like Varahamihira and Brahmagupta.[40][43]
Several Greco-Roman astrological treatises are also known to have been exported to India during the first few centuries of our era. The Yavanajataka was a Sanskrit text of the 3rd century CE on Greek horoscopy and mathematical astronomy.[6] Rudradaman's capital at Ujjain "became the Greenwich of Indian astronomers and the Arin of the Arabic and Latin astronomical treatises; for it was he and his successors who encouraged the introduction of Greek horoscopy and astronomy into India."[44]
Later in the 6th century, the Romaka Siddhanta ("Doctrine of the Romans"), and the Paulisa Siddhanta ("Doctrine of Paul") were considered as two of the five main astrological treatises, which were compiled by Varhamihira in his Paca-siddhntik ("Five Treatises"), a compendium of Greek, Egyptian, Roman and Indian astronomy.[45] Varhamihira goes on to state that "The Greeks, indeed, are foreigners, but with them this science (astronomy) is in a flourishing state."[9] Another Indian text, the Gargi-Samhita, also similarly compliments the Yavanas (Greeks) noting that the Yavanas though barbarians must be respected as seers for their introduction of astronomy in India.[9]
Indian astronomy reached China with the expansion of Buddhism during the Later Han (25220 CE).[46] Further translation of Indian works on astronomy was completed in China by the Three Kingdoms era (220265 CE).[46] However, the most detailed incorporation of Indian astronomy occurred only during the Tang Dynasty (618907 CE) when a number of Chinese scholarssuch as Yi Xing were versed both in Indian and Chinese astronomy.[46] A system of Indian astronomy was recorded in China as Jiuzhi-li (718 CE), the author of which was an Indian by the name of Qutan Xidaa translation of Devanagari Gotama Siddhathe director of the Tang dynasty's national astronomical observatory.[46]
Fragments of texts during this period indicate that Arabs adopted the sine function (inherited from Indian mathematics) instead of the chords of arc used in Hellenistic mathematics.[47] Another Indian influence was an approximate formula used for timekeeping by Muslim astronomers.[48] Through Islamic astronomy, Indian astronomy had an influence on European astronomy via Arabic translations. During the Latin translations of the 12th century, Muhammad al-Fazari's Great Sindhind (based on the Surya Siddhanta and the works of Brahmagupta), was translated into Latin in 1126 and was influential at the time.[49]
In the 17th century, the Mughal Empire saw a synthesis between Islamic and Hindu astronomy, where Islamic observational instruments were combined with Hindu computational techniques. While there appears to have been little concern for planetary theory, Muslim and Hindu astronomers in India continued to make advances in observational astronomy and produced nearly a hundred Zij treatises. Humayun built a personal observatory near Delhi, while Jahangir and Shah Jahan were also intending to build observatories but were unable to do so. After the decline of the Mughal Empire, it was a Hindu king, Jai Singh II of Amber, who attempted to revive both the Islamic and Hindu traditions of astronomy which were stagnating in his time. In the early 18th century, he built several large observatories called Yantra Mandirs in order to rival Ulugh Beg's Samarkand observatory and in order to improve on the earlier Hindu computations in the Siddhantas and Islamic observations in Zij-i-Sultani. The instruments he used were influenced by Islamic astronomy, while the computational techniques were derived from Hindu astronomy.[50][51]
Some scholars have suggested that knowledge of the results of the Kerala school of astronomy and mathematics may have been transmitted to Europe through the trade route from Kerala by traders and Jesuit missionaries.[52] Kerala was in continuous contact with China, Arabia and Europe.  The existence of circumstantial evidence[53] such as communication routes and a suitable chronology certainly make such a transmission a possibility. However, there is no direct evidence by way of relevant manuscripts that such a transmission took place.[52]
In the early 18th century, Jai Singh II of Amber invited European Jesuit astronomers to one of his Yantra Mandir observatories, who had bought back the astronomical tables compiled by Philippe de La Hire in 1702. After examining La Hire's work, Jai Singh concluded that the observational techniques and instruments used in European astronomy were inferior to those used in India at the time  it is uncertain whether he was aware of the Copernican Revolution via the Jesuits.[54] He did, however, employ the use of telescopes. 
In his Zij-i Muhammad Shahi, he states: "telescopes were constructed in my kingdom and using them a number of observations were carried out".[55]
Following the arrival of the British East India Company in the 18th century, the Hindu and Islamic traditions were slowly displaced by European astronomy, though there were attempts at harmonising these traditions. The Indian scholar Mir Muhammad Hussain had travelled to England in 1774 to study Western science and, on his return to India in 1777, he wrote a Persian treatise on astronomy. He wrote about the heliocentric model, and argued that there exists an infinite number of universes (awalim), each with their own planets and stars, and that this demonstrates the omnipotence of God, who is not confined to a single universe. Hussain's idea of a universe resembles the modern concept of a galaxy, thus his view corresponds to the modern view that the universe consists of billions of galaxies, each one consisting of billions of stars.[56] 
The last known Zij treatise was the Zij-i Bahadurkhani, written in 1838 by the Indian astronomer Ghulam Hussain Jaunpuri (17601862) and printed in 1855, dedicated to Bahadur Khan. The treatise incorporated the heliocentric system into the Zij tradition.[57]




The Sun is the star at the center of the Solar System. It is a nearly perfect ball of hot plasma,[18][19] heated to incandescence by nuclear fusion reactions in its core, radiating the energy mainly as visible light, ultraviolet light, and infrared radiation. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39million kilometres (864,000 miles), or 109 times that of Earth. Its mass is about 330,000 times that of Earth; it accounts for about 99.86% of the total mass of the Solar System.[20] Roughly three quarters of the Sun's mass consists of hydrogen (~73%); the rest is mostly helium (~25%), with much smaller quantities of heavier elements, including oxygen, carbon, neon and iron.[21]
The Sun is a G-type main-sequence star (G2V) based on its spectral class. As such, it is informally and not completely accurately referred to as a yellow dwarf (its light is closer to white than yellow). It formed approximately 4.6billion[a][14][22] years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became so hot and dense that it eventually initiated nuclear fusion in its core. It is thought that almost all stars form by this process.
The Sun's core fuses about 600million tons of hydrogen into helium every second, converting 4million tons of matter into energy every second as a result. This energy, which can take between 10,000 and 170,000 years to escape the core, is the source of the Sun's light and heat. When hydrogen fusion in its core has diminished to the point at which the Sun is no longer in hydrostatic equilibrium, its core will undergo a marked increase in density and temperature while its outer layers expand, eventually transforming the Sun into a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury and Venus, and render Earth uninhabitable  but not for about five billion years. After this, it will shed its outer layers and become a dense type of cooling star known as a white dwarf, and no longer produce energy by fusion, but still glow and give off heat from its previous fusion.
The enormous effect of the Sun on Earth has been recognized since prehistoric times. The Sun was thought of by some cultures as a deity. The synodic rotation of Earth and its orbit around the Sun are the basis of some solar calendars. The predominant calendar in use today is the Gregorian calendar which is based upon the standard 16th Century interpretation that the Sun's observed movement is primarily due to it actually moving.[23]
The English word sun developed from Old English sunne. Cognates appear in other Germanic languages, including West Frisian sinne, Dutch zon, Low German Snn, Standard German Sonne,  Bavarian Sunna, Old Norse sunna, and Gothic sunn. All these words stem from Proto-Germanic *sunnn.[24][25] This is ultimately related to the word for sun in other branches of the Indo-European language family, though in most cases a nominative stem with an l is found, rather than the genitive stem in n, as for example in Latin sl, ancient Greek  (hlios), Welsh haul and Russian  (solntse; pronounced sontse), as well as (with *l > r) Sanskrit  (svr) and Persian  (xvar). Indeed, the l-stem survived in Proto-Germanic as well, as *swelan, which gave rise to Gothic sauil (alongside sunn) and Old Norse prosaic sl (alongside poetic sunna), and through it the words for sun in the modern Scandinavian languages: Swedish and Danish solen, Icelandic slin, etc.[25]
In English, the Greek and Latin words occur in poetry as personifications of the Sun, Helios (/hilis/) and Sol (/sl/),[2][1] while in science fiction Sol may be used as a name for the Sun to distinguish it from other stars. The term sol with a lower-case s is used by planetary astronomers for the duration of a solar day on another planet such as Mars.[26]
The principal adjectives for the Sun in English are sunny for sunlight and, in technical contexts, solar (/solr/),[3] from Latin sol[27]  the latter found in terms such as solar day, solar eclipse and Solar System (occasionally Sol system).
From the Greek helios comes the rare adjective heliac (/hilik/).[28]
The English weekday name Sunday stems from Old English Sunnandg "sun's day", a Germanic interpretation of the Latin phrase dis slis, itself a translation of the ancient Greek   (hmera hliou) 'day of the sun'.[29]
The Sun is a G-type main-sequence star that constitutes about 99.86% of the mass of the Solar System. The Sun has an absolute magnitude of +4.83, estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs.[30][31] The Sun is a Population I, or heavy-element-rich,[b] star.[32] The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae.[33] This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. The heavy elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.[32]
The Sun is by far the brightest object in the Earth's sky, with an apparent magnitude of 26.74.[34][35] This is about 13billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of 1.46. One astronomical unit (about 150,000,000km; 93,000,000mi) is defined as the mean distance of the Sun's center to Earth's center, though the distance varies as Earth moves from perihelion in January to aphelion in July.[36] The distances can vary between 147,098,074km (perihelion) and 152,097,701km (aphelion), and extreme values can range from 147,083,346km to 152,112,126km.[37] At its average distance, light travels from the Sun's horizon to Earth's horizon in about 8 minutes and 19 seconds, while light from the closest points of the Sun and Earth takes about two seconds less. The energy of this sunlight supports almost all life[c] on Earth by photosynthesis,[38] and drives Earth's climate and weather.
The Sun does not have a definite boundary, but its density decreases exponentially with increasing height above the photosphere.[39] For the purpose of measurement, the Sun's radius is considered to be the distance from its center to the edge of the photosphere, the apparent visible surface of the Sun.[40] By this measure, the Sun is a near-perfect sphere with an oblateness estimated at 9millionths,[41] which means that its polar diameter differs from its equatorial diameter by only 10 kilometres (6.2mi).[42] The tidal effect of the planets is weak and does not significantly affect the shape of the Sun.[43] The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the apparent rotational period of the Sun at its equator is about 28 days.[44] Viewed from a vantage point above its north pole, the Sun rotates counterclockwise around its axis of spin.[d][45]
The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately 1,368W/m2 (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth).[46] Sunlight on the surface of Earth is attenuated by Earth's atmosphere, so that less power arrives at the surface (closer to 1,000W/m2) in clear conditions when the Sun is near the zenith.[47] Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light.[48] The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths.[49] Solar ultraviolet radiation ionizes Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.[50]
The Sun emits light across the visible spectrum, so its color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. The Solar radiance per wavelength peaks in the green portion of the spectrum when viewed from space.[51][52] When the Sun is low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.[53]
The Sun is a G2V star, with G2 indicating its surface temperature of approximately 5,778K (5,505C, 9,941F), and V that it, like most stars, is a main-sequence star.[54][55] The average luminance of the Sun is about 1.88gigacandela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44Gcd/m2.[e] However, the luminance is not constant across the disk of the Sun, due to limb darkening.
The Sun is composed primarily of the chemical elements hydrogen and helium. At this time in the Sun's life, they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively.[56] All heavier elements, called metals in astronomy, account for less than 2% of the mass, with oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%) being the most abundant.[57]
The Sun's original chemical composition was inherited from the interstellar medium out of which it formed. Originally it would have contained about 71.1% hydrogen, 27.4% helium, and 1.5% heavier elements.[56] The hydrogen and most of the helium in the Sun would have been produced by Big Bang nucleosynthesis in the first 20 minutes of the universe, and the heavier elements were produced by previous generations of stars before the Sun was formed, and spread into the interstellar medium during the final stages of stellar life and by events such as supernovae.[58]
Since the Sun formed, the main fusion process has involved fusing hydrogen into helium. Over the past 4.6billion years, the amount of helium and its location within the Sun has gradually changed. Within the core, the proportion of helium has increased from about 24% to about 60% due to fusion, and some of the helium and heavy elements have settled from the photosphere towards the center of the Sun because of gravity. The proportions of metals (heavier elements) is unchanged. Heat is transferred outward from the Sun's core by radiation rather than by convection (see Radiative zone below), so the fusion products are not lifted outward by heat; they remain in the core[59] and gradually an inner core of helium has begun to form that cannot be fused because presently the Sun's core is not hot or dense enough to fuse helium. In the current photosphere, the helium fraction is reduced, and the metallicity is only 84% of what it was in the protostellar phase (before nuclear fusion in the core started). In the future, helium will continue to accumulate in the core, and in about 5billion years this gradual build-up will eventually cause the Sun to exit the main sequence and become a red giant.[60]
The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System.[61] The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by the settling of heavy elements. The two methods generally agree well.[21]
In the 1970s, much research focused on the abundances of iron-group elements in the Sun.[62][63] Although significant research was done, until 1978 it was difficult to determine the abundances of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.[62]
The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s,[64] and these were subsequently improved.[65] In 1978, the abundances of singly ionized elements of the iron group were derived.[62]
Various authors have considered the existence of a gradient in the isotopic compositions of solar and planetary noble gases,[66] e.g. correlations between isotopic compositions of neon and xenon in the Sun and on the planets.[67]
Prior to 1983, it was thought that the whole Sun has the same composition as the solar atmosphere.[68] In 1983, it was claimed that it was fractionation in the Sun itself that caused the isotopic-composition relationship between the planetary and solar-wind-implanted noble gases.[68]
The structure of the Sun contains the following layers:
Temperature profile in the Sun
Mass inside a given radius
Density profile
Pressure profile
The core of the Sun extends from the center to about 2025% of the solar radius.[69] It has a density of up to 150g/cm3[70][71] (about 150 times the density of water) and a temperature of close to 15.7million kelvins (K).[71] By contrast, the Sun's surface temperature is approximately 5800K. Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the radiative zone above.[69] Through most of the Sun's life, energy has been produced by nuclear fusion in the core region through a series of nuclear reactions called the pp (protonproton) chain; this process converts hydrogen into helium.[72] Only 0.8% of the energy generated in the Sun comes from another sequence of fusion reactions called the CNO cycle, though this proportion is expected to increase as the Sun becomes older.[73][74]
The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The remainder of the Sun is heated by this energy as it is transferred outwards through many successive layers, finally to the solar photosphere where it escapes into space through radiation (photons) or advection (massive particles).[54][75]
The protonproton chain occurs around 9.21037 times each second in the core, converting about 3.71038 protons into alpha particles (helium nuclei) every second (out of a total of ~8.91056 free protons in the Sun), or about 6.21011kg/s.[54] Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nucleus) releases around 0.7% of the fused mass as energy,[76] so the Sun releases energy at the massenergy conversion rate of 4.26million metric tons per second (which requires 600 metric megatons of hydrogen [77]), for 384.6yottawatts (3.8461026W),[5] or 9.1921010megatons of TNT per second. The large power output of the Sun is mainly due to the huge size and density of its core (compared to Earth and objects on Earth), with only a fairly small amount of power being generated per cubic metre. Theoretical models of the Sun's interior indicate a maximum power density, or energy production, of approximately 276.5 watts per cubic metre at the center of the core,[78] which is about the same power density inside a compost pile.[79][f]
The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the density and hence the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the density and increasing the fusion rate and again reverting it to its present rate.[80][81]
From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer.[82] The temperature drops from approximately 7million to 2million kelvins with increasing distance from the core.[71] This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection, which explains why the transfer of energy through this zone is by radiation instead of thermal convection.[71] Ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions.[82] The density drops a hundredfold (from 20 g/cm3 to 0.2 g/cm3) between 0.25 solar radii and 0.7 radii, the top of the radiative zone.[82]
The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone  results in a large shear between the twoa condition where successive horizontal layers slide past one another.[83] Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.[71]
The Sun's convection zone extends from 0.7 solar radii (500,000km) to near the surface. In this layer, the solar plasma is not dense enough or hot enough to transfer the heat energy of the interior outward via radiation. Instead, the density of the plasma is low enough to allow convective currents to develop and move the Sun's energy outward towards its surface. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, an orderly motion of the mass develops into thermal cells that carry the majority of the heat outward to the Sun's photosphere above. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it again picks up heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m3 (about 1/10,000 the density of air at sea level).[71]
The thermal columns of the convection zone form an imprint on the surface of the Sun giving it a granular appearance called the solar granulation at the smallest scale and supergranulation at larger scales. Turbulent convection in this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun.[71] The Sun's thermal columns are Bnard cells and take the shape of roughly hexagonal prisms.[84]
The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light.[85] Photons produced in this layer escape the Sun through the transparent solar atmosphere above it and become solar radiation, sunlight. The change in opacity is due to the decreasing amount of H ions, which absorb visible light easily.[85] Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H ions.[86][87]
The photosphere is tens to hundreds of kilometers thick, and is slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or limb of the solar disk, in a phenomenon known as limb darkening.[85] The spectrum of sunlight has approximately the spectrum of a black-body radiating at 5,777K (5,504C; 9,939F), interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~1023m3 (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionizedthe extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.[88]
During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed helium, after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.[89]
During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, parts of the Sun's surrounding atmosphere can be seen. It is composed of four distinct parts: the chromosphere, the transition region, the corona and the heliosphere.
The coolest layer of the Sun is a temperature minimum region extending to about 500km above the photosphere, and has a temperature of about 4,100K.[85] This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.[90]
The chromosphere, transition region, and corona are much hotter than the surface of the Sun.[85] The reason is not well understood, but evidence suggests that Alfvn waves may have enough energy to heat the corona.[91]
Above the temperature minimum layer is a layer about 2,000km thick, dominated by a spectrum of emission and absorption lines.[85] It is called the chromosphere from the Greek root chroma, meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses.[82] The temperature of the chromosphere increases gradually with altitude, ranging up to around 20,000K near the top.[85] In the upper part of the chromosphere helium becomes partially ionized.[92]
Above the chromosphere, in a thin (about 200km) transition region, the temperature rises rapidly from around 20000K in the upper chromosphere to coronal temperatures closer to 1000000K.[93] The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma.[92] The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion.[82] The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.[94]
The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 1015m3 to 1016m3.[92][g] The average temperature of the corona and solar wind is about 1,000,0002,000,000 K; however, in the hottest regions it is 8,000,00020,000,000 K.[93] Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.[93][95]
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere.  A flow of plasma outward from the Sun into interplanetary space is the solar wind.[95]
The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes superalfvnicthat is, where the flow becomes faster than the speed of Alfvn waves,[96] at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvn waves. The solar wind travels outward continuously through the heliosphere,[97][98] forming the solar magnetic field into a spiral shape,[95] until it impacts the heliopause more than 50AU from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause.[99] In late 2012 Voyager 1 recorded a marked increase in cosmic ray collisions and a sharp drop in lower energy particles from the solar wind, which suggested that the probe had passed through the heliopause and entered the interstellar medium,[100] and indeed did so August 25, 2012 at approximately 122 astronomical units from the sun.[101] The heliosphere has a heliotail which stretches out behind it due to the Sun's movement.[102]
High-energy gamma ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000years.[103] In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state if the rate of energy generation in its core were suddenly changed.[104]
Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing 23 of them because the neutrinos had changed flavor by the time they were detected.[105]
The Sun has a magnetic field that varies across its surface. Its polar field is 12 gauss (0.00010.0002T), whereas the field is typically 3,000 gauss (0.3T) in features on the Sun called sunspots and 10100 gauss (0.0010.01T) in solar prominences.[5] The magnetic field varies in time and location. The quasi-periodic 11-year solar cycle is the most prominent variation in which the number and size of sunspots waxes and wanes.[107][108][109]
Sunspots are visible as dark patches on the Sun's photosphere and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, so they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend to form closer to the solar equator, a phenomenon known as Sprer's law. The largest sunspots can be tens of thousands of kilometers across.[110]
An 11-year sunspot cycle is half of a 22-year BabcockLeighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of the toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned eastwest and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.[111][112]
During the solar cycle's declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number and size. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.[113][114]
The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field.[95] In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind.[95] At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral.[95] The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50400T (at the photosphere) reduces with the inverse-cube of the distance, leading to a predicted magnetic field of 0.1nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5nT, about a hundredtimes greater.[115] The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.
The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System.[116] The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.
With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.
In December 2019, a new type of solar magnetic explosion was observed, known as forced magnetic reconnection. Previously, in a process called spontaneous magnetic reconnection, it was observed that the solar magnetic field lines diverge explosively and then converge again instantaneously. Forced Magnetic Reconnection was similar, but it was triggered by an explosion in the corona.[117]
Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance,[118] which, in turn, might influence Earth's long-term climate.[119]
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures.[120] Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.[121]
A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.[122][123]
The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for over four billion[a] years and will remain fairly stable for more than five billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo dramatic changes, both internally and externally.
The Sun formed about 4.6billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars.[124] This age is estimated using computer models of stellar evolution and through nucleocosmochronology.[14] The result is consistent with the radiometric date of the oldest Solar System material, at 4.567billion years ago.[125][126] Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity.[127] As one fragment of the cloud collapsed it also began to rotate due to conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accumulated more matter from the surrounding disk, eventually triggering nuclear fusion.
HD 162826 and HD 186302 are hypothesized stellar siblings of the Sun, having formed in the same molecular cloud.
The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10billion years as a main-sequence star.[129]
The Sun is gradually becoming hotter in its core, hotter at the surface, larger in radius, and more luminous during its time on the main sequence: since the beginning of its main sequence life, it has expanded in radius by 15% and the surface has increased in temperature from 5620K to 5777K, resulting in a 48% increase in luminosity from 0.677 solar luminosities to its present-day 1.0 solar luminosity. This occurs because the helium atoms in the core have a higher mean molecular weight than the hydrogen atoms that were fused, resulting in less thermal pressure. The core is therefore shrinking, allowing the outer layers of the Sun to move closer to the center, releasing gravitational potential energy. According to the virial theorem, half this released gravitational energy goes into heating, which leads to a gradual increase in the rate at which fusion occurs and thus an increase in the luminosity. This process speeds up as the core gradually becomes denser.[130] At present, it is increasing in brightness by about 1% every 100million years. It takes at least 1 billion years from now to deplete liquid water from the Earth from such increase.[131]
The Sun does not have enough mass to explode as a supernova. Instead, when it runs out of hydrogen in the core in approximately 5billion years, core hydrogen fusion will stop and there will be nothing to prevent the core from contracting. The release of gravitational potential energy causes the luminosity of the star to increase, ending the main sequence phase and leading the star to expand over the next billion years: first into a subgiant, and then into a red giant.[130][132][133] The heating due to gravitational contraction will also lead to hydrogen fusion in a shell just outside the core, where unfused hydrogen remains, contributing to the increased luminosity, which will eventually reach more than 1000 times its present luminosity.[130] As a red giant, the Sun will grow so large that it will engulf Mercury, Venus, and probably Earth, reaching about 0.75AU.[133][134] The Sun will spend around a billion years as a red-giant branch star and lose around a third of its mass.[133]
After the red-giant branch, the Sun has approximately 120million years of active life left, but much happens. First, the core, full of degenerate helium ignites violently in the helium flash, where it is estimated that 6% of the core, itself 40% of the Sun's mass, will be converted into carbon within a matter of minutes through the triple-alpha process.[135] The Sun then shrinks to around 10 times its current size and 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch, but a star of the Sun's metallicity does not evolve blueward along the horizontal branch. Instead, it just becomes moderately larger and more luminous over about 100million years as it continues to react helium in the core.[133]
When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch phase, and the Sun is alternately reacting hydrogen in a shell or helium in a deeper shell. After about 20million years on the early asymptotic giant branch, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU.[136] According to a 2008 model, Earth's orbit will have initially expanded significantly due to the Sun's loss of mass as a red giant, but will later start shrinking due to tidal forces (and, eventually, drag from the lower chromosphere) so that it is engulfed by the Sun during the tip of the red-giant branch phase, 3.8 and 1million years after Mercury and Venus have respectively suffered the same fate. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the red-giant branch produce smaller, less luminous stars at the tip of the asymptotic giant branch, perhaps only 2,000 times the luminosity and less than 200 times the radius.[133] For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phaselasting approximately 500,000 yearsthe Sun will only have about half of its current mass.
The post-asymptotic-giant-branch evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionized into a planetary nebula as the exposed core reaches 30,000 K, as if it is in a sort of blue loop. The final naked core, a white dwarf, will have a temperature of over 100,000 K, and contain an estimated 54.05% of the Sun's present-day mass.[133] The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to a hypothetical black dwarf.[137][138]
The Sun is moved by the gravitational pull of the planets. One can think of the barycentre of the Solar System as being stationary (or as moving in a steady motion around the galaxy). The centre of the sun is always within 2.2 solar radii of the barycentre. This motion of the Sun is mainly due to Jupiter, Saturn, Uranus, and Neptune. For some periods of several decades, the motion is rather regular, forming a trefoil pattern, whereas between these periods it appears more chaotic.[139] After 179 years (nine times the synodic period of Jupiter and Saturn) the pattern more or less repeats, but rotated by about 24.[140] The orbits of the inner planets, including of the Earth, are similarly displaced by the same gravitational forces, so the movement of the Sun has little effect on the relative positions of the Earth and the Sun or on solar irradiance on the Earth as a function of time.[141]
The Sun orbits the center of the Milky Way, and it is presently moving in the direction of the constellation of Cygnus. A simple model of the motion of a star in the galaxy gives the galactic coordinates X, Y, and Z as:
where U, V, and W are the respective velocities with respect to the local standard of rest, A and B are the Oort constants, 






0


=
A

B


{\displaystyle \Omega _{0}=A-B}

 is the angular velocity of galactic rotation for the local standard of rest, 




=



4



0


B




{\displaystyle \kappa ={\sqrt {-4\Omega _{0}B}}}

 is the "epicyclic frequency", and  is the vertical oscillation frequency.[142] For the sun, the present values of U, V, and W are estimated as 



(
U
(
0
)
,
V
(
0
)
,
W
(
0
)
)
=
(
10.00
,
5.25
,
7.17
)


{\displaystyle (U(0),V(0),W(0))=(10.00,5.25,7.17)}

 km/s, and estimates for the other constants are A=15.5km/s/kpc, B=12.2km/s/kpc, =37km/s/kpc, and =74km/s/kpc. We take X(0) and Y(0) to be zero and Z(0) is estimated to be 17 parsecs.[143] This model implies that the Sun circulates around a point that is itself going around the galaxy. The period of the Sun's circulation around the point is 



2


/




{\displaystyle 2\pi /\kappa }

. which, using the equivalence that a parsec equals 1km/s times 0.978million years, comes to 166million years, shorter than the time it takes for the point to go around the galaxy. In the (X, Y) coordinates, the Sun describes an ellipse around the point, whose length in the Y direction is
and whose width in the X direction is
The ratio of length to width of this ellipse, the same for all stars in our neighborhood, is 



2


/



1.50.


{\displaystyle 2\Omega /\kappa \approx 1.50.}


The moving point is presently at
The oscillation in the Z direction takes the Sun
above the galactic plane and the same distance below it, with a period of 



2


/




{\displaystyle 2\pi /\nu }

 or 83million years, approximately 2.7 times per orbit.[144] Although 



2


/




0




{\displaystyle 2\pi /\Omega _{0}}

 is 222million years, the value of 






{\displaystyle \Omega }

 at the point around which the Sun circulates is
(see Oort constants), corresponding to 235million years, and this is the time that the point takes to go once around the galaxy. Other stars with the same value of 



X
+
V

/

(
2
B
)


{\displaystyle X+V/(2B)}

 have to take the same amount of time to go around the galaxy as the sun and thus remain in the same general vicinity as the Sun.
The Sun's orbit around the Milky Way is perturbed due to the non-uniform mass distribution in Milky Way, such as that in and between the galactic spiral arms. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events.[145] It takes the Solar System about 225250million years to complete one orbit through the Milky Way (a galactic year),[146] so it is thought to have completed 2025 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251km/s (156mi/s).[147] At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel 1AU.[148]
The Milky Way is moving with respect to the cosmic microwave background radiation (CMB) in the direction of the constellation Hydra with a speed of 550km/s, and the Sun's resultant velocity with respect to the CMB is about 370km/s in the direction of Crater or Leo.[149]
The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of 7.58.5 kiloparsecs (2428kly) from the Galactic Center.[150][151]
[152][153][154][155]
The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga,[156] or multiple supernovae in subgroup B1 of the Pleiades moving group.[157] The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years.[158] The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The Apex of the Sun's Way, or the solar apex, is the direction that the Sun travels relative to other nearby stars. This motion is towards a point in the constellation Hercules, near the star Vega. Stars within 100 parsecs of the sun (326 light-years) have speeds relative to the sun which can be modeled approximately by a Maxwell-Boltzmann distribution (especially for the lower speeds) or a log normal distribution (especially for the higher speeds), but with more high-speed stars (greater than 300km/s) than predicted by either distribution. The mean velocity of these stars (not the mean speed) relative to the sun (or the mean velocity of the sun relative to them) is around 20km/s.[159]
Within 32.6ly of the Sun there are 315 known stars in 227 systems, as of 2000, including 163 single stars. It is estimated that a further 130 systems within this range have not yet been identified. Out to 81.5ly, there may be up to 7,500 stars, of which around 2,600 are known. The number of substellar objects in that volume are expected to be comparable to the number of stars.[160] Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.[161]
The Gaia Catalogue of Nearby Stars, all within 100 parsecs, contains 331,312 stars and is thought to include at least 92% of the stars of stellar spectral type M9 or "earlier" (i.e. hotter).[159]
The temperature of the photosphere is approximately 6,000K, whereas the temperature of the corona reaches 10000002000000K.[93] The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.[95]
It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating.[93] The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone.[93] These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat.[162] The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller eventsnanoflares.[163]
Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvn waves have been found to dissipate or refract before reaching the corona.[164] In addition, Alfvn waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.[93]
Theoretical models of the Sun's development suggest that 3.8 to 2.5billion years ago, during the Archean eon, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history and that the young Earth was somewhat warmer than it is today. One theory among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.[165]
However, examination of Archaean sediments appears inconsistent with the hypothesis of high greenhouse concentrations. Instead, the moderate temperature range may be explained by a lower surface albedo brought about by less continental area and the lack of biologically induced cloud condensation nuclei. This would have led to increased absorption of solar energy, thereby compensating for the lower solar output.[166]
The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon causes day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. The Sun has played an important part in many world religions, as described in a later section.
In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.[167]
One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras. He reasoned that it was not the chariot of Helios, but instead a giant flaming ball of metal even larger than the land of the Peloponnesus and that the Moon reflected the light of the Sun.[168] For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000km) or 804,000,000 stadia (148 to 153million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately 7.71million kilometers (0.0515AU).[169]
The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This view was developed in a more detailed mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.
Observations of sunspots were recorded during the Han Dynasty (206 BCAD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century.[170] The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.[171]
Arabic astronomical contributions include Al-Battani's discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing.[172] (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the Earth's orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.[173]
From an observation of a transit of Venus in 1032, the Persian astronomer and polymath Ibn Sina concluded that Venus is closer to Earth than the Sun.[174] In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.
In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors.[175] In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum.[176] The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat.[177] Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20million years, well short of the time span of at least 300million years suggested by some geological discoveries of that time.[177][178] In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.[179]
Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source.[180] However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his massenergy equivalence relation E = mc2.[181] In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass.[182] The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun.[183][184] In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.[185]
The first satellites designed for long term observation of the Sun from interplanetary space were NASA's Pioneers 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.[187][188]
In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion.[189] The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station.[94] Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona.[94] Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.[189]
In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle Challenger mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.[190]
Launched in 1991, Japan's Yohkoh (Sunbeam) satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.[191]
One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995.[94] Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009.[192] It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010.[193] Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch.[94] Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.[194]
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The Ulysses probe was launched in 1990 to study the Sun's polar regions. It first traveled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Once Ulysses was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.[195]
Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, Genesis, was designed to allow astronomers to directly measure the composition of solar material.[196]
The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.[197][198]
The Parker Solar Probe was launched in 2018 aboard a Delta IV Heavy rocket and will reach a perigee of 0.046AU in 2025, making it the closest-orbiting manmade satellite as the first spacecraft to fly low into the solar corona.[199]
The Indian Space Research Organisation has scheduled the launch of a 100kg satellite named Aditya for mid 2020. Its main instrument will be a coronagraph for studying the dynamics of the solar corona.[200]
The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes.[201][202] Looking directly at the Sun (sungazing) causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness.[203][204] UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun.[205] Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused;[206][207] conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.
Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.[208]
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.[209]
Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, not by the brightest object in the field. During partial eclipses, most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2mm to ~6mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer.[210] The hazard is insidious for inexperienced observers and for children because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.
During sunrise and sunset, sunlight is attenuated because of Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere,[211] and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.[212]
An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.[213]
Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other biological effects such as the production of vitamin D and sun tanning. It is also the main cause of skin cancer. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the Earth.[214]
The Sun has eight known planets. This includes four terrestrial planets (Mercury, Venus, Earth, and Mars), two gas giants (Jupiter and Saturn), and two ice giants (Uranus and Neptune). The Solar System also has at least five dwarf planets, an asteroid belt, numerous comets, and a large number of icy bodies which lie beyond the orbit of Neptune.
Solar deities play a major role in many world religions and mythologies.[215] Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a god. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichn Itz in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumnal equinoxes.
The ancient Sumerians believed that the Sun was Utu,[216][217] the god of justice and twin brother of Inanna, the Queen of Heaven,[216] who was identified as the planet Venus.[217] Later, Utu was identified with the East Semitic god Shamash.[216][217] Utu was regarded as a helper-deity, who aided those in distress,[216] and, in iconography, he is usually portrayed with a long beard and clutching a saw,[216] which represented his role as the dispenser of justice.[216]
From at least the Fourth Dynasty of Ancient Egypt, the Sun was worshipped as the god Ra, portrayed as a falcon-headed divinity surmounted by the solar disk, and surrounded by a serpent. In the New Empire period, the Sun became identified with the dung beetle, whose spherical ball of dung was identified with the Sun. In the form of the sun disc Aten, the Sun had a brief resurgence during the Amarna Period when it again became the preeminent, if not only, divinity for the Pharaoh Akhenaton.[218][219]
The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been an antecedent to Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers categorized it as one of the seven planets (Greek planetes, "wanderer"); the naming of the days of the weeks after the seven planets dates to the Roman era.[220][221][222]
In Proto-Indo-European religion, the Sun was personified as the goddess *Seh2ul.[223][224] Derivatives of this goddess in Indo-European languages include the Old Norse Sl, Sanskrit Surya, Gaulish Sulis, Lithuanian Saul, and Slavic Solntse.[224] In ancient Greek religion, the sun deity was the male god Helios,[225] who in later times was syncretized with Apollo.[226]
In the Bible, Malachi 4:2 mentions the "Sun of Righteousness" (sometimes translated as the "Sun of Justice"),[227] which some Christians have interpreted as a reference to the Messiah (Christ).[228] In ancient Roman culture, Sunday was the day of the sun god. It was adopted as the Sabbath day by Christians who did not have a Jewish background. The symbol of light was a pagan device adopted by Christians, and perhaps the most important one that did not come from Jewish traditions. In paganism, the Sun was a source of life, giving warmth and illumination to mankind. It was the center of a popular cult among Romans, who would stand at dawn to catch the first rays of sunshine as they prayed. The celebration of the winter solstice (which influenced Christmas) was part of the Roman cult of the unconquered Sun (Sol Invictus). Christian churches were built with an orientation so that the congregation faced toward the sunrise in the East.[229]
Tonatiuh, the Aztec god of the sun, was usually depicted holding arrows and a shield[230] and was closely associated with the practice of human sacrifice.[230] The sun goddess Amaterasu is the most important deity in the Shinto religion,[231][232] and she is believed to be the direct ancestor of all Japanese emperors.[231]
Solar System Local Interstellar Cloud Local Bubble Gould Belt Orion Arm Milky Way Milky Way subgroup Local Group  Local Sheet  Virgo Supercluster  Laniakea Supercluster Observable universe UniverseEach arrow () may be read as "within" or "part of".

This glossary of astronomy is a list of definitions of terms and concepts relevant to astronomy and cosmology, their sub-disciplines, and related fields. Astronomy is concerned with the study of celestial objects and phenomena that originate outside the atmosphere of Earth. The field of astronomy features an extensive vocabulary and a significant amount of jargon.
Also called visual brightness (V).Also called the argument of perifocus or argument of pericenter.Also called the north node.Also called a celestial body.Also spelled astronomical catalog.Also called a celestial object.Also called obliquity.Also called critical velocity or critical rotation.Also spelled circumstellar disk.Also called a compact object.Also called space dust.Also called the cosmic microwave background radiation (CMBR).Also called break-up velocity.Also called the south node.Also called the plane of the ecliptic or simply the ecliptic.Also elliptic orbit.Also called an exoplanet.Also called the Cusp of Aries.Also called background stars.Also called the galactic core or galactic center.Also called the galactic year or cosmic year.Also called a geosynchronous equatorial orbit (GEO).Also called the Hill radius.Also called Laplace's invariable plane or the Laplace plane.Also called a Keplerian orbit.Also called the EdgeworthKuiper belt.Also called a Lagrange point, libration point, or L-point.Also called the Lenakaeia Supercluster, Local Supercluster, or Local SCI.Also called the Northward equinox.Also called a shooting star or falling star.Also called the normalized polar moment of inertia.Also called the MK classification.Also called rise width.Also called a stellar association.Also called a moon.Also called the pikOort cloud.Also called orbital plot.Also simply called space.Also called the pericenter.Also called a reference plane.Also called a planetary object.Also sometimes called planetology.Also called a planemo or planetary body.Also called a gravitational primary, primary body, or central body.Also called direct motion.Also called a quasi-stellar radio sourceAlso called an interstellar planet, nomad planet, orphan planet, and starless planet.Also major semi-axis.Also called the Southward equinox.Also called positional astronomy.Also called standard acceleration due to gravity.Also spelled star catalog.Also called a stellar system.Also called the stellar envelope.Also called spectral classification.Also simply called a stellar model.Also called a substar.Also called tidal acceleration.Also called the Johnson system or JohnsonMorgan system.Also called the Local Supercluster (LSC or LC).An acronym of X-ray bright optically normal galaxy.
Babylonian astronomy was the study or recording of celestial objects during the early history of Mesopotamia.
Babylonian astronomy seemed to have focused on a select group of stars and constellations known as Ziqpu stars.[1] These constellations may have been collected from various earlier sources. The earliest catalogue, Three Stars Each, mentions stars of the Akkadian Empire, of Amurru, of Elam and others.[citation needed]
A numbering system based on sixty was used, a sexagesimal system. This system simplified the calculating and recording of unusually great and small numbers. The modern practices of dividing a circle into 360 degrees, of 60 minutes each, began with the Sumerians.[2]
During the 8th and 7th centuries BC, Babylonian astronomers developed a new empirical approach to astronomy. They began studying and recording their belief system and philosophies dealing with an ideal nature of the universe and began employing an internal logic within their predictive planetary systems. This was an important contribution to astronomy and the philosophy of science, and some modern scholars have thus referred to this novel approach as the first scientific revolution.[3] This approach to astronomy was adopted and further developed in Greek and Hellenistic astrology. Classical Greek and Latin sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were considered as priest-scribes specializing in astrology and other forms of divination.
Only fragments of Babylonian astronomy have survived, consisting largely of contemporary clay tablets containing astronomical diaries, ephemerides and procedure texts, hence current knowledge of Babylonian planetary theory is in a fragmentary state.[4] Nevertheless, the surviving fragments show that Babylonian astronomy was the first "successful attempt at giving a refined mathematical description of astronomical phenomena" and that "all subsequent varieties of scientific astronomy, in the Hellenistic world, in India, in Islam, and in the West  depend upon Babylonian astronomy in decisive and fundamental ways."[5]
The origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descendants in direct line from the work of the late Babylonian astronomers.[6]
"Old" Babylonian astronomy was practiced during and after the First Babylonian dynasty (ca. 1830 BCE) and before the Neo-Babylonian Empire (ca. 626 BCE).
The Babylonians were the first to recognize that astronomical phenomena are periodic and apply mathematics to their predictions.[citation needed] Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena were recorded in the series of cuneiform tablets known as the Enma Anu Enlilthe oldest significant astronomical text that we possess is Tablet 63 of the Enma Anu Enlil, the Venus tablet of Ammisaduqa, which lists the first and last visible risings of Venus over a period of about 21 years. It is the earliest evidence that planetary phenomena were recognized as periodic.[citation needed]
An object labelled the ivory prism was recovered from the ruins of Nineveh. First presumed to be describing rules to a game, its use was later deciphered to be a unit converter for calculating the movement of celestial bodies and constellations.[7]
Babylonian astronomers developed zodiacal signs. They are made up of the division of the sky into three sets of thirty degrees and the constellations that inhabit each sector.[8]
The MUL.APIN contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and settings of the planets, and lengths of daylight as measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.[9][10][11] There are dozens of cuneiform Mesopotamian texts with real observations of eclipses, mainly from Babylonia.
The Babylonians were the first civilization known to possess a functional theory of the planets.[11] The oldest surviving planetary astronomical text is the Babylonian Venus tablet of Ammisaduqa, a 7th-century BCE copy of a list of observations of the motions of the planet Venus that probably dates as early as the second millennium BCE. The Babylonian astrologers also laid the foundations of what would eventually become Western astrology.[12] The Enuma anu enlil, written during the Neo-Assyrian period in the 7th century BCE,[13] comprises a list of omens and their relationships with various celestial phenomena including the motions of the planets.[14]
In contrast to the world view presented in Mesopotamian and Assyro-Babylonian literature, particularly in Mesopotamian and Babylonian mythology, very little is known about the cosmology and world view of the ancient Babylonian astrologers and astronomers.[15] This is largely due to the current fragmentary state of Babylonian planetary theory,[4] and also due to Babylonian astronomy being independent from cosmology at the time.[16] Nevertheless, traces of cosmology can be found in Babylonian literature and mythology.
In Babylonian cosmology, the Earth and the heavens were depicted as a "spatial whole, even one of round shape" with references to "the circumference of heaven and earth" and "the totality of heaven and earth". Their worldview was not exactly geocentric either. The idea of geocentrism, where the center of the Earth is the exact center of the universe, did not yet exist in Babylonian cosmology, but was established later by the Greek philosopher Aristotle's On the Heavens. In contrast, Babylonian cosmology suggested that the cosmos revolved around circularly with the heavens and the earth being equal and joined as a whole.[17] The Babylonians and their predecessors, the Sumerians, also believed in a plurality of heavens and earths. This idea dates back to Sumerian incantations of the 2nd millennium BCE, which refers to there being seven heavens and seven earths, linked possibly chronologically to the creation by seven generations of gods.[18]
It was a common Mesopotamian belief that gods could and did indicate future events to mankind. This indication of future events were considered to be omens. The Mesopotamian belief in omens pertains to astronomy and its predecessor astrology because it was a common practice at the time to look to the sky for omens. The other way to receive omens at the time was to look at animal entrails. This method of recovering omens is classified as a producible omen, meaning it can be produced by humans, but sky omens are produced without human action and therefore seen as much more powerful. Both producible and unproducible omens however, were seen as messages from the gods. Just because gods sent the signs didn't mean that Mesopotamians believed their fate was sealed either, the belief during this time was that omens were avoidable. In mathematical terms, the Mesopotamians viewed omens as if x, then y, where x is the protasis and y is the apodosis.[19][pageneeded] The relationship Mesopotamians had with omens can be seen in the Omen Compendia, a Babylonian text composed starting from the beginning of the second millennium on-wards.[19] It is the primary source text that tells us that ancient Mesopotamians saw omens as preventable. The text also contains information on Sumerian rites to avert evil, or nam-bur-bi. A term later adopted by the Akkadians as namburbu, roughly, [the evil] loosening. The god Ea was the one believed to send the omens. Concerning the severity of omens, eclipses were seen as the most dangerous.[20]
The Enuma Anu Enlil is a series of cuneiform tablets that gives insight on different sky omens Babylonian astronomers observed.[21] Celestial bodies such as the Sun and Moon were given significant power as omens.  Reports from Nineveh and Babylon, circa 2500-670 B.C.E., show lunar omens observed by the Mesopotamians.  "When the moon disappears, evil will befall the land.  When the moon disappears out of its reckoning, an eclipse will take place".[22]
The astrolabes (not to be mistaken for the later astronomical measurement device of the same name) are one of the earliest documented cuneiform tablets that discuss astronomy and date back to the Old Babylonian Kingdom. They are a list of thirty-six stars connected with the months in a year,[8] generally considered to be written between 1800 and 1100 B.C.E..  No complete texts have been found, but there is a modern compilation by Pinches, assembled from texts housed in the British Museum that is considered excellent by other historians who specialize in Babylonian astronomy. Two other texts concerning the astrolabes that should be mentioned are the Brussels and Berlin compilations. They offer similar information to the Pinches anthology, but do contain some differing information from each other.[23]
The thirty-six stars that make up the astrolabes are believed to be derived from the astronomical traditions from three Mesopotamian city-states, Elam, Akkad, and Amurru. The stars followed and possibly charted by these city-states are identical stars to the ones in the astrolabes. Each region had a set of twelve stars it followed, which combined equals the thirty-six stars in the astrolabes. The twelve stars of each region also correspond to the months of the year. The two cuneiform texts that provide the information for this claim are the large star list K 250 and K 8067. Both of these tablets were translated and transcribed by Weidner. During the reign of Hammurabi these three separate traditions were combined. This combining also ushered in a more scientific approach to astronomy as connections to the original three traditions weakened. The increased use of science in astronomy is evidenced by the traditions from these three regions being arranged in accordance to the paths of the stars of Ea, Anu, and Enlil, an astronomical system contained and discussed in the Mul.apin.[23]
MUL.APIN is a collection of two cuneiform tablets (Tablet 1 and Tablet 2) that document aspects of Babylonian astronomy such as the movement of celestial bodies and records of solstices and eclipses.[7] Each tablet is also split into smaller sections called Lists. It was comprised in the general time frame of the astrolabes and Enuma Anu Enlil, evidenced by similar themes, mathematical principles, and occurrences.[24]
Tablet 1 houses information that closely parallels information contained in astrolabe B.  The similarities between Tablet 1 and astrolabe B show that the authors were inspired by the same source for at least some of the information.  There are six lists of stars on this tablet that relate to sixty constellations in charted paths of the three groups of Babylonian star paths, Ea, Anu, and Enlil. there are also additions to the paths of both Anu and Enlil that are not found in astrolabe B.[24]
The exploration of the Sun, Moon, and other celestial bodies affected the development of Mesopotamian culture. The study of the sky led to the development of a calendar and advanced mathematics in these societies. The Babylonians were not the first complex society to develop a calendar globally and nearby in North Africa, the Egyptians developed a calendar of their own. The Egyptian calendar was solar based, while the Babylonian calendar was lunar based. A potential blend between the two that has been noted by some historians is the adoption of a crude leap year by the Babylonians after the Egyptians developed one. The Babylonian leap year shares no similarities with the leap year practiced today. it involved the addition of a thirteenth month as a means to re-calibrate the calendar to better match the growing season.[25]
Babylonian priests were the ones responsible for developing new forms of mathematics and did so to better calculate the movements of celestial bodies. One such priest, Nabu-rimanni, is the first documented Babylonian astronomer. He was a priest for the moon god and is credited with writing lunar and eclipse computation tables as well as other elaborate mathematical calculations. The computation tables are organized in seventeen or eighteen tables that document the orbiting speeds of planets and the Moon. His work was later recounted by astronomers during the Seleucid dynasty.[25]
A team of scientists at the University of Tsukuba studied Assyrian cuneiform tablets, reporting unusual red skies which might be aurorae incidents, caused by geomagnetic storms between 680 and 650 BCE.[26]
Neo-Babylonian astronomy refers to the astronomy developed by Chaldean astronomers during the Neo-Babylonian, Achaemenid, Seleucid, and Parthian periods of Mesopotamian history. A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747734 BCE). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year Saros cycle of lunar eclipses, for example.[27] The Greco-Egyptian astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time.
The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (32360 BCE). In the 3rd century BCE, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting records.
Though there is a lack of surviving material on Babylonian planetary theory,[4] it appears most of the Chaldean astronomers were concerned mainly with ephemerides and not with theory. It had been thought that most of the predictive Babylonian planetary models that have survived were usually strictly empirical and arithmetical, and usually did not involve geometry, cosmology, or speculative philosophy like that of the later Hellenistic models,[28] though the Babylonian astronomers were concerned with the philosophy dealing with the ideal nature of the early universe.[3] Babylonian procedure texts describe, and ephemerides employ, arithmetical procedures to compute the time and place of significant astronomical events.[29] More recent analysis of previously unpublished cuneiform tablets in the British Museum, dated between 350 and 50 BCE, demonstrates that Babylonian astronomers sometimes used geometrical methods, prefiguring the methods of the Oxford Calculators, to describe the motion of Jupiter over time in an abstract mathematical space.[30][31]
In contrast to Greek astronomy which was dependent upon cosmology, Babylonian astronomy was independent from cosmology.[16] Whereas Greek astronomers expressed "prejudice in favor of circles or spheres rotating with uniform motion", such a preference did not exist for Babylonian astronomers, for whom uniform circular motion was never a requirement for planetary orbits.[32] There is no evidence that the celestial bodies moved in uniform circular motion, or along celestial spheres, in Babylonian astronomy.[33]
Contributions made by the Chaldean astronomers during this period include the discovery of eclipse cycles and saros cycles, and many accurate astronomical observations. For example, they observed that the Sun's motion along the ecliptic was not uniform, though they were unaware of why this was; it is today known that this is due to the Earth moving in an elliptic orbit around the Sun, with the Earth moving swifter when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.[34]
Chaldean astronomers known to have followed this model include Naburimannu (fl. 6th3rd century BCE), Kidinnu (d. 330 BCE), Berossus (3rd century BCE), and Sudines (fl. 240 BCE). They are known to have had a significant influence on the Greek astronomer Hipparchus and the Egyptian astronomer Ptolemy, as well as other Hellenistic astronomers.
The only surviving planetary model from among the Chaldean astronomers is that of the Hellenistic Seleucus of Seleucia (b. 190 BCE), who supported the Greek Aristarchus of Samos' heliocentric model.[35][36][37] Seleucus is known from the writings of Plutarch, Aetius, Strabo, and Muhammad ibn Zakariya al-Razi. The Greek geographer Strabo lists Seleucus as one of the four most influential astronomers, who came from Hellenistic Seleuceia on the Tigris, alongside Kidenas (Kidinnu), Naburianos (Naburimannu), and Sudines. Their works were originally written in the Akkadian language and later translated into Greek.[38] Seleucus, however, was unique among them in that he was the only one known to have supported the heliocentric theory of planetary motion proposed by Aristarchus,[39][40][41] where the Earth rotated around its own axis which in turn revolved around the Sun. According to Plutarch, Seleucus even proved the heliocentric system through reasoning, though it is not known what arguments he used.[42]
According to Lucio Russo, his arguments were probably related to the phenomenon of tides.[43] Seleucus correctly theorized that tides were caused by the Moon, although he believed that the interaction was mediated by the Earth's atmosphere. He noted that the tides varied in time and strength in different parts of the world. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun.[38]
According to Bartel Leendert van der Waerden, Seleucus may have proved the heliocentric theory by determining the constants of a geometric model for the heliocentric theory and by developing methods to compute planetary positions using this model. He may have used trigonometric methods that were available in his time, as he was a contemporary of Hipparchus.[44]
None of his original writings or Greek translations have survived, though a fragment of his work has survived only in Arabic translation, which was later referred to by the Persian philosopher Muhammad ibn Zakariya al-Razi (865-925).[45]
Many of the works of ancient Greek and Hellenistic writers (including mathematicians, astronomers, and geographers) have been preserved up to the present time, or some aspects of their work and thought are still known through later references. However, achievements in these fields by earlier ancient Near Eastern civilizations, notably those in Babylonia, were forgotten for a long time. Since the discovery of key archaeological sites in the 19th century, many cuneiform writings on clay tablets have been found, some of them related to astronomy. Most known astronomical tablets have been described by Abraham Sachs and later published by Otto Neugebauer in the Astronomical Cuneiform Texts (ACT).  Herodotus writes that the Greeks learned such aspects of astronomy as the gnomon and the idea of the day being split into two halves of twelve from the Babylonians.[23]  Other sources point to Greek pardegms, a stone with 365-366 holes carved into it to represent the days in a year, from the Babylonians as well.[7]
Since the rediscovery of the Babylonian civilization, it has been theorized that there was significant information exchange between classical and Hellenistic astronomy and Chaldean. The best documented borrowings are those of Hipparchus (2nd century BCE) and Claudius Ptolemy (2nd century CE).
Some scholars support that the Metonic cycle may have been learned by the Greeks from Babylonian scribes. Meton of Athens, a Greek astronomer of the 5th century BCE, developed a lunisolar calendar based on the fact that 19 solar years is about equal to 235 lunar months, a period relation that perhaps was also known to the Babylonians.
In the 4th century BCE, Eudoxus of Cnidus wrote a book on the fixed stars. His descriptions of many constellations, especially the twelve signs of the zodiac show similarities to Babylonian. The following century Aristarchus of Samos used an eclipse cycle called the Saros cycle to determine the year length. However, the position that there was an early information exchange between Greeks and Chaldeans are weak inferences; possibly, there had been a stronger information exchange between the two after Alexander the Great established his empire over Persia in the latter part of the 4th century BCE.
In 1900, Franz Xaver Kugler demonstrated that Ptolemy had stated in his Almagest IV.2 that Hipparchus improved the values for the Moon's periods known to him from "even more ancient astronomers" by comparing eclipse observations made earlier by "the Chaldeans", and by himself. However Kugler found that the periods that Ptolemy attributes to Hipparchus had already been used in Babylonian ephemerides, specifically the collection of texts nowadays called "System B" (sometimes attributed to Kidinnu). Apparently Hipparchus only confirmed the validity of the periods he learned from the Chaldeans by his newer observations.  Later Greek knowledge of this specific Babylonian theory is confirmed by 2nd-century papyrus, which contains 32 lines of a single column of calculations for the Moon using this same "System B", but written in Greek on papyrus rather than in cuneiform on clay tablets.[46]
It is clear that Hipparchus (and Ptolemy after him) had an essentially complete list of eclipse observations covering many centuries. Most likely these had been compiled from the "diary" tablets: these are clay tablets recording all relevant observations that the Chaldeans routinely made. Preserved examples date from 652 BCE to CE 130, but probably the records went back as far as the reign of the Babylonian king Nabonassar: Ptolemy starts his chronology with the first day in the Egyptian calendar of the first year of Nabonassar; i.e., 26 February 747 BCE.
This raw material by itself must have been tough to use, and no doubt the Chaldeans themselves compiled extracts of e.g., all observed eclipses (some tablets with a list of all eclipses in a period of time covering a saros have been found). This allowed them to recognise periodic recurrences of events. Among others they used in System B (cf. Almagest IV.2):
The Babylonians expressed all periods in synodic months, probably because they used a lunisolar calendar. Various relations with yearly phenomena led to different values for the length of the year.
Similarly various relations between the periods of the planets were known. The relations that Ptolemy attributes to Hipparchus in Almagest IX.3 had all already been used in predictions found on Babylonian clay tablets.
Other traces of Babylonian practice in Hipparchus' work are
All this knowledge was transferred to the Greeks probably shortly after the conquest by Alexander the Great (331 BCE). According to the late classical philosopher Simplicius (early 6th century), Alexander ordered the translation of the historical astronomical records under supervision of his chronicler Callisthenes of Olynthus, who sent it to his uncle Aristotle. It is worth mentioning here that although Simplicius is a very late source, his account may be reliable. He spent some time in exile at the Sassanid (Persian) court, and may have accessed sources otherwise lost in the West. It is striking that he mentions the title tresis (Greek: guard) which is an odd name for a historical work, but is in fact an adequate translation of the Babylonian title massartu meaning "guarding" but also "observing".  Anyway, Aristotle's pupil Callippus of Cyzicus introduced his 76-year cycle, which improved upon the 19-year Metonic cycle, about that time. He had the first year of his first cycle start at the summer solstice of 28 June 330 BCE (Julian proleptic date), but later he seems to have counted lunar months from the first month after Alexander's decisive battle at Gaugamela in fall 331 BCE.  So Callippus may have obtained his data from Babylonian sources and his calendar may have been anticipated by Kidinnu. Also it is known that the Babylonian priest known as Berossus wrote around 281 BCE a book in Greek on the (rather mythological) history of Babylonia, the Babyloniaca, for the new ruler Antiochus I; it is said that later he founded a school of astrology on the Greek island of Kos. Another candidate for teaching the Greeks about Babylonian astronomy/astrology was Sudines who was at the court of Attalus I Soter late in the 3rd century BC.[citation needed]
Historians have also found evidence that Athens during the late 5th century may have been aware of Babylonian astronomy. astronomers, or astronomical concepts and practices through the documentation by Xenophon of Socrates telling his students to study astronomy to the extent of being able to tell the time of night from the stars.  This skill is referenced in the poem of Aratos, which discusses telling the time of night from the zodiacal signs.[7]
In any case, the translation of the astronomical records required profound knowledge of the cuneiform script, the language, and the procedures, so it seems likely that it was done by some unidentified Chaldeans. Now, the Babylonians dated their observations in their lunisolar calendar, in which months and years have varying lengths (29 or 30 days; 12 or 13 months respectively). At the time they did not use a regular calendar (such as based on the Metonic cycle like they did later), but started a new month based on observations of the New Moon. This made it very tedious to compute the time interval between events.
What Hipparchus may have done is transform these records to the Egyptian calendar, which uses a fixed year of always 365 days (consisting of 12 months of 30 days and 5 extra days): this makes computing time intervals much easier. Ptolemy dated all observations in this calendar. He also writes that "All that he (=Hipparchus) did was to make a compilation of the planetary observations arranged in a more useful way" (Almagest IX.2).  Pliny states (Naturalis Historia II.IX(53)) on eclipse predictions: "After their time (=Thales) the courses of both stars (=Sun and Moon) for 600 years were prophesied by Hipparchus,..."  This seems to imply that Hipparchus predicted eclipses for a period of 600 years, but considering the enormous amount of computation required, this is very unlikely. Rather, Hipparchus would have made a list of all eclipses from Nabonasser's time to his own.

Cultural astronomy, sometimes called the study of Astronomy in Culture, has been described as investigating "the diversity of ways in which cultures, both ancient and modern, perceive celestial objects and integrate them into their view of the world."[1] As such, it encompassed the interdisciplinary fields studying the astronomies of current or ancient societies and cultures.[2] It developed from the two interdisciplinary fields of archaeoastronomy, the study of the use of astronomy and its role in ancient cultures and civilizations, and ethnoastronomy, "a closely allied research field which merges astronomy, textual scholarship, ethnology, and the interpretation of ancient iconography for the purpose of reconstructing lifeways, astronomical techniques, and rituals."[3] It is also related to historical astronomy (analyzing historical astronomical data), history of astronomy (understanding and study and evolution of the discipline of astronomy over the course of human knowledge) and history of astrology (investigating relationships between astrology and astronomy).

Observational astronomy is a division of astronomy that is concerned with recording data about the observable universe, in contrast with theoretical astronomy, which is mainly concerned with calculating the measurable implications of physical models. It is the practice and study of observing celestial objects with the use of telescopes and other astronomical instruments.
As a science, the study of astronomy is somewhat hindered in that direct experiments with the properties of the distant universe are not possible. However, this is partly compensated by the fact that astronomers have a vast number of visible examples of stellar phenomena that can be examined. This allows for observational data to be plotted on graphs, and general trends recorded. Nearby examples of specific phenomena, such as variable stars, can then be used to infer the behavior of more distant representatives. Those distant yardsticks can then be employed to measure other phenomena in that neighborhood, including the distance to a galaxy.
Galileo Galilei turned a telescope to the heavens and recorded what he saw. Since that time, observational astronomy has made steady advances with each improvement in telescope technology.
A traditional division of observational astronomy is based on the region of the electromagnetic spectrum observed:
In addition to using electromagnetic radiation, modern astrophysicists can also make observations using neutrinos, cosmic rays or gravitational waves. Observing a source using multiple methods is known as multi-messenger astronomy.
Optical and radio astronomy can be performed with ground-based observatories, because the atmosphere is relatively transparent at the wavelengths being detected. Observatories are usually located at high altitudes so as to minimise the absorption and distortion caused by the Earth's atmosphere. Some wavelengths of infrared light are heavily absorbed by water vapor, so many infrared observatories are located in dry places at high altitude, or in space.
The atmosphere is opaque at the wavelengths used by X-ray astronomy, gamma-ray astronomy, UV astronomy and (except for a few wavelength "windows") far infrared astronomy, so observations must be carried out mostly from balloons or space observatories. Powerful gamma rays can, however be detected by the large air showers they produce, and the study of cosmic rays is a rapidly expanding branch of astronomy.
For much of the history of observational astronomy, almost all observation was performed in the visual spectrum with optical telescopes. While the Earth's atmosphere is relatively transparent in this portion of the electromagnetic spectrum, most telescope work is still dependent on seeing conditions and air transparency, and is generally restricted to the night time. The seeing conditions depend on the turbulence and thermal variations in the air. Locations that are frequently cloudy or suffer from atmospheric turbulence limit the resolution of observations. Likewise the presence of the full Moon can brighten up the sky with scattered light, hindering observation of faint objects.
For observation purposes, the optimal location for an optical telescope is undoubtedly in outer space. There the telescope can make observations without being affected by the atmosphere. However, at present it remains costly to lift telescopes into orbit. Thus the next best locations are certain mountain peaks that have a high number of cloudless days and generally possess good atmospheric conditions (with good seeing conditions). The peaks of the islands of Mauna Kea, Hawaii and La Palma possess these properties, as to a lesser extent do inland sites such as Llano de Chajnantor, Paranal, Cerro Tololo and La Silla in Chile. These observatory locations have attracted an assemblage of powerful telescopes, totalling many billion US dollars of investment.
The darkness of the night sky is an important factor in optical astronomy. With the size of cities and human populated areas ever expanding, the amount of artificial light at night has also increased. These artificial lights produce a diffuse background illumination that makes observation of faint astronomical features very difficult without special filters. In a few locations such as the state of Arizona and in the United Kingdom, this has led to campaigns for the reduction of light pollution. The use of hoods around street lights not only improves the amount of light directed toward the ground, but also helps reduce the light directed toward the sky.
Atmospheric effects (astronomical seeing) can severely hinder the resolution of a telescope. Without some means of correcting for the blurring effect of the shifting atmosphere, telescopes larger than about 1520cm in aperture can not achieve their theoretical resolution at visible wavelengths. As a result, the primary benefit of using very large telescopes has been the improved light-gathering capability, allowing very faint magnitudes to be observed. However the resolution handicap has begun to be overcome by adaptive optics, speckle imaging and interferometric imaging, as well as the use of space telescopes.
Astronomers have a number of observational tools that they can use to make measurements of the heavens. For objects that are relatively close to the Sun and Earth, direct and very precise position measurements can be made against a more distant (and thereby nearly stationary) background. Early observations of this nature were used to develop very precise orbital models of the various planets, and to determine their respective masses and gravitational perturbations. Such measurements led to the discovery of the planets Uranus, Neptune, and (indirectly) Pluto. They also resulted in an erroneous assumption of a fictional planet Vulcan within the orbit of Mercury (but the explanation of the precession of Mercury's orbit by Einstein is considered one of the triumphs of his general relativity theory).
In addition to examination of the universe in the optical spectrum, astronomers have increasingly been able to acquire information in other portions of the electromagnetic spectrum. The earliest such non-optical measurements were made of the thermal properties of the Sun. Instruments employed during a solar eclipse could be used to measure the radiation from the corona.
With the discovery of radio waves, radio astronomy began to emerge as a new discipline in astronomy. The long wavelengths of radio waves required much larger collecting dishes in order to make images with good resolution, and later led to the development of the multi-dish interferometer for making high-resolution aperture synthesis radio images (or "radio maps"). The development of the microwave horn receiver led to the discovery of the microwave background radiation associated with the Big Bang.[4]
Radio astronomy has continued to expand its capabilities, even using radio astronomy satellites to produce interferometers with baselines much larger than the size of the Earth. However, the ever-expanding use of the radio spectrum for other uses is gradually drowning out the faint radio signals from the stars. For this reason, in the future radio astronomy might be performed from shielded locations, such as the far side of the Moon.
The last part of the twentieth century saw rapid technological advances in astronomical instrumentation. Optical telescopes were growing ever larger, and employing adaptive optics to partly negate atmospheric blurring. New telescopes were launched into space, and began observing the universe in the infrared, ultraviolet, x-ray, and gamma ray parts of the electromagnetic spectrum, as well as observing cosmic rays. Interferometer arrays produced the first extremely high-resolution images using aperture synthesis at radio, infrared and optical wavelengths. Orbiting instruments such as the Hubble Space Telescope produced rapid advances in astronomical knowledge, acting as the workhorse for visible-light observations of faint objects. New space instruments under development are expected to directly observe planets around other stars, perhaps even some Earth-like worlds.
In addition to telescopes, astronomers have begun using other instruments to make observations.
Neutrino astronomy is the branch of astronomy that observes astronomical objects with neutrino detectors in special observatories, usually huge underground tanks. Nuclear reactions in stars and supernova explosions produce very large numbers of neutrinos, a very few of which may be detected by a neutrino telescope. Neutrino astronomy is motivated by the possibility of observing processes that are inaccessible to optical telescopes, such as the Sun's core.
Gravitational wave detectors are being designed that may capture events such as collisions of massive objects such as neutron stars or black holes.[5]
Robotic spacecraft are also being increasingly used to make highly detailed observations of planets within the Solar System, so that the field of planetary science now has significant cross-over with the disciplines of geology and meteorology.
The key instrument of nearly all modern observational astronomy is the telescope. This serves the dual purposes of gathering more light so that very faint objects can be observed, and magnifying the image so that small and distant objects can be observed. Optical astronomy requires telescopes that use optical components of great precision. Typical requirements for grinding and polishing a curved mirror, for example, require the surface to be within a fraction of a wavelength of light of a particular conic shape. Many modern "telescopes" actually consist of arrays of telescopes working together to provide higher resolution through aperture synthesis.
Large telescopes are housed in domes, both to protect them from the weather and to stabilize the environmental conditions.  For example, if the temperature is different from one side of the telescope to the other, the shape of the structure changes, due to thermal expansion pushing optical elements out of position. This can affect the image.  For this reason, the domes are usually bright white (titanium dioxide) or unpainted metal.  Domes are often opened around sunset, long before observing can begin, so that air can circulate and bring the entire telescope to the same temperature as the surroundings.  To prevent wind-buffet or other vibrations affecting observations, it is standard practice to mount the telescope on a concrete pier whose foundations are entirely separate from those of the surrounding dome and building.
To do almost any scientific work requires that telescopes track objects as they wheel across the visible sky.  In other words, they must smoothly compensate for the rotation of the Earth.  Until the advent of computer controlled drive mechanisms, the standard solution was some form of equatorial mount, and for small telescopes this is still the norm.   However, this is a structurally poor design and becomes more and more cumbersome as the diameter and weight of the telescope increases.  The world's largest equatorial mounted telescope is the 200 inch (5.1 m) Hale Telescope, whereas recent 810 m telescopes use the structurally better altazimuth mount, and are actually physically smaller than the Hale, despite the larger mirrors.  As of 2006, there are design projects underway for gigantic alt-az telescopes: the Thirty Metre Telescope [1], and the 100 m diameter Overwhelmingly Large Telescope.[7]
Amateur astronomers use such instruments as the Newtonian reflector, the Refractor and the increasingly popular  Maksutov telescope.
The photograph has served a critical role in observational astronomy for over a century, but in the last 30 years it has been largely replaced for imaging applications by digital sensors such as CCDs and CMOS chips. Specialist areas of astronomy such as photometry and interferometry have utilised electronic detectors for a much longer period of time. Astrophotography uses specialised photographic film (or usually a glass plate coated with photographic emulsion), but there are a number of drawbacks, particularly a low quantum efficiency, of the order of 3%, whereas CCDs can be tuned for a QE >90% in a narrow band.  Almost all modern telescope instruments are electronic arrays, and older telescopes have been either been retrofitted with these instruments or closed down.  Glass plates are still used in some applications, such as surveying,[citation needed] because the resolution possible with a chemical film is much higher than any electronic detector yet constructed.
Prior to the invention of photography, all astronomy was done with the naked eye.  However, even before films became sensitive enough, scientific astronomy moved entirely to film, because of the overwhelming advantages:
The blink comparator is an instrument that is used to compare two nearly identical photographs made of the same section of sky at different points in time. The comparator alternates illumination of the two plates, and any changes are revealed by blinking points or streaks. This instrument has been used to find asteroids, comets, and variable stars.
The position or cross-wire micrometer is an implement that has been used to measure double stars. This consists of a pair of fine, movable lines that can be moved together or apart. The telescope lens is lined up on the pair and oriented using position wires that lie at right angles to the star separation. The movable wires are then adjusted to match the two star positions. The separation of the stars is then read off the instrument, and their true separation determined based on the magnification of the instrument.
A vital instrument of observational astronomy is the spectrograph. The absorption of specific wavelengths of light by elements allows specific properties of distant bodies to be observed. This capability has resulted in the discovery of the element of helium in the Sun's emission spectrum, and has allowed astronomers to determine a great deal of information concerning distant stars, galaxies, and other celestial bodies. Doppler shift (particularly "redshift") of spectra can also be used to determine the radial motion or distance with respect to the Earth.
Early spectrographs employed banks of prisms that split light into a broad spectrum. Later the grating spectrograph was developed, which reduced the amount of light loss compared to prisms and provided higher spectral resolution. The spectrum can be photographed in a long exposure, allowing the spectrum of faint objects (such as distant galaxies) to be measured.
Stellar photometry came into use in 1861 as a means of measuring stellar colors. This technique measured the magnitude of a star at specific frequency ranges, allowing a determination of the overall color, and therefore temperature of a star. By 1951 an internationally standardized system of UBV-magnitudes (Ultraviolet-Blue-Visual) was adopted.
Photoelectric photometry using the CCD is now frequently used to make observations through a telescope. These sensitive instruments can record the image nearly down to the level of individual photons, and can be designed to view in parts of the spectrum that are invisible to the eye. The ability to record the arrival of small numbers of photons over a period of time can allow a degree of computer correction for atmospheric effects, sharpening up the image. Multiple digital images can also be combined to further enhance the image. When combined with the adaptive optics technology, image quality can approach the theoretical resolution capability of the telescope.
Filters are used to view an object at particular frequencies or frequency ranges. Multilayer film filters can provide very precise control of the frequencies transmitted and blocked, so that, for example, objects can be viewed at a particular frequency emitted only by excited hydrogen atoms. Filters can also be used to partially compensate for the effects of light pollution by blocking out unwanted light. Polarization filters can also be used to determine if a source is emitting polarized light, and the orientation of the polarization.
Astronomers observe a wide range of astronomical sources, including high-redshift galaxies, AGNs, the afterglow from the Big Bang and many different types of stars and protostars.
A variety of data can be observed for each object. The position coordinates locate the object on the sky using the techniques of spherical astronomy, and the magnitude determines its brightness as seen from the Earth. The relative brightness in different parts of the spectrum yields information about the temperature and physics of the object. Photographs of the spectra allow the chemistry of the object to be examined.
Parallax shifts of a star against the background can be used to determine the distance, out to a limit imposed by the resolution of the instrument. The radial velocity of the star and changes in its position over time (proper motion) can be used to measure its velocity relative to the Sun. Variations in the brightness of the star give evidence of instabilities in the star's atmosphere, or else the presence of an occulting companion. The orbits of binary stars can be used to measure the relative masses of each companion, or the total mass of the system. Spectroscopic binaries can be found by observing doppler shifts in the spectrum of the star and its close companion.
Stars of identical masses that formed at the same time and under similar conditions typically have nearly identical observed properties. Observing a mass of closely associated stars, such as in a globular cluster, allows data to be assembled about the distribution of stellar types. These tables can then be used to infer the age of the association.
For distant galaxies and AGNs observations are made of the overall shape and properties of the galaxy, as well as the groupings where they are found. Observations of certain types of variable stars and supernovae of known luminosity, called standard candles, in other galaxies allows the inference of the distance to the host galaxy. The expansion of space causes the spectra of these galaxies to be shifted, depending on the distance, and modified by the Doppler effect of the galaxy's radial velocity. Both the size of the galaxy and its redshift can be used to infer something about the distance of the galaxy. Observations of large numbers of galaxies are referred to as redshift surveys, and are used to model the evolution of galaxy forms.

In observational astronomy, an asterism is a pattern or group of stars that can be seen in the night sky.  Asterisms range from simple shapes of just a few stars to more complex collections of many stars covering large portions of the sky.  The stars themselves may be bright naked-eye objects or fainter, even telescopic, but they are generally all of a similar brightness to each other.  The larger brighter asterisms are useful for people who are familiarizing themselves with the night sky. For example, the asterism known as the Big Dipper comprises the seven brightest stars in the constellation Ursa Major. Another is the asterism of the Southern Cross, within the constellation of Crux.
The stars within an asterism may be physically associated. For example, the stars of Collinder 399, which resembles a coathanger, are members of an open cluster, the stars of Orion's Belt are all members of the Orion OB1 association, and five of the seven stars of The Plough are members of the Ursa Major Moving Group. In other cases, the stars are unrelated, such as in the Summer Triangle.
The 88 constellations into which the sky is divided are based on asterisms considered to represent an object, person, or animal, often mythological. However, they are formally defined regions of sky, and contain all the celestial objects within their boundaries. Asterisms do not have officially determined boundaries and are a more general concept which may refer to any identified pattern of stars.[1][2]
In many early civilizations, it was already common to associate groups of stars in connect-the-dots stick-figure patterns; some of the earliest records are those of ancient India in the Vedanga Jyotisha and the Babylonians.[citation needed] This process was essentially arbitrary, and different cultures have identified different constellations, although a few of the more obvious patterns tend to appear in the constellations of multiple cultures, such as those of Orion and Scorpius. As anyone could arrange and name a grouping of stars there was no distinct difference between a constellation and an asterism. e.g. Pliny the Elder (2379 AD) in his book Naturalis Historia refers and mentions 72 asterisms.[3]
A general list containing 48 constellations likely began to develop with the astronomer Hipparchus  (c. 190  c. 120 BC ), and was mostly accepted as standard in Europe for 1,800 years. As constellations were considered to be composed only of the stars that constituted the figure, it was always possible to use any leftover stars to create and squeeze in a new grouping among the established constellations.[citation needed]
Furthermore, exploration by Europeans to other parts of the globe exposed them to stars unknown to them. Two astronomers particularly known for greatly expanding the number of southern constellations were Johann Bayer (15721625) and Nicolas Louis de Lacaille (17131762). Bayer had listed twelve figures made out of stars that were too far south for Ptolemy to have seen; Lacaille created 14 new groups, mostly for the area surrounding South Celestial Pole. Many of these proposed constellations have been formally accepted, but the rest have historically remained as asterisms.[citation needed]
In 1928, the International Astronomical Union (IAU) precisely divided the sky into 88 official constellations following geometric boundaries encompassing all of the stars within them. Any additional new selected groupings of stars or former constellations are often considered as asterisms. However, depending on the particular literature source, any technical distinctions between the terms 'constellation' and 'asterism' often remain somewhat ambiguous.[citation needed]
Component stars of some asterisms are bright and mark out simple geometric shapes.
Some asterisms may also be part of a constellation referring to the traditional figuring of the whole outline, for example Orion's Belt, and the Y in Aquarius (historically called "the Urn").[9]
Other asterisms are also composed of stars from one constellation, but do not refer to the traditional figures. 
Other asterisms that are formed from stars in more than one constellation.
Asterisms range from the large and obvious to the small, and even telescopic.

Those who cannot remember the past are condemned to repeat it.[1]
George Santayana
History (from Greek , historia, meaning "inquiry; knowledge acquired by investigation")[2] is the study of the past.[3][4] Events before the invention of writing systems are considered prehistory. "History" is an umbrella term comprising past events as well as the memory, discovery, collection, organization, presentation, and interpretation of these events. Historians seek knowledge of the past using historical sources such as written documents, oral accounts, art and material artifacts, and ecological markers.[5]
History also includes the academic discipline which uses narrative to describe, examine, question, and analyze past events, and investigate their patterns of cause and effect.[6][7] Historians  often debate which narrative best explains an event, as well as the significance of different causes and effects. Historians also debate the nature of history as an end in itself, as well as its usefulness to give perspective on the problems of the present.[6][8][9][10]
Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends.[11][12] History differs from myth in that it is supported by evidence. However, ancient cultural influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematic elements of historical investigation. History is often taught as part of primary and secondary education, and the academic study of history is a major discipline in university studies.
Herodotus, a 5th-century BC Greek historian, is often considered the "father of history" in the Western tradition,[13] although he has also been criticized as the "father of lies".[14][15] Along with his contemporary Thucydides, he helped form the foundations for the modern study of past events and societies. Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals, was reputed to date from as early as 722BC, although only 2nd-centuryBC texts have survived.
The word history comes from the Ancient Greek [16] (histora), meaning "inquiry", "knowledge from inquiry", or "judge". It was in that sense that Aristotle used the word in his History of Animals.[17] The ancestor word  is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boiotic inscriptions (in a legal sense, either "judge" or "witness", or similar). The Greek word was borrowed into Classical Latin as historia, meaning "investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative". History was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as str ("history, narrative, story"), but this word fell out of use in the late Old English period.[18] Meanwhile, as Latin became Old French (and Anglo-Norman), historia developed into forms such as istorie, estoire, and historie, with new developments in the meaning: "account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c.1240), body of knowledge relative to human evolution, science (c.1265), narrative of real or imaginary events, story (c.1462)".[18]
It was from Anglo-Norman that history was borrowed into Middle English, and this time the loan stuck. It appears in the 13th-century Ancrene Wisse, but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's Confessio Amantis of the 1390s (VI.1383): "I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire". In Middle English, the meaning of history was "story" in general. The restriction to the meaning "the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs" arose in the mid-15th century.[18] With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about natural history. For him, historia was "the knowledge of objects determined by space and time", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).[19]
In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese ( vs. ) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both "history" and "story". Historian in the sense of a "researcher of history" is attested from 1531. In all European languages, the substantive history is still used to mean both "what happened with men", and "the scholarly study of the happened", the latter sense sometimes distinguished with a capital letter, or the word historiography.[17] The adjective historical is attested from 1661, and historic from 1669.[20]
Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, "All history is contemporary history". History is facilitated by the formation of a "true discourse of past" through the production of narrative and analysis of past events relating to the human race.[21] The modern discipline of history is dedicated to the institutional production of this discourse.
All events that are remembered and preserved in some authentic form constitute the historical record.[22] The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the "true past"). Part of the historian's role is to skillfully and objectively utilize the vast amount of sources from the past, most often found in the archives. The process of creating a narrative inevitably generates a silence as historians remember or emphasize different events of the past.[23][clarification needed]
The study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences.[24] It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification.[25] In the 20th century, French historian Fernand Braudel revolutionized the study of history, by using such outside disciplines as economics, anthropology, and geography in the study of global history.
Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three.[26] But writing is the marker that separates history from what comes before.
Archaeology is especially helpful in unearthing buried sites and objects, which contribute to the study of history. Archaeological finds rarely stand alone, with narrative sources complementing its discoveries. Archaeology's methodologies and approaches are independent from the field of history. "Historical archaeology" is a specific branch of archaeology which often contrasts its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, USA, has sought to understand the contradiction between textual documents idealizing "liberty" and the material record, demonstrating the possession of slaves and the inequalities of wealth made apparent by the study of the total historical environment.
There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant intersections are often present. It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.[27]
The history of the world is the memory of the past experience of Homo sapiens sapiens around the world, as that experience has been preserved, largely in written records. By "prehistory", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world.[28] In 1961, British historian E. H. Carr wrote:
The line of demarcation between prehistoric and historical times is crossed when people cease to live only in the present, and become consciously interested both in their past and in their future. History begins with the handing down of tradition; and tradition means the carrying of the habits and lessons of the past into the future. Records of the past begin to be kept for the benefit of future generations.[29]This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Mori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.
Historiography has a number of related meanings.[30] Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative towards long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, "medieval historiography during the 1960s" means "Works of medieval history written during the 1960s").[30] Thirdly, it may refer to why history is produced: the philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.[31][32]
The following questions are used by historians in modern work.
The first four are known as historical criticism; the fifth, textual criticism; and, together, external criticism. The sixth and final inquiry about a source is called internal criticism.
The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.
Herodotus of Halicarnassus (484 BCc.425 BC)[33] has generally been acclaimed as the "father of history". However, his contemporary Thucydides (c.460 BCc.400 BC) is credited with having first approached history with a well-developed historical method in his work the History of the Peloponnesian War. Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention (though Herodotus was not wholly committed to this idea himself).[33] In his historical method, Thucydides emphasized chronology, a nominally neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.[34]
There were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (14590 BC), author of the Records of the Grand Historian (Shiji). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his Shiji as the official format for historical texts, as well as for biographical literature.[citation needed]
Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.[27]
In the preface to his book, the Muqaddimah (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized "idle superstition and uncritical acceptance of historical data." As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science".[35] His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history,[36] and he is thus considered to be the "father of historiography"[37][38] or the "father of the philosophy of history".[39]
In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods:

From the successive strata of our historical deposits, they [Historians] diligently gather all the highly colored fragments, pounce upon everything that is curious and sparkling and chuckle like children over their glittering acquisitions; meanwhile the rich veins of wisdom that ramify amidst this worthless debris, lie utterly neglected. Cumbrous volumes of rubbish are greedily accumulated, while those masses of rich ore, that should have been dug out, and from which golden truths might have been smelted, are left untaught and unsought[40]By the "rich ore" Spencer meant scientific theory of history. Meanwhile, Henry Thomas Buckle expressed a dream of history becoming one day science:

In regard to nature, events apparently the most irregular and capricious have been explained and have been shown to be in accordance with certain fixed and universal laws. This have been done because men of ability and, above all, men of patient, untiring thought have studied events with the view of discovering their regularity, and if human events were subject to a similar treatment, we have every right to expect similar results[41]Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to what really happened and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them.[42]

As Historians like Ranke and many who followed him have pursued it, no, history is not a science. Thus if Historians tell us that, given the manner in which he practices his craft, it cannot be considered a science, we must take him at his word. If he is not doing science, then, whatever else he is doing, he is not doing science. The traditional Historian is thus no scientist and history, as conventionally practiced, is not a science.[43]In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multi-disciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archaeology while Wehler, Bloch, Fischer, Stone, Febvre and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from the pen of a professional Historian.[44] Whatever other theories of history we have, they were written by experts from other fields (for example, Marxian theory of history). More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.
In sincere opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. histoire des mentalits). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socio-economic groups. Another genre of social history to emerge in the post-WWII era was Alltagsgeschichte (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.
Marxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as Franois Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book In Defence of History, Richard J. Evans defended the worth of history. Another defence of history from post-modernist criticism was the Australian historian Keith Windschuttle's 1994 book, The Killing of History.
Today, most historians begin their research process in the archives, on either a physical or digital platform. They often propose an argument and use their research to support it. John H. Arnold proposed that history is an argument, which creates the possibility of creating change.[5] Digital information companies, such as Google, have sparked controversy over the role of internet censorship in information access.[45]
The Marxist theory of historical materialism theorises that society is fundamentally determined by the material conditions at any given time in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families.[46] Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.[47] Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.[48]
Many historians believe that theproduction of history is embedded with bias because events and known facts in history can be interpreted in a variety of ways. Constantin Fasolt suggested that history is linked to politics by the practice of silence itself.[49] He also said:  A second common view of the link between history and politics rests on the elementary observation that historians are often influenced by politics.[49] According to Michel-Rolph Trouillot, the historical process is rooted in the archives, therefore silences, or parts of history that are forgotten, may bean intentional part of a narrative strategy that dictates how areas of history are remembered.[23] Historical omissions can occur in many ways and can have a profound effect on historical records. Information can also purposely be excluded or left out accidentally. Historians have coined multiple terms that describe the act of omitting historical information, including: silencing,[23] selective memory,[50] and erasures.[51]Gerda Lerner, a twentieth century historian who focused much of her work on historical omissions involving women and their accomplishments, explained the negative impact that these omissions had on minority groups.[50]
Environmental historian William Cronon proposed three ways to combat bias and ensure authentic and accurate narratives: narratives must not contradict known fact, they must make ecological sense (specifically for environmental history), and published work must be reviewed by scholarly community and other historians to ensure accountability.[51]
These are approaches to history; not listed are histories of other fields, such as history of science, history of mathematics and history of philosophy.
Historical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow "organising ideas and classificatory generalisations" to be used by historians.[52] The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.[53]
The field of history generally leaves prehistory to archaeologists, who have entirely different sets of tools and theories. The usual method for periodisation of the distant prehistoric past, in archaeology is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age and Iron Age and their sub-divisions also based on different styles of material remains. Here prehistory is divided into a series of "chapters" so that periods in history could unfold not only in a relative chronology but also narrative chronology.[54] This narrative content could be in the form of functional-economic interpretation. There are periodisations, however, that do not have this narrative aspect, relying largely on relative chronology, and that are thus devoid of any specific meaning
Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighbouring cultures with writing have left some history of cultures without it, which may be used. Periodisation, however, is not viewed as a perfect framework, with one account explaining that "cultural changes do not conveniently start and stop (combinedly) at periodisation boundaries" and that different trajectories of change are also needed to be studied in their own right before they get intertwined with cultural phenomena.[55]
Particular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book Histoire de France (1833), "without geographical basis, the people, the makers of history, seem to be walking on air."[56] Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Allen Semple, cited as a crucial influence on the course of history and racial temperament.[57]
Military history concerns warfare, strategies, battles, weapons, and the psychology of combat.[58] The "new military history" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.[59]
The history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include Church History, The Catholic Historical Review, and History of Religions. Topics range widely from political and cultural and artistic dimensions, to theology and liturgy.[60] This subject studies religions from all regions and areas of the world where humans have lived.[61]
Social history, sometimes called the new social history, is the field that includes history of ordinary people and their strategies and institutions for coping with life.[62] In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%.[63] In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%)  identified themselves with social history while political history came next with 1425 (25%).[64]
The "old" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible."[65] While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in".[66]
The chief subfields of social history include:
Cultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic.
Cultural history includes the study of art in society as well is the study of images and human visual production (iconography).[67]
Diplomatic history  focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars.[68] More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of political history is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, "diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies."[69] She adds that after 1945, the trend reversed, allowing social history to replace it.
Although economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments.[70] Business history  deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history. Business history is most often taught in business schools.[71]
Environmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.[72] It is an offshoot of the environmental movement, which was kickstarted by Rachel Carson's Silent Spring in the 1960s.
World history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States,[73] Japan[74] and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.
It has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.
The World History Association publishes the Journal of World History every quarter since 1990.[75] The H-World discussion list[76] serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.
A people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other type of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.[77]
Intellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.[78][79]
Gender history is a subfield of History and Gender studies, which looks at the past from the perspective of gender. The outgrowth of gender history from women's history stemmed from many non-feminist historians dismissing the importance of women in history. According to Joan W. Scott, Gender is a constitutive element of social relationships based on perceived differences between the sexes, and gender is a primary way of signifying relations of power,[80] meaning that gender historians study the social effects of perceived differences between the sexes and how all genders utilize allotted power in societal and political structures. Despite being a relatively new field, gender history has had a significant effect on the general study of history. Gender history traditionally differs from women's history in its inclusion of all aspects of gender such as masculinity and femininity, and today's gender history extends to include people who identify outside of that binary. 
LGBT history deals with the first recorded instances of same-sex love and sexuality of ancient civilizations, and involves the history of lesbian, gay, bisexual and transgender (LGBT) peoples and cultures around the world.[81]
Public history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.[82]
Professional and amateur historians discover, collect, organize, and present information about past events. They discover this information through archaeological evidence, written primary sources, verbal stories or oral histories, and other archival material. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.
Since the 20th century, Western historians have disavowed the aspiration to provide the "judgement of history".[83] The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final.[84] A related issue to that of the judgement of history is that of collective memory.
Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions.
It is closely related to deceptive historical revisionism. Works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.
A major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defence of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.[85][86]
In the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.[87]
From the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.[88]
At the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.
The teaching of history in French schools was influenced by the Nouvelle histoire as disseminated after the 1960s by Cahiers pdagogiques and Enseignement and other journals for teachers. Also influential was the Institut national de recherche et de documentation pdagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis Franois, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote "active methods" which would give pupils "the immense happiness of discovery." Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.[89]
In several countries history textbooks are tools to foster nationalism and patriotism, and give students the official narrative about national enemies.[90]
In many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favourable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained.[91] It was standard policy in communist countries to present only a rigid Marxist historiography.[92][93]
In the United States, textbooks published by the same company often differ in content from state to state.[94] An example of content that is represented different in different regions of the country is the history of the Southern states, where slavery and the American Civil War are treated as controversial topics. McGraw-Hill Education for example, was criticised for describing Africans brought to American plantations as "workers" instead of slaves in a textbook.[95]
Academic historians have often fought against the politicization of the textbooks, sometimes with success.[96][97]
In 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an "almost pacifistic and deliberately unpatriotic undertone" and reflects "principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace."  The result is that "German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness."[98]



HIStory: Past, Present and Future, Book I is the ninth studio album by American singer Michael Jackson, released on June 20, 1995. It was the fifth Jackson album released through Epic Records, and the first on his label MJJ Productions. It comprises two discs: HIStory Begins, a greatest hits compilation, and HIStory Continues, comprising new material written and produced by Jackson and collaborators. The album includes appearances by Janet Jackson, Shaquille O'Neal, Slash, and the Notorious B.I.G. The genres span R&B, pop, hip hop, elements of hard rock and funk rock. The themes include environmental awareness, isolation, greed, suicide, injustice, and Jackson's conflicts and common-ground with the media.
Starting in the late 1980s, Jackson and the tabloid press had a difficult relationship. In 1993, the relationship between Jackson and the press collapsed when he was accused of child sexual abuse. Although he was not charged, Jackson was subject to intense media scrutiny while the criminal investigation took place. Several of the album's 15 new songs pertain to the child sexual abuse allegations made against him in 1993 and Jackson's perceived mistreatment by the media, mainly the tabloids. Because of this, HIStory has been described as Jackson's most "personal" album.
HIStory debuted at number one on the Billboard 200, among nineteen other countries. Six singles were released, including the protest songs "Earth Song" and "They Don't Care About Us". "Scream", a duet between Jackson and his sister Janet, became the first song to debut in the top five on the Billboard Hot 100, reaching number five. "You Are Not Alone" was the first song in history to debut at number one on the Billboard Hot 100; it was Jackson's final number-one single on that chart. Though the album received generally positive reviews, the lyrics of "They Don't Care About Us" drew accusations of antisemitism, to which Jackson responded that the lines had been misinterpreted and replaced them on later pressings.
Jackson embarked on the HIStory World Tour, which grossed $165 million (equivalent to $268 million in 2019), making it the highest-grossing solo concert tour of the 1990s. It was Jackson's third and final concert tour as a solo artist. The album has sold over 20 million copies worldwide, making it one of the best-selling albums of all time, and the best-selling multi-disc album. In August 2018, it was certified 8 platinum by the Recording Industry Association of America (RIAA). It was nominated for five Grammy Awards at the 1996 Grammy Awards, including Album of the Year, winning Best Music Video Short Form for "Scream". Jackson won an American Music Award for Favorite Pop/Rock Male Artist at the 1996 American Music Awards.
Starting in the late 1980s, Jackson and the tabloid press had a difficult relationship. In 1986, tabloids claimed that Jackson slept in a hyperbaric oxygen chamber and had offered to buy the bones of Joseph Merrick (the "Elephant Man"), both of which Jackson denied.[2][3] These stories inspired the derogatory nickname "Wacko Jacko", which Jackson despised. He stopped leaking untruths to the press,[4] and the media began creating their own stories.[4] In 1989, Jackson released "Leave Me Alone", a song about the victimization he felt by the press.[5]
In 1993, the relationship between Jackson and the press collapsed when he was accused of child sexual abuse. Although he was not charged, Jackson was subject to intense media scrutiny while the criminal investigation took place. Complaints[whose?] about the coverage and media included misleading and sensational headlines;[6] paying for stories of Jackson's alleged criminal activity[7] and confidential material from the police investigation;[8] using unflattering pictures of Jackson;[9] and using headlines that strongly implied Jackson's guilt.[9] In 1994, Jackson said of the media coverage: "I am particularly upset by the handling of the matter by the incredible, terrible mass media. At every opportunity, the media has dissected and manipulated these allegations to reach their own conclusions."[10]
Jackson began taking painkillers, Valium, Xanax and Ativan to deal with the stress of the allegations.[11] A few months after the allegations became news, Jackson stopped eating.[12] Soon after, Jackson's health deteriorated to the extent that he canceled the remainder of his tour and went into rehabilitation.[13][14] Jackson booked the whole fourth floor of a clinic and was put on Valium IV to wean him from painkillers.[13][14] The media showed Jackson little sympathy. In 1993, the Daily Mirror held a "Spot the Jacko" contest, offering readers a trip to Disney World if they could correctly predict where Jackson would appear next.[13] The same year, a Daily Express headline read "Drug Treatment Star Faces Life on the Run", while a News of the World headline accused Jackson of being a fugitive; these tabloids also falsely alleged that Jackson had traveled to Europe to have cosmetic surgery that would make him unrecognizable on his return.[13] In early November 1993, talk show host Geraldo Rivera set up a mock trial with a jury of audience members, though Jackson had not been charged with a crime.[15]
HIStory was Jackson's first studio album since his 1991 album Dangerous nearly four years prior, and his first new material to be released since being accused of child sexual abuse in 1993.[16]  The album is a two-disc album: Disc one (HIStory Begins) contains previously released material from Jackson's four previous post-Motown studio albums, Off the Wall (1979), Thriller (1982), Bad (1987) and Dangerous (1991), and the second disc (HIStory Continues) comprises new material recorded from September 1994 to March 1995.[17] Jackson co-wrote and co-produced a majority of the new songs; other writers include Jimmy Jam and Terry Lewis, Dallas Austin, the Notorious B.I.G., Bruce Swedien, R. Kelly and Ren Moore, and other producers include David Foster and Bill Bottrell.[17]
Similarly to Jackson's previous studio albums Thriller and Bad, HIStory contains lyrics that deal with paranoia. The majority of the new songs were written by Jackson. Several of the album's 15 new songs pertain to the child sexual abuse allegations made against him in 1993[18] and Jackson's perceived mistreatment by the media, mainly the tabloids.[19] Because of this, the album has been described as being Jackson's most "personal".[20] Two of the album's new tracks are covers.[18] The genres of the songs on the album span R&B, pop, hip hop, elements of hard rock ("D.S.") and funk rock ("Scream"), and ballads.[18][20][21][22] The lyrics pertain to isolation, greed, environmental concerns, injustice. "Scream" is a duet with Jackson's sister Janet; with "spitting"[18] lyrics about injustice.[20]
The lyrics for the R&B ballad "You Are Not Alone", written by R. Kelly, pertain to isolation.[20] Two Belgian songwriters, brothers Eddy and Danny Van Passel, claimed to have written the melody in 1993. In September 2007, a Belgian judge ruled the song had been plagiarized from the Van Passel brothers, and it was banned from radio play in Belgium.[23][24] "D.S.", a hard rock song, has lyrics about a "cold man" named "Dom S. Sheldon". Critics interpreted it as an attack on Thomas Sneddon, who had led the prosecution in Jackson's trial.[18][21][25][26]
"Money" was interpreted as being directed at Evan Chandler, the father of the boy who accused Jackson of child sexual abuse.[18] The lyrics of "Childhood" pertain to Jackson's childhood.[27] Similar to "Scream", the lyrics to "They Don't Care About Us" pertain to injustice, as well as racism. In "This Time Around", Jackson asserts himself as having been "falsely accused".[18] The song includes a guest rap by the Notorious B.I.G. (aka Biggie Smalls).[28] "Earth Song" was described as a "slow blues-operatic",[20] and its lyrics pertain to environmental concerns. On HIStory, Jackson covered Charlie Chaplin's "Smile" and the Beatles' "Come Together".[20]
"2 Bad" was influenced by hip-hop, with a sample of RunD.M.C.'s King of Rock and another guest rap verse by Shaquille O'Neal. The similarity in lyrics and name have led to some seeing it as a spiritual successor to Jackson's 1987 track, "Bad".[29] "Stranger in Moscow" is a pop ballad that is interspersed with sounds of rain,[18] in which Jackson references a "swift and sudden fall from grace".[20] "Tabloid Junkie" is a hard funk song[30] with lyrics instructing listeners to not believe everything they read in the media and tabloids.[20][21] The album's title track, "HIStory" contained multiple samples, including Martin Luther King Jr.'s "I Have a Dream" speech.[31] "HIStory" was not released as a single from HIStory, but its remix was from Blood on the Dance Floor: HIStory in the Mix in 1997.
As an introduction for "Little Susie", Jackson used his own variation of Pie Jesu from Maurice Durufl's Requiem. Some[who?] speculate, the inspiration behind the song likely came from an artist called Gottfried Helnwein. An urban legend states that Little Susie was written about a girl called Susie Condry who was murdered in 1972. However no evidence of this event can be found. Jackson admired Helnweint's work and had purchased some of his paintings. One of them, "Beautiful Victim", inspired the song. Helnwein later painted a portrait of Michael.[32] There appears to be a similarity between the "Beautiful Victim" painting and the artwork included for the song in HIStory. The song was written and recorded during Off the Wall.[32]
On June 15, 1995, The New York Times claimed that "They Don't Care About Us" contained antisemitic slurs in the lines "Jew me, sue me, everybody do me / Kick me, kike me, don't you black or white me".[33] In a statement, Jackson responded:
The idea that these lyrics could be deemed objectionable is extremely hurtful to me, and misleading. The song, in fact, is about the pain of prejudice and hate and is a way to draw attention to social and political problems. I am the voice of the accused and the attacked. I am the voice of everyone. I am the skinhead, I am the Jew, I am the black man, I am the white man. I am not the one who was attacking. It is about the injustices to young people and how the system can wrongfully accuse them. I am angry and outraged that I could be so misinterpreted.[33]Jackson's manager and record label said the lyrics opposed prejudice and had been taken out of context.[33] The following day, David A. Lehrer and Rabbi Marvin Hier, leaders of two Jewish organizations, stated that Jackson's attempt to make a song critical of discrimination had backfired. They felt the lyrics might be ambiguous and were unsuitable for young audiences because they might not understand the song's context. They acknowledged that Jackson meant well and suggested that he write an explanation in the album booklet.[34] In his review of HIStory, Jon Pareles of The New York Times wrote that the song "gives the lie to his entire catalogue of brotherhood anthems with a burst of anti-Semitism".[35]
On June 17, Jackson promised that future copies of the album would include an apology, and concluded: "I just want you all to know how strongly I am committed to tolerance, peace and love, and I apologize to anyone who might have been hurt."[36] On June 23, Jackson announced that "Jew me" and "kike me" would be replaced with "do me" and "strike me" on future copies of the album.[37] He reiterated his acceptance that the song was offensive to some.[37][38] Spike Lee, who would direct the music videos for "They Don't Care About Us", said that he felt there was a double standard in the music industry, and that the word "nigger" in music does not cause controversy.[39] Rapper Notorious B.I.G. used the word "nigga" on another song on the album, "This Time Around", but it did not attract media attention.[39]
HIStory's music videos displayed different themes and elements, while some of them encouraged awareness of poverty and had a positive effect on their shooting locations. The promo for "They Don't Care About Us" was directed by Spike Lee; Jackson said that Lee chose to direct the video because the song "has an edge, and Spike Lee had approached me. It's a public awareness song and that's what he is all about. It's a protest kind of song... and I think he was perfect for it."[40] Jackson also collaborated with 200 members of the cultural group Olodum, who played music in the video.[41] The resulting media interest exposed Olodum to 140 countries, bringing them worldwide fame and increasing their status in Brazil.[42] Lcia Nagib, of The New Brazilian Cinema, said of the music video: When Michael Jackson decided to shoot his new music video in a favela of Rio de Janeiro... he used the favela people as extras in a visual super-spectacle... All the while there is a vaguely political appeal in there... The interesting aspect of Michael Jackson's strategy is the efficiency with which it gives visibility to poverty and social problems in countries like Brazil without resorting to traditional political discourse. The problematic aspect is that it does not entail a real intervention in that poverty.[43]In 2009, Billboard described the area as "now a model for social development" and stated that Jackson's influence was partially responsible for this improvement.[44] For the first time in Jackson's career, he made a second music video for a single.[45] This second version was filmed in a prison with cell mates; the video shows Jackson handcuffed and contains real footage of police attacking African Americans, the Ku Klux Klan, genocide, execution, and other human rights abuses.[45] Jackson's music video for "Earth Song" received praise for its environmental recognition. In 1995, the video received a Genesis Award for Doris Day Music Award, given each year for animal sensitivity.[46] In 2008, a writer for the Nigeria Exchange said that "'Earth Song' drew the world's attention to the degradation and bastardization of the earth as a fall out of various human activities".[47]
Two other music videos from HIStory have been influential. Jackson's "Stranger In Moscow" music video influenced the advertising campaign for International Cricket Council Champions Trophy 2004, which featured "a series of smart outdoor ads and a classy TV spot".[48] The television commercial was inspired by "Stranger In Moscow"s video where "the maiden in black splash about in the rain, with kids playing cricket for company".[48] "Scream" was a creative influence on other music videos such as "No Scrubs" (1999) by TLC.[49] This influence was also present on the 2008 release of "Shawty Get Loose" by Lil Mama and Chris Brown.[50] Reacting to the comparisons made between the videos, Mama explained, "I feel honored, because that was one of the initial goals, and I feel that it was executed well", she added that the emulation was intentional and that Brown was the only logical choice to step into Michael Jackson's role.[50]
Sony Music spent $30million to promote the album.[51] The music press were anticipating how well it would sell. One analyst for SoundScan expressed the opinion that the press was out of touch with the public when it came to Jackson; the public liked him, while the press did not.[52] He believed that "naysayers" in the media would be left surprised with the commercial reception.[52]
Also, during this period of time, Jackson did a series of personal appearances, becoming the first time that he faced the public eye following the allegations. On June 14, 1995, Jackson did the interview TV special "Primetime Live" along with his then wife Lisa Marie Presley and the interviewer Diane Sawyer. The special was watched by an audience 60 million in the United States and 500 millions worldwide. However, it received mixed reviews by critics. On September 7, 1995, he opened the MTV Video Music Awards with a 15 minutes medley.[53][54][55]
"Smile", "This Time Around" and "D.S." were released as promotional singles in 1995 and December 1997. Due to lack of radio airplay, "Smile" and "D.S." did not chart on any music charts worldwide. "This Time Around", was released as a radio-only single in the United States in December 1995. The song peaked at number 23 on the Billboard Hot R&B Singles chart and at number 18 on the Billboard Hot Dance Music/Club Play chart solely off radio airplay.[56]
To promote the album, Jackson embarked on the HIStory World Tour, which grossed $165 million (equivalent to $268 million in 2019).[57] It was Jackson's third and final concert tour as a solo artist. The tour, beginning in Prague, Czech Republic on September 7, 1996, attracted more than 4.5million fans from 58 cities in 35 countries around the world. The average concert attendance was 54,878 and the tour lasted 82 tour dates. Jackson performed no concerts in the United States, besides two concerts in January 1997 in Hawaii at the Aloha Stadium, to a crowd of 35,000 each; he was the first artist to sell out the stadium.[58][59] VIP seats cost, on average, $200 per person.[59] Each concert lasted an estimated two hours and ten minutes.[59] The tour concluded in Durban, South Africa on October 15, 1997.[60]
The album cover depicts a 10-foot sculpture of Jackson in a "warrior-like" pose, created in 1994 by Diana Walczak.[61] To promote the tour, Epic placed ten 30-foot replicas of the statue in locations around the world,[62] including the River Thames in London, Alexanderplatz in Berlin, Eindhoven in the Netherlands, and the pedestal of the destroyed Stalin Monument in Prague.[63] The statues were built over three months by a team of 30, made from steel and fiberglass, and weighed around 20,000 pounds each.[62] Another statue, built from wood and plaster, was placed at the Los Angeles Tower Records store.[64] In 2016, the original statue was installed at the Mandalay Bay casino in Las Vegas.[61]
Six singles were released from HIStory. "Scream"/"Childhood" was the first single released in May 1995. "Scream" was sung and performed by Jackson and his sister Janet Jackson. The single had the best ever debut at number five - where it peaked, on the Billboard Hot 100.[65] The song received a Grammy nomination for "Best Pop Collaboration With Vocals".[65][66] The music video for "Scream" is one of Jackson's most critically acclaimed songs and music videos, receiving numerous awards. With a US$9million music video production budget, "Scream" is the most expensive music video ever made as of 2015.[67]
"You Are Not Alone" was the second single released from HIStory. Having debuted at number one on the Billboard Hot 100 on September 2, 1995,[68] it became the first song to debut at number one on the chart, succeeding the record previously held from Jackson's "Scream" single.[65] "You Are Not Alone" was released in August 1995, and it topped the charts in various international markets, including the United Kingdom,[28] France, and Spain.[69] The song was seen as a major artistic and commercial success.[66]
"Earth Song" was the third single released in November 1995. "Earth Song" did not chart on Billboard 100. Internationally, the song topped four countries' charts, as well as charting within the top-ten in nine other nations.[70] The song topped the UK Singles Chart for six weeks over Christmas in 1995 and sold onemillion copies there, making it his most successful United Kingdom single, surpassing the success of his single "Billie Jean".[66]
"This Time Around" was released as the album's fourth single on December 26, 1995, with a guest rap by the Notorious B.I.G.. It was the album's first promotional single, and was released in the United States only. Tag lines for a December 1995 HBO special were heavily marketed on the copies of this single, but the special was canceled after Jackson had fallen ill.
"They Don't Care About Us" was the fifth single. "They Don't Care About Us" peaked at number thirty on the Billboard 100, and it charted within the top-ten of Billboard's Hot Dance Music and Hot R&B Singles Charts.[56] The song charted better in other countries, compared to the United States, managing to chart within the top-ten in fourteen countries. "They Don't Care About Us" topped the German Singles chart for three weeks,[71] while peaking at number two in Spain, number three in Austria, Sweden, and Switzerland, as well as charting at number four in France, the United Kingdom and the Netherlands.[72]
"Stranger in Moscow" was released as the sixth and final single in November 1996. The song was well received by critics. In the United States, the song peaked at number ninety one on the Billboard Hot 100.[73] Outside of the United States, the song was a success, topping in Spain and Italy, while peaking within the top-ten in the United Kingdom, Switzerland, and New Zealand, among others.[74][75]
"Smile" was originally intended to be the album's seventh and final single, and was to be released in CD and 12" format on January 20, 1998. However, the release was canceled due to unknown reasons, and most of the copies were subsequently destroyed. Only a few copies were sent out for airplay.
HIStory debuted at number one on the Billboard 200 and Top R&B/Hip-Hop Albums charts selling over 391,000 copies in its first week.[76][77] In its second week, the album stayed at the top with 263,000 copies sold, a decline of 33%.[78] In its third week, it slipped to number 2 with 142,000 copies sold, a 46% decline.[79] However, the album spent just six weeks at the top 10, selling over one million copies in total.[80] By the end of 1995, the album had sold more than 1.9 million units. According to SoundScan, the set fell short of many observers' expectations. Eventually, as of 2009, the album had sold 2.5 million copies in the United States (5 million units in total).[81][82]
However, the album was a massive success in other countries. The album was certified eight times platinum by the Recording Industry Association of America (RIAA) on August 23, 2018, in the United States. Because HIStory is a double disc album, its CDs are therefore counted separately for certification purposes, meaning the album achieved platinum status in the United States after 500,000 copies were shipped, not one million.
In Europe, before it was released, three million copies were shipped, breaking records as the most shipped album ever. The International Federation of the Phonographic Industry certified HIStory six times platinum, denoting six million shipments within the continent, including 1.5 million in Germany and 1.2 million shipments in the United Kingdom.[83][80]
In the United Kingdom, the album debuted at number one and sold 100,000 copies in just two days. It was certified 4x platinum by the BPI.[84]  In Australia, an advance order of 130,000 copies was the largest initial shipment in Sony Australia's history and in Spain HIStory was the 20th best selling album of 1995 and the 12th best selling album by a foreign artist.[85] Eventually, it sold 1.6 million copies.[86]
In Chile, the album topped the charts and broke all sales records in the country when it sold 25,000 units within 72 hours of its release on June 16.[87]
HIStory: Past, Present and Future, Book I has sold over 20 million copies worldwide,[88] and is the best selling multiple-disc release, making it one of the best-selling albums of all time[89] The greatest hits disc was reissued as a single disc on November 13, 2001, under the title Greatest Hits: HIStory, Volume I and had sold four million copies worldwide by 2010.[90] The second disc was released separately in some European countries in 2011.
HIStory received generally positive reviews. Jon Pareles of The New York Times wrote that "It has been a long time since Michael Jackson was simply a performer. He's the main asset of his own corporation, which is a profitable subsidiary of Sony."[91] Some reviewers commented on the unusual format of a new studio album being accompanied by a "greatest hits" collection, with Q magazine saying "from the new songs' point of view, it's like taking your dad with you into a fight."[92] Fred Shuster of the Daily News of Los Angeles described "This Time Around", "Money" and "D.S." as "superb slices of organic funk that will fuel many of the summer's busiest dance floors".[93]James Hunter of Rolling Stone gave HIStory four-out-of-five stars and found that it "unfolds in Jackson's outraged response to everything he has encountered in the last year or so." Hunter felt it was an "odd, charmless second chapter" compared to Jackson's earlier hits.[20] However, he described "This Time Around" as a "dynamite jam" that was "ripe for remixes", and "Scream" and "Tabloid Junkie" as "adventurous". He said "Earth Song" had "noble sentiments" and sounded "primarily like a showpiece".[20] Jim Farber of the New York Daily News gave the album a mixed review and commented that he would give the first disc on its own.[19] Jon Pareles of The New York Times believed that Jackson "muttered" lyrics such as "They thought they really had control of me".[91] Chris Willman of the Los Angeles Times said of "This Time Around", "a tough, rhythm-guitar-driven track co-written and co-produced by hit-maker Dallas Austin that sports one of the album's better grooves".[97]
Stephen Thomas Erlewine of Allmusic gave HIStory three-out-of-five stars, but commented that it was a "monumental achievement" of Jackson's ego.[94] Erlewine remarked that on the HIStory Begins CD, it contains "some of the greatest music in pop history" but that it leaves some hits out, citing "Say Say Say" and "Dirty Diana"  commenting that "yet it's filled with enough prime material to be thoroughly intoxicating".[94] Erlewine noted that HIStory Continues is "easily the most personal album Jackson has recorded" and that its songs' lyrics referencing the molestation accusations create a "thick atmosphere of paranoia".[94] He cited "You Are Not Alone" and "Scream" as being "well-crafted pop that ranks with his best material", but concludes that "nevertheless, HIStory Continues stands as his weakest album since the mid-'70s."[94] David Browne of Entertainment Weekly, gave "HIStory Begins" an "A-" grade but the album's new material a "C-", which "winds up a B" for the entire album.[21] Browne commented that the music "rarely seems to transport him (and thereby us) to a higher plane."[21]
Controversy with the album came with Jackson having to rerecord some lyrics in "They Don't Care About Us" after he was accused of antisemitism, and contributor R. Kelly was accused of having plagiarized one of the album's songs, "You Are Not Alone", leading to its banning on Belgian radio.
HIStory was nominated for six Grammy Awards at the 1996 and 1997 ceremonies respectively, winning one award. "You Are Not Alone" was nominated for Best Pop Vocal Performance  Male and for Song of the Year. "Scream" was nominated for Best Pop Collaboration with Vocals and "Scream" won Best Music Video - Short Form and "Earth Song" was nominated for the same award the following year. The album itself was nominated for Album of the Year. At the 1995 MTV Video Music Awards, "Scream" received ten nominations, winning in three categories.[98] In 1998, the album was ranked at number 96 in BBC's Music of the Millennium, a list of 100 albums chosen by Channel 4 viewers, The Guardian readers and HMV customers as the best of the millennium.[99]
Adapted from the album's liner notes and AllMusic.[17][100]

* Sales figures based on certification alone.^ Shipments figures based on certification alone. Sales+streaming figures based on certification alone.


This is the history of American aerospace manufacturing company Boeing.
In 1909 William E. Boeing, a wealthy lumber entrepreneur who studied at Yale University, became fascinated with airplanes after seeing one at the Alaska-Yukon-Pacific Exposition in Seattle.  In 1910 he bought the Heath Shipyard, a wooden boat manufacturing facility at the mouth of the Duwamish River, which would become his first airplane factory.[2] In 1915 Boeing traveled to Los Angeles to be taught flying by Glenn Martin and purchased a Martin "Flying Birdcage" seaplane (so-called because of all the guy-wires holding it together). The aircraft was shipped disassembled by rail to the northeast shore of Lake Union, where Martin's pilot and handyman James Floyd Smith assembled it in a tent hangar.  The Birdcage was damaged in a crash during testing, and when Martin informed Boeing that replacement parts would not become available for months, Boeing realized he could build his own plane in that amount of time.  He put the idea to his friend George Conrad Westervelt, a U.S. Navy engineer, who agreed to work on an improved design and help build the new airplane, called the "B&W" seaplane. Boeing made good use of his Duwamish boatworks and its woodworkers under the direction of Edward Heath, from whom he bought it, in fabricating wooden components to be assembled at Lake Union.  Westervelt was transferred to the east coast by the Navy before the plane was finished, however, Boeing hired Wong Tsu to replace Westervelt's engineering expertise, and completed two B&Ws in the lakeside hangar.  On June 15, 1916, the B&W took its maiden flight. Seeing the opportunity to be a regular producer of airplanes, with the expertise of Mr. Wong, suitable productive facilities, and an abundant supply of spruce wood suitable for aircraft, Boeing incorporated his airplane manufacturing business as "Pacific Aero Products Co" on July 15, 1916.[3][4]  The B&W airplanes were offered to the US Navy but they were not interested, and regular production of airplanes would not begin until US entry into World War I a year later.
On April 26, 1917[A 1], Boeing changed the name to the "Boeing Airplane Company". Boeing was later reincorporated in Delaware; the original Certificate of Incorporation was filed with the Secretary of State of Delaware on July 19, 1934.   
In 1917, the company moved its operations to Boeing's Duwamish boatworks, which became Boeing Plant 1.  The Boeing Airplane Company's first engineer was Wong Tsu, a Chinese graduate of the Massachusetts Institute of Technology hired by Boeing in May 1916.[8] He designed the Boeing Model C, which was Boeing's first financial success.[9] On April 6, 1917, the U.S. had declared war on Germany and entered World War I. With the U.S. entering the war, Boeing knew that the U.S. Navy needed seaplanes for training, so Boeing shipped two new Model Cs to Pensacola, Florida, where the planes were flown for the Navy. The Navy liked the Model C and ordered 50 more.[10] In light of this financial windfall, "from Bill Boeing onward, the company's chief executives through the decades were careful to note that without Wong Tsu's efforts, especially with the Model C, the company might not have survived the early years to become the dominant world aircraft manufacturer."[9]
When World War I ended in 1918, a large surplus of cheap, used military planes flooded the commercial airplane market, preventing aircraft companies from selling any new airplanes, driving many out of business. Others, including Boeing, started selling other products. Boeing built dressers, counters, and furniture, along with flat-bottom boats called Sea Sleds.[10]
In 1919, the Boeing B-1 flying boat made its first flight. It accommodated one pilot and two passengers and some mail. Over the course of eight years, it made international airmail flights from Seattle to Victoria, British Columbia.[11]  On May 24, 1920, the Boeing Model 8 made its first flight. It was the first airplane to fly over Mount Rainier.[12]
In 1923, Boeing entered competition against Curtiss to develop a pursuit fighter for the U.S. Army Air Service. The Army accepted both designs and Boeing continued to develop its PW-9 fighter into the subsequent radial-engined F2B F3B, and P12/F4B fighters,[13] which made Boeing a leading manufacturer of fighters over the course of the next decade.
In 1925, Boeing built its Model 40 mail airplane for the U.S. government to use on airmail routes. In 1927, an improved version, the Model 40A was built. The Model 40A won the U.S. Post Office's contract to deliver mail between San Francisco and Chicago. This model also had a cabin to accommodate two passengers.[14]
That same year, Boeing created an airline named Boeing Air Transport, which merged a year later with Pacific Air Transport and the Boeing Airplane Company. The first airmail flight for the airline was on July 1, 1927.[14]  In 1929, the company merged with Pratt & Whitney, Hamilton Aero Manufacturing Company, and Chance Vought under the new title United Aircraft and Transport Corporation. The merger was followed by the acquisition of the Sikorsky Manufacturing Corporation, Stearman Aircraft Corporation, and Standard Metal Propeller Company.  United Aircraft then purchased National Air Transport in 1930.
On July 27, 1928, the 12-passenger Boeing 80 biplane made its first flight. With three engines, it was Boeing's first plane built with the sole intention of being a passenger transport. An upgraded version, the 80A, carrying eighteen passengers, made its first flight in September 1929.[14]
In the early 1930s, Boeing became a leader in all-metal aircraft construction, and in the design revolution that established the path for transport aircraft through the 1930s. In 1930, Boeing built the Monomail, a low-wing all-metal monoplane that carried mail. The low drag airframe with cantilever wings and the retractable landing gear was so revolutionary that the engines and propellers of the time were not adequate to realize the potential of the plane. By the time controllable pitch propellers were developed, Boeing was building its Model 247 airliner. Two Monomails were built. The second one, the Model 221, had a 6-passenger cabin.[15][16] In 1931, the Monomail design became the foundation of the Boeing YB-9, the first all-metal, cantilever-wing, monoplane bomber.  Five examples entered service between September 1932 and March 1933.  The performance of the twin-engine monoplane bomber led to a reconsideration of air defense requirements, although it was soon rendered obsolete by rapidly advancing bomber designs.
In 1932, Boeing introduced the Model 248, the first all-metal monoplane fighter.  The P-26 Peashooter was in front-line service with the US Army Air Corps from 1934 to 1938.
In 1933, the Boeing 247 was introduced, which set the standard for all competitors in the passenger transport market. The 247 was an all-metal low-wing monoplane that was much faster, safer, and easier to fly than other passenger aircraft. For example, it was the first twin-engine passenger aircraft that could fly on one engine. In an era of unreliable engines, this vastly improved flight safety. Boeing built the first 59 aircraft exclusively for its own United Airlines subsidiary's operations. The direction established with the 247 was further developed by Douglas Aircraft, resulting in one of the most successful designs in aviation history. 
The Air Mail Act of 1934 prohibited airlines and manufacturers from being under the same corporate umbrella, so the company split into three smaller companies  Boeing Airplane Company, United Airlines, and United Aircraft Corporation, the precursor to United Technologies. Boeing retained the Stearman facilities in Wichita, Kansas.  Following the breakup of United Aircraft, William Boeing sold off his shares and left Boeing. Clairmont "Claire" L. Egtvedt, who had become Boeing's president in 1933, became the chairman as well. He believed the company's future was in building bigger planes.[17][18] Work began in 1936 on Boeing Plant 2 to accommodate the production of larger modern aircraft. 
From 1934 to 1937, Boeing was developing an experimental long-range bomber, the XB-15.  At its introduction in 1937, it was the largest heavier-than-air craft built to date.  Trials revealed that its speed was unsatisfactory, but the design experience was used in the development of the Model 299 prototype four-engine bomber of 1935, which was developed into the YB-17 of 1936, and the Model 314 flying boat that first flew in 1938.
Overlapping with the period of the YB-15 development, an agreement with Pan American World Airways (Pan Am) was reached, to develop and build a commercial flying boat able to carry passengers on transoceanic routes. The first flight of the Boeing 314 Clipper was in June 1938. It was the largest civil aircraft of its time, with a capacity of 90 passengers on day flights, and of 40 passengers on night flights. One year later, the first regular passenger service from the U.S. to the UK was inaugurated. Subsequently, other routes were opened, so that soon Pan Am flew with the Boeing 314 to destinations all over the world.
In 1938, Boeing completed work on its Model 307 Stratoliner. This was the world's first pressurized-cabin transport aircraft, and it was capable of cruising at an altitude of 20,000 feet (6,100m)  above most weather disturbances. It was based on the B-17, using the same wings, tail, and engines.
During World War 2, Boeing built a large number of B-17 and B-29 bombers. Boeing ranked twelfth among United States corporations in the value of wartime production contracts.[19] Many of the workers were women whose husbands had gone to war. In the beginning of March 1944, production had been scaled up in such a manner that over 350 planes were built each month. To prevent an attack from the air, the manufacturing plants had been covered with greenery and farmland items. During the war years the leading aircraft companies of the U.S. cooperated. The Boeing-designed B-17 bomber was assembled also by Vega (a subsidiary of Lockheed Aircraft Corp.) and Douglas Aircraft Co., while the B-29 was assembled also by Bell Aircraft Co. and by Glenn L. Martin Company.[20]  In 1942 Boeing started the development of the C-97 Stratofreighter, the first of a generation of heavy-lift military transports; it became operational in 1947.  The C-97 design would be successfully adapted for use as an aerial refueling tanker, although its role as a mode of transport was soon limited by designs that had advantages in either versatility or capacity.
After the war, most orders of bombers were cancelled and 70,000 people lost their jobs at Boeing.[citation needed] The company aimed to recover quickly by selling its Stratocruiser (the Model 377), a luxurious four-engine commercial airliner derived from the C-97. However, sales of this model were not as expected and Boeing had to seek other opportunities to overcome the situation.[citation needed]  In 1947 Boeing flew its first jet aircraft, the XB-47, from which the highly successful B-47 and B-52 bombers were derived.
Boeing developed military jets such as the B-47 Stratojet[21] and B-52 Stratofortress bombers in the late-1940s and into the 1950s. During the early 1950s, Boeing used company funds to develop the 36780 jet airliner demonstrator that led to the KC-135 Stratotanker and Boeing 707 jetliner. Some of these were built at Boeing's facilities in Wichita, Kansas, which existed from 1931 to 2014.
Between the last delivery of a 377 in 1950 and the first order for the 707 in 1955, Boeing was shut out of the commercial aircraft market.
In the mid-1950s technology had advanced significantly, which gave Boeing the opportunity to develop and manufacture new products. One of the first was the guided short-range missile used to intercept enemy aircraft. By that time the Cold War had become a fact of life, and Boeing used its short-range missile technology to develop and build an intercontinental missile.
In 1958, Boeing began delivery of its 707, the United States' first commercial jet airliner, in response to the British De Havilland Comet, French Sud Aviation Caravelle and Soviet Tupolev Tu-104, which were the world's first generation of commercial jet aircraft. With the 707, a four-engine, 156-passenger airliner, the U.S. became a leader in commercial jet manufacturing. A few years later, Boeing added a second version of this aircraft, the Boeing 720, which was slightly faster and had a shorter range.
Boeing was a major producer of small turbine engines during the 1950s and 1960s. The engines represented one of the company's major efforts to expand its product base beyond military aircraft after World War II. Development on the gas turbine engine started in 1943 and Boeing's gas turbines were designated models 502 (T50), 520 (T60), 540, 551 and 553. Boeing built 2,461 engines before production ceased in April 1968. Many applications of the Boeing gas turbine engines were considered to be firsts, including the first turbine-powered helicopter and boat.[22]
Vertol Aircraft Corporation was acquired by Boeing in 1960,[23] and was reorganized as Boeing's Vertol division. The twin-rotor CH-47 Chinook, produced by Vertol, took its first flight in 1961. This heavy-lift helicopter remains a work-horse vehicle to the present day. In 1964, Vertol also began production of the CH-46 Sea Knight.
In December 1960, Boeing announced the model 727 jetliner, which went into commercial service about three years later. Different passenger, freight, and convertible freighter variants were developed for the 727. The 727 was the first commercial jetliner to reach 1,000 sales.[24]
On May 21, 1961, the company shortened its name to the current "Boeing Company".[25][not specific enough to verify]
Boeing won a contract in 1961 to manufacture the S-IC stage of the Saturn V rocket, manufactured at the Michoud Assembly Facility in New Orleans, Louisiana.
In 1966, Boeing president William M. Allen asked Malcolm T. Stamper to spearhead production of the new 747 airliner on which the company's future was riding. This was a monumental engineering and management challenge and included the construction of the world's biggest factory in which to build the 747 at Everett, Washington, a plant which is the size of 40 football fields.[26]
In 1967, Boeing introduced another short- and medium-range airliner, the twin-engine 737. It has since become the best-selling commercial jet aircraft in aviation history.[27] Several versions have been developed, mainly to increase seating capacity and range.  The 737 remains in production as of February 2018 with the latest 737 MAX series.
The roll-out ceremonies for the first 747-100 took place in 1968, at the massive new factory in Everett, about an hour's drive from Boeing's Seattle home. The aircraft made its first flight a year later. The first commercial flight occurred in 1970. The 747 has an intercontinental range and a larger seating capacity than Boeing's previous aircraft.
Boeing also developed hydrofoils in the 1960s. The screw-driven USS High Point (PCH-1) was an experimental submarine hunter. The patrol hydrofoil USS Tucumcari (PGH-2) was more successful. Only one was built, but it saw service in Vietnam and Europe before running aground in 1972. Its waterjet and fully submersed flying foils were the examples for the later Pegasus-class patrol hydrofoils and the Model 929 Jetfoil ferries in the 1980s. The Tucumcari and later boats were produced in Renton. While the Navy hydrofoils were withdrawn from service in the late 1980s, the Boeing Jetfoils are still in service in Asia.
In the early 1970s Boeing suffered from the simultaneous decline in Vietnam War military spending, the slowing of the space program as Project Apollo neared completion, the recession of 196970,[28]:291 and the company's $2billion debt as it built the new 747 airliner.[28]:303 Boeing did not receive any orders for more than a year. Its bet for the future, the 747, was delayed in production by three months because of problems with its Pratt & Whitney engines. Then in March 1971, Congress voted to discontinue funding for the development of the Boeing 2707 supersonic transport (SST), the US's answer to the British-French Concorde, forcing the end of the project.[29][30][31][32][33][34]
Commercial Airplane Group, by far the largest unit of Boeing, went from 83,700 employees in 1968 to 20,750 in 1971. Each unemployed Boeing employee cost at least one other job in the Seattle area, and unemployment rose to 14%, the highest in the United States.[citation needed] Housing vacancy rates rose to 16% from 1% in 1967.[citation needed] U-Haul dealerships ran out of trailers because so many people moved out. A billboard appeared near the airport:[28]:303304
Will the last person leaving SEATTLE -Turn out the lights.[28]:303In January 1970, the first 747, a four-engine long-range airliner, flew its first commercial flight with Pan American World Airways.  The 747 changed the airline industry, providing much larger seating capacity than any other airliner in production.  The company has delivered over 1,500 Boeing 747s. The 747 has undergone continuous improvements to keep it technologically up-to-date. Larger versions have also been developed by stretching the upper deck. The newest version of the 747, the 747-8, remains in production as of 2018.[citation needed]
Boeing launched three Jetfoil 929-100 hydrofoils that were acquired in 1975 for service in the Hawaiian Islands. When the service ended in 1979 the three hydrofoils were acquired by Far East Hydrofoil for service between Hong Kong and Macau.[35]
During the 1970s, Boeing also developed the US Standard Light Rail Vehicle, which has been used in San Francisco, Boston, and Morgantown, West Virginia.[36]
In 1983, the economic situation began to improve. Boeing assembled its 1,000th 737 passenger aircraft. During the following years, commercial aircraft and their military versions became the basic equipment of airlines and air forces. As passenger air traffic increased, the competition was harder, mainly from Airbus, a European newcomer in commercial airliner manufacturing. Boeing had to offer new aircraft and developed the single-aisle 757, the larger, twin-aisle 767, and upgraded versions of the 737. An important project of these years was the Space Shuttle, to which Boeing contributed with its experience in space rockets acquired during the Apollo era. Boeing participated also with other products in the space program and was the first contractor for the International Space Station program.
During the decade several military projects went into production, including Boeing support of the B-2 stealth bomber. As part of an industry team led by Northrop, Boeing built the B-2's outer wing portion, aft center fuselage section, landing gear, fuel system, and weapons delivery system. At its peak in 1991, the B-2 was the largest military program at Boeing, employing about 10,000 people. The same year, the US's National Aeronautic Association awarded the B-2 design team the Collier Trophy for the greatest achievement in aerospace in America. The first B-2 rolled out of the bomber's final assembly facility in Palmdale, California, in November 1988 and it flew for the first time on July 17, 1989.[37]
The Avenger air defense system and a new generation of short-range missiles also went into production. During these years, Boeing was very active in upgrading existing military equipment and developing new ones. Boeing also contributed to wind power development with the experimental MOD-2 Wind Turbines for NASA and the United States Department of Energy, and the MOD-5B for Hawaii.[38]
Boeing was one of seven competing companies that bid for the Advanced Tactical Fighter. Boeing agreed to team with General Dynamics and Lockheed so that all three companies would participate in the development if one of the three companies' designs was selected. The Lockheed design was eventually selected and developed into the F-22 Raptor.[39]
In April 1994, Boeing introduced the most modern commercial jet aircraft at the time, the twin-engine 777, with a seating capacity of approximately 300 to 370 passengers in a typical three-class layout, in between the 767 and the 747. The longest range twin-engined aircraft in the world, the 777 was the first Boeing airliner to feature a "fly-by-wire" system and was conceived partly in response to the inroads being made by the European Airbus into Boeing's traditional market. This aircraft reached an important milestone by being the first airliner to be designed entirely by using computer-aided design (CAD) techniques.[40] The 777 was also the first airplane to be certified for 180 minute ETOPS at entry into service by the FAA.[41]  Also in the mid-1990s, the company developed the revamped version of the 737, known as the 737 "Next-Generation", or 737NG. It has since become the fastest-selling version of the 737 in history, and on April 20, 2006, sales passed those of the "Classic 737", with a follow-up order for 79 aircraft from Southwest Airlines.
In 1995, Boeing chose to demolish the headquarters complex on East Marginal Way South instead of upgrading it to match new seismic standards. The headquarters were moved to an adjacent building and the facility was demolished in 1996.[42]  In 1997, Boeing was headquartered on East Marginal Way South, by King County Airport, in Seattle.[43]
In 1996, Boeing acquired Rockwell's aerospace and defense units. The Rockwell business units became a subsidiary of Boeing, named Boeing North American, Inc. In December 1996, Boeing announced its intention to merge with McDonnell Douglas and, following regulatory approval, this was completed on August 4, 1997.[44] This had been delayed by objections from the European Commission, which ultimately placed 3 conditions on the merger: termination of exclusivity agreements with 3 US airlines, separate accounts would be maintained for the McDonnell-Douglas civil aircraft business, and some defense patents were to be made available to competitors.[45] Following the merger, the McDonnell Douglas MD-95 was renamed the Boeing 717, and the production of the MD-11 trijet was limited to the freighter version. Boeing introduced a new corporate identity with completion of the merger, incorporating the Boeing Stratotype wordmark introduced in 1947, and a stylized version of the McDonnell Douglas symbol, which was the adoption of the Douglas Aircraft logo from 1962. It was done by graphic designer Rick Eiber followed a request from Boeing.[46][47][48][49]
An aerospace analyst criticized the CEO and his deputy, Philip M. Condit and Harry Stonecipher, for thinking of their personal benefits first, and causing the problems to Boeing many years later. Instead of investing the huge cash reserve to build new airplanes, they initiated a program to buy back Boeing stock for more than US$10billion.[50][importance?]
In May 1999, Boeing studied buying Embraer to encourage commonality between the E-Jets and the Boeing 717, but this was nixed by then-president Harry Stonecipher. He preferred buying Bombardier Aerospace, but its owner, the Beaudoin family, asked for a price too high for Boeing which remembered its mid-1980s purchase of de Havilland Canada, losing a million dollars every day for three years before selling it to Bombardier in 1992.[51]
In January 2000, Boeing chose to expand its presence in another aerospace field of satellite communications by purchasing Hughes Electronics.[52]
In March 2001, Boeing announced the pending relocation of its headquarters from Seattle to one of three cities: Chicago, Dallas, orDenver.[53][54] All three had offered packages of multimillion-dollar tax breaks, and the selection of Chicago was announced on May10;[55][56][57] the move was completed in early September, just before 9/11.[58] Its offices are located in the Fulton River District just outside the Chicago Loop.[59]
In October 2001, Boeing lost to its rival Lockheed Martin in the fierce competition for the multibillion-dollar Joint Strike Fighter contract.[60][61] Boeing's entry, the X-32, was rejected in favor of Lockheed's X-35. Boeing continues to serve as the prime contractor on the International Space Station and has built several of the major components.
Boeing began development of the KC-767 aerial refueling tanker in the early 2000s. Italy and Japan ordered four KC-767s each.  After development delays and FAA certification, Boeing delivered the tankers to Japan from 2008[62][63] with the second KC-767 following on March 5.[64] to 2010.[65]  Italy received its four KC-767 during 2011.[66][67][68]
In 2004, Boeing ended production of the 757 after 1,050 aircraft were produced. More advanced stretched versions of the 737 were beginning to compete against the 757, and the planned 787-3 was to fill much of the top end of the 757 markets. Also that year, Boeing announced that the 717, the last civil aircraft to be designed by McDonnell Douglas, would cease production in 2006. The 767 was in danger of cancellation as well, with the 787 replacing it, but orders for the freighter version extended the program.
After several decades of success, Boeing lost ground to Airbus and subsequently lost its lead in the airliner market in 2003. Multiple Boeing projects were pursued and then canceled, notably the Sonic Cruiser, a proposed jetliner that would travel just under the speed of sound, cutting intercontinental travel times by as much as 20%. It was launched in 2001 along with a new advertising campaign to promote the company's new motto, "Forever New Frontiers", and to rehabilitate its image. However, the plane's fate was sealed by the changes in the commercial aviation market following the September 11 attacks and the subsequent weak economy and an increase in fuel prices.
Subsequently, Boeing streamlined its production and turned its attention to a new model, the Boeing 787 Dreamliner, using much of the technology developed for the Sonic Cruiser, but in a more conventional aircraft designed for maximum efficiency. The company also launched new variants of its successful 737 and 777 models. The 787 proved to be a highly popular choice with airlines and won a record number of pre-launch orders. With delays to Airbus' A380 program several airlines threatened to switch their A380 orders to Boeing's new 747 version, the 747-8.[69] Airbus's response to the 787, the A350, received a lukewarm response at first when it was announced as an improved version of the A330, and then gained significant orders when Airbus promised an entirely new design. The 787 program encountered delays, with the first flight not occurring until late 2009.[70]
After regulatory approval, Boeing formed a joint venture, United Launch Alliance with its competitor, Lockheed Martin, on December 1, 2006. The new venture is the largest provider of rocket launch services to the U.S. government.[71]
In 2005, Gary Scott, ex-Boeing executive and then head of Bombardier's CSeries program, suggested a collaboration on the upcoming CSeries, but an internal study assessed Embraer as the best partner for regional jets. The Brazilian government wanted to retain control and blocked an acquisition.[51]
On August 2, 2005, Boeing sold its Rocketdyne rocket engine division to Pratt & Whitney. On May 1, 2006, Boeing agreed to purchase Dallas, Texas-based Aviall, Inc. for $1.7billion and retain $350million in debt. Aviall, Inc. and its subsidiaries, Aviall Services, Inc. and ILS formed a wholly owned subsidiary of Boeing Commercial Aviation Services (BCAS).[72]
Realizing that increasing numbers of passengers have become reliant on their computers to stay in touch, Boeing introduced Connexion by Boeing, a satellite-based Internet connectivity service that promised air travelers unprecedented access to the World Wide Web. The company debuted the product to journalists in 2005, receiving generally favorable reviews. However, facing competition from cheaper options, such as cellular networks, it proved too difficult to sell to most airlines. In August 2006, after a short and unsuccessful search for a buyer for the business, Boeing chose to discontinue the service.[73][74]
On August 18, 2007, NASA selected Boeing as the manufacturing contractor for the liquid-fueled upper stage of the Ares I rocket.[75] The stage, based on both Apollo-Saturn and Space Shuttle technologies, was to be constructed at NASA's Michoud Assembly Facility near New Orleans; Boeing constructed the S-IC stage of the Saturn V rocket at this site in the 1960s.
Boeing launched the 777 Freighter in May 2005 with an order from Air France. The freighter variant is based on the 200LR. Other customers include FedEx and Emirates. Boeing officially announced in November 2005 that it would produce a larger variant of the 747, the 747-8, in two versions, commencing with the Freighter version with firm orders for two cargo carriers. The second version, named the Intercontinental, is for passenger airlines. Both 747-8 versions feature a lengthened fuselage, new, advanced engines and wings, and the incorporation of other technologies developed for the 787.
Boeing also received the launch contract from the U.S. Navy for the P-8 Poseidon Multimission Maritime Aircraft, an anti-submarine warfare patrol aircraft. It has also received orders for the 737 AEW&C "Wedgetail" aircraft. The company has also introduced new extended range versions of the 737. These include the 737-700ER and 737-900ER. The 737-900ER is the latest and will extend the range of the 737-900 to a similar range as the successful 737-800 with the capability to fly more passengers, due to the addition of two extra emergency exits.
The 777-200LR Worldliner embarked on a well-received global demonstration tour in the second half of 2005, showing off its capacity to fly farther than any other commercial aircraft. On November 10, 2005, the 777-200LR set a world record for the longest non-stop flight. The plane, which departed from Hong Kong traveling to London, took a longer route, which included flying over the U.S. It flew 11,664 nautical miles (21,601km) during its 22-hour 42-minute flight. It was flown by Pakistan International Airlines pilots and PIA was the first airline to fly the 777-200LR Worldliner.
On August 11, 2006, Boeing agreed to form a joint-venture with the large Russian titanium producer, VSMPO-Avisma for the machining of titanium forgings. The forgings will be used on the 787 program.[76]  In December 2007, Boeing and VSMPO-Avisma created a joint venture, Ural Boeing Manufacturing, and signed a contract on titanium product deliveries until 2015, with Boeing planning to invest $27billion in Russia over the next 30 years.[77]
In February 2011, Boeing received a contract for 179 KC-46 U.S. Air Force tankers at a value of $35billion.[78] The KC-46 tankers are based on the KC-767.
Boeing, along with Science Applications International Corporation (SAIC), were the prime contractors in the U.S. military's Future Combat Systems program.[79]  The FCS program was canceled in June 2009 with all remaining systems swept into the BCT Modernization program.[80]  Boeing works jointly with SAIC in the BCT Modernization program like the FCS program but the U.S. Army will play a greater role in creating baseline vehicles and will only contract others for accessories.
Defense Secretary Robert M. Gates' shift in defense spending to, "make tough choices about specific systems and defense priorities based solely on the national interest and then stick to those decisions over time"[81] hit Boeing especially hard, because of their heavy involvement with canceled Air Force projects.[82]
In May 2003, the U.S. Air Force announced it would lease 100 KC-767 tankers to replace the oldest 136 KC-135s. In November 2003, responding to critics who argued that the lease was more expensive than an outright purchase, the DoD announced a revised lease of 20 aircraft and a purchase of 80. In December 2003, the Pentagon announced the project was to be frozen while an investigation of allegations of corruption by one of its former procurement staffers, Darleen Druyun (who began employment at Boeing in January) was begun. The fallout of this resulted in the resignation of Boeing CEO Philip M. Condit and the termination of CFO Michael M. Sears.[83] Harry Stonecipher, former McDonnell Douglas CEO and Boeing COO, replaced Condit on an interim basis. Druyun pleaded guilty to inflating the price of the contract to favor her future employer and to passing information on the competing Airbus A330 MRTT bid. In October 2004, she received a sentence of nine months in federal prison, seven months in a community facility, and three years probation.[84]
In March 2005, the Boeing board forced President and CEO Harry Stonecipher to resign. Boeing said an internal investigation revealed a "consensual" relationship between Stonecipher and a female executive that was "inconsistent with Boeing's Code of Conduct" and "would impair his ability to lead the company".[85] James A. Bell served as interim CEO (in addition to his normal duties as Boeing's CFO) until the appointment of Jim McNerney as the new Chairman, President, and CEO on June 30, 2005.
In June 2003, Lockheed Martin sued Boeing, alleging that the company had resorted to industrial espionage in 1998 to win the Evolved Expendable Launch Vehicle (EELV) competition. Lockheed Martin claimed that the former employee Kenneth Branch, who went to work for McDonnell Douglas and Boeing, passed nearly 30,000 pages of proprietary documents to his new employers. Lockheed Martin argued that these documents allowed Boeing to win 19 of the 28 tendered military satellite launches.[86][87]
In July 2003, Boeing was penalized, with the Pentagon stripping seven launches away from the company and awarding them to Lockheed Martin.[86] Furthermore, the company was forbidden to bid for rocket contracts for a twenty-month period, which expired in March 2005.[87]  In early September 2005, it was reported that Boeing was negotiating a settlement with the U.S. Department of Justice in which it would pay up to $500million to cover this and the Darleen Druyun scandal.[88]
In July 2009, naturalized citizen Dongfan Chung, an engineer working with Boeing, was the first person convicted under the Economic Espionage Act of 1996.[89]) Chung is suspected of having passed to China classified information on designs including the Delta IV rocket, F-15 Eagle, B-52 Stratofortress and the CH-46 and CH-47 helicopters.[90]
Until the late 1970s, the U.S. had a near monopoly in the Large Civil Aircraft (LCA) sector.[91] The Airbus consortium (created in 1969) started competing effectively in the 1980s. At that stage the U.S. became concerned about European competition and the alleged subsidies paid by the European governments for the developments of the early models of the Airbus family. This became a major issue of contention, as the European side was equally concerned by subsidies accruing to U.S. LCA manufacturers through NASA and Defense programs.
Europe and the U.S. started bilateral negotiations for the limitation of government subsidies to the LCA sector in the late 1980s. Negotiations were concluded in 1992 with the signing of the EC-US Agreement on Trade in Large Civil Aircraft which imposes disciplines on government support on both sides of the Atlantic which are significantly stricter than the relevant World Trade Organization (WTO) rules: Notably, the Agreement regulates in detail the forms and limits of government support, prescribes transparency obligations and commits the parties to avoiding trade disputes.[92]
In 2004, the EU and the U.S. agreed to discuss a possible revision of the 1992 EC-US Agreement provided that this would cover all forms of subsidies including those used in the U.S., and in particular the subsidies for the Boeing 787; the first new aircraft to be launched by Boeing for 14years. In October 2004 the U.S. began legal proceedings at the WTO by requesting WTO consultations on European launch investment to Airbus. The U.S. also unilaterally withdrew from the 1992 EU-US Agreement.[93] The U.S. claimed Airbus had violated a 1992 bilateral accord when it received what Boeing deemed "unfair" subsidies from several European governments. Airbus responded by filing a separate complaint, contesting that Boeing had also violated the accord when it received tax breaks from the U.S. Government. Moreover, the EU also complained that the investment subsidies from Japanese airlines violated the accord.
On January 11, 2005, Boeing and Airbus agreed that they would attempt to find a solution to the dispute outside of the WTO. However, in June 2005, Boeing and the United States government reopened the trade dispute with the WTO, claiming that Airbus had received illegal subsidies from European governments. Airbus has also responded to this claim against Boeing, reopening the dispute and also accusing Boeing of receiving subsidies from the U.S. Government.[94]
On September 15, 2010, the WTO ruled that Boeing had received billions of dollars in government subsidies.[95] Boeing responded by stating that the ruling was a fraction of the size of the ruling against Airbus and that it required few changes in its operations.[96] Boeing has received $8.7billion in support from Washington state.[97]
In summer 2010, Boeing acquired Fairfax, VA-based C4ISR and combat systems developer Argon ST to expand its C4ISR, cyber and intelligence capabilities.[98]
In 2011, Boeing was hesitating between re-engineing the 737 or developing an all-new small airplane for which Embraer could have been involved, but when the A320neo was launched with new engines, that precipitated the 737 MAX decision.[51]
On November 17, Boeing received its largest provisional order for $21.7billion at list prices from Indonesian LCC Lion Air for 201 737 MAX, 29 737-900ERs and 150 purchase rights, days after its previous order record of $18billion for 50 777-300ER from Emirates.[99]
On January 5, 2012, Boeing announced it would close its facilities in Wichita, Kansas with 2,160 workers before 2014, more than 80 years after it was established, where it had employed as many as 40,000 people.[100][101]
In May 2013, Boeing announced it would cut 1,500 IT jobs in Seattle over the next three years through layoffs, attrition and mostly relocation to St. Louis and North Charleston, South Carolina  600 jobs each.[102][103]
In September, Boeing announced their Long Beach facility manufacturing the C-17 Globemaster III military transport would shut down.[104]
In January 2014, the company announced US$1.23billion profits for Q4 2013, a 26% increase, due to higher demand for commercial aircraft.[105] The last plane to undergo maintenance in Boeing Wichita's facility left in May 2014.[106]
In September 2014, NASA awarded contracts to Boeing and SpaceX for transporting astronauts to the International Space Station.[107]
In June 2015, Boeing announced that James McNerney would step down as CEO to be replaced by Boeing's COO, Dennis Muilenburg, on July 1, 2015.[108] The 279th and last C-17 was delivered in summer before closing the site, affecting 2,200 jobs.[104] Also in 2015, Boeing reportedly started the studies of the 797/NMA, after revealing that a replacement of its own 757 would be a replacement rather than a re-engining.[109]
In February 2016, Boeing announced that Boeing President and CEO Dennis Muilenburg was elected the 10th Chairman of the Board, succeeding James McNerney.[110] In March, Boeing announced plans to cut 4,000 jobs from its commercial airplane division by mid-year.[111] On May 13, 2016, Boeing opened a $1billion, 27-acre (11-hectare) factory in Washington state that will make carbon-composite wings for the Boeing 777X to be delivered from 2020.[112]
On 28 April 2016, Bombardier Aerospace recorded a firm order from Delta Air Lines for 75 CSeries CS100s plus 50 options. 
On 27 April 2017, Boeing filed a petition for dumping them at $19.6m each, below their $33.2m production cost.
On 9 June 2017, the US International Trade Commission (USITC) found that the US industry could be threatened.
On 26 September, the US Department of Commerce (DoC) observed subsidies of 220% and intended to collect deposits accordingly, plus a preliminary 80% anti-dumping duty, resulting in a duty of 300%.
The DoC announced its final ruling, a total duty of 292%, on 20 December.
On 10 January 2018, the Canadian government filed a complaint at the World Trade Organization against the US.
On 26 January 2018, the four USITC commissioners unanimously determined that US industry is not threatened and no duty orders will be issued, overturning the imposed duties.
The Commission public report was made available by February 2018.
On March 22, Boeing declined to appeal the ruling. While the USITC had determined there was no threat, the ruling came too late for Bombardier, as the dumping petition by Boeing had already paved the way for Bombardier to relinquish a controlling interest in the CSeries to Airbus in October 2017.
In October 2017, Boeing announced plans to acquire Aurora Flight Sciences to expand its capabilities to develop autonomous, electric-powered and long-flight-duration aircraft for its commercial and military businesses, pending regulatory approval.[113][114]
In 2017, Boeing won 912 net orders for $134.8billion at list prices including 745737s, 94787s and 60777s, and delivered 763 airliners including 529737s, 136787s and 74777s.[115]
In January 2018, a joint venture was formed by auto seat maker Adient (50.01%) and Boeing (49.99%) to develop and manufacture airliner seats for new installations or retrofit, a $4.5billion market in 2017 which will grow to $6billion by 2026, to be based in Kaiserslautern near Frankfurt and distributed by Boeing subsidiary Aviall, with its customer service center in Seattle.[116]
On June 4, 2018, Boeing and Safran announced a 50-50 partnership to design, build and service auxiliary power units (APU) after regulatory and antitrust clearance in the second half of 2018.[117] This could threaten the dominance of Honeywell and United Technologies in the APU market.[118]
At a June 2018 AIAA conference, Boeing unveiled a hypersonic transport project.[119]
On July 5, 2018, Boeing and Embraer announced a joint venture, covering Embraer's airliner business.[120] This is seen as a reaction to Airbus acquiring a majority of the competing Bombardier CSeries on October 16, 2017.[121]
In September 2018, Boeing signed a deal with the Pentagon worth up to $2.4billion to provide helicopters for protecting nuclear-missile bases.[122] Boeing acquired the satellite company Millennium Space System in September 2018.[citation needed]
On March 10, 2019, an Ethiopian Airlines Boeing 737 MAX 8 crashed just minutes after take-off from Addis Ababa. Initial reports noted similarities with the crash of a Lion Air MAX 8 in October 2018. In the following days, numerous countries and airlines grounded all 737 MAX aircraft.[123] On March 13, the FAA became the last major authority to ground the aircraft, reversing its previous stance that the MAX was safe to fly.[124] On March 19, the U.S. Department of Transportation requested an audit of the regulatory process that led to the aircraft's certification in 2017,[125][126] amid concerns that current U.S. rules allow manufacturers to largely "self-certify" aircraft.[127] During March 2019 Boeing's share price dropped significantly.[128] In May 2019 Boeing admitted that it had known of issues with the 737 MAX before the second crash, and only informed the Federal Aviation Authority of the software issue a month after the Lion Air crash.[129][130]
On April 23, 2019, the Wall Street Journal reported that Boeing, SSL and aerospace company The Carlyle Group had been helping the Chinese Peoples Liberation Army enable its mass surveillance on ethnic groups such as the Uighur Muslims in the Xinjiang autonomous region in northwestern China as well as giving a high-speed internet access to the artificial islands in the South China sea among others through the use of new satellites. The companies have been selling the new satellites to a Chinese company called AsiaSat which is a joint-venture between the Carlyle Group and the Chinese State-owned CITIC which then sells space on these satellites to Chinese companies. The companies stated that they never specifically intended for their technology to be used by China's Ministry of Public Security and the Police.[131]
On July 18, 2019, when presenting its second-quarter results, Boeing announced that it had recorded a $4.9 billion after-tax charge corresponding to its initial estimate of the cost of compensation to airlines for the 737 MAX groundings, but not the cost of lawsuits, potential fines, or the less tangible cost to its reputation. It also noted a $1.7 billion rise in estimated MAX production costs, primarily due to higher costs associated with the reduced production rate.[132][133]
On November 18, 2019, Boeing (49%) and Embraer (51%) announced a joint venture to market the C-390 Millennium tactical transport aircraft, called Boeing Embraer  Defense, to operate after the regulatory approvals and closing conditions.[134]
The joint ventures with Embraer, for commercial and military aircraft, were both canceled in April 2020, as Boeing was heavily affected financially by the grounding of the 737 MAX and the impact of the 201920 coronavirus pandemic on aviation.[135] Boeing claimed that Embraer had failed to meet required conditions by April 24,[136] while Embraer accused Boeing of manufacturing "false claims" in order to avoid its commitments, and stated that it would pursue "all remedies against Boeing for the damages incurred".[137] The Master Teaming Agreement for marketing of the C-390 continued, though the prospects of international sales facilitated by Boeing diminished.[138]
In late April 2020, due to the 737 MAX grounding, Boeing left behind studies for the New Midsized Airplane/797 in favor of refreshments of its geriatric 757 and 767, alternatively called the 757-Plus and 767X.[139] In May 2020, the company cut 12,000 jobs due to the drop in air travel during the COVID-19 pandemic.[140] In July 2020, Boeing reported a loss of $2.4 billion as a result of the pandemic and grounding of the 737 MAX aircraft. As a result of the profit loss, the company announced that it is planning to do more job and production cuts.[141]
In May 2006, four concept designs being examined by Boeing were outlined in The Seattle Times based on corporate internal documents. The research aims in two directions: low-cost airplanes, and environmental-friendly planes. Codenamed after some of The Muppets characters, a design team known as the Green Team concentrated primarily on reducing fuel usage. All four designs illustrated rear-engine layouts.[142]
As with most concepts, these designs are only in the exploratory stage, intended to help Boeing evaluate the potentials of such radical technologies.[142]
In 2015, Boeing patented its own force field technology, also known as the shock wave attenuation system, that would protect vehicles from shock waves generated by nearby explosions.[143] Boeing has yet to confirm when they plan to build and test the technology.[144]
The Boeing Yellowstone Project is the company's project to replace its entire civil aircraft portfolio with advanced technology aircraft. New technologies to be introduced include composite aerostructures, more electrical systems (reduction of hydraulic systems), and more fuel-efficient turbofan engines, such as the Pratt & Whitney PW1000G Geared Turbofan, General Electric GEnx, the CFM International LEAP56, and the Rolls-Royce Trent 1000. The term "Yellowstone" refers to the technologies, while "Y1" through "Y3" refer to the actual aircraft.
Notes
References

Art history is the study of aesthetic objects and visual expression in historical and stylistic context.[1] Traditionally, the discipline of art history emphasized painting, drawing, sculpture, architecture, ceramics and decorative arts, yet today, art history examines broader aspects of visual culture, including the various visual and conceptual outcomes related to an ever-evolving definition of art.[2][3] Art history encompasses the study of objects created by different cultures around the world and throughout history that convey meaning, importance or serve usefulness primarily through visual representations.
As a discipline, art history is distinguished from art criticism, which is concerned with establishing a relative artistic value upon individual works with respect to others of comparable style or sanctioning an entire style or movement; and art theory or "philosophy of art", which is concerned with the fundamental nature of art. One branch of this area of study is aesthetics, which includes investigating the enigma of the sublime and determining the essence of beauty. Technically, art history is not these things, because the art historian uses historical method to answer the questions: How did the artist come to create the work?, Who were the patrons?, Who were their teachers?, Who was the audience?, Who were their disciples?, What historical forces shaped the artist's oeuvre and how did he or she and the creation, in turn, affect the course of artistic, political and social events? It is, however, questionable whether many questions of this kind can be answered satisfactorily without also considering basic questions about the nature of art. The current disciplinary gap between art history and the philosophy of art (aesthetics) often hinders this inquiry.[4]
Art of Asia
Art of Europe
Art of Africa
Art of the Americas
Art of Australia
Art history is an interdisciplinary practice that analyzes the various factorscultural, political, religious, economic or artisticwhich contribute to visual appearance of a work of art.
Art historians employ a number of methods in their research into the ontology and history of objects.
Art historians often examine work in the context of its time. At best, this is done in a manner which respects its creator's motivations and imperatives; with consideration of the desires and prejudices of its patrons and sponsors; with a comparative analysis of themes and approaches of the creator's colleagues and teachers; and with consideration of iconography and symbolism. In short, this approach examines the work of art in the context of the world within which it was created.
Art historians also often examine work through an analysis of form; that is, the creator's use of line, shape, color, texture and composition. This approach examines how the artist uses a two-dimensional picture plane or the three dimensions of sculptural or architectural space to create their art. The way these individual elements are employed results in representational or non-representational art. Is the artist imitating an object or can the image be found in nature? If so, it is representational. The closer the art hews to perfect imitation, the more the art is realistic. Is the artist not imitating, but instead relying on symbolism or in an important way striving to capture nature's essence, rather than copy it directly? If so the art is non-representationalalso called abstract. Realism and abstraction exist on a continuum. Impressionism is an example of a representational style that was not directly imitative, but strove to create an "impression" of nature. If the work is not representational and is an expression of the artist's feelings, longings and aspirations or is a search for ideals of beauty and form, the work is non-representational or a work of expressionism.
An iconographical analysis is one which focuses on particular design elements of an object. Through a close reading of such elements, it is possible to trace their lineage, and with it draw conclusions regarding the origins and trajectory of these motifs. In turn, it is possible to make any number of observations regarding the social, cultural, economic and aesthetic values of those responsible for producing the object.
Many art historians use critical theory to frame their inquiries into objects. Theory is most often used when dealing with more recent objects, those from the late 19th century onward. Critical theory in art history is often borrowed from literary scholars and it involves the application of a non-artistic analytical framework to the study of art objects. Feminist, Marxist, critical race, queer and postcolonial theories are all well established in the discipline. As in literary studies, there is an interest among scholars in nature and the environment, but the direction that this will take in the discipline has yet to be determined.

The earliest surviving writing on art that can be classified as art history are the passages in Pliny the Elder's Natural History (c. AD 77-79), concerning the development of Greek sculpture and painting.[5] From them it is possible to trace the ideas of Xenokrates of Sicyon (c. 280 BC), a Greek sculptor who was perhaps the first art historian.[6] Pliny's work, while mainly an encyclopaedia of the sciences, has thus been influential from the Renaissance onwards. (Passages about techniques used by the painter Apelles c. (332-329 BC), have been especially well-known.) Similar, though independent, developments occurred in the 6th century China, where a canon of worthy artists was established by writers in the scholar-official class. These writers, being necessarily proficient in calligraphy, were artists themselves. The artists are described in the Six Principles of Painting formulated by Xie He.[7]
While personal reminiscences of art and artists have long been written and read (see Lorenzo Ghiberti Commentarii, for the best early example),[8] it was Giorgio Vasari, the Tuscan painter, sculptor and author of the Lives of the Most Excellent Painters, Sculptors, and Architects, who wrote the first true history of art.[9] He emphasized art's progression and development, which was a milestone in this field. His was a personal and a historical account, featuring biographies of individual Italian artists, many of whom were his contemporaries and personal acquaintances. The most renowned of these was Michelangelo, and Vasari's account is enlightening, though biased[citation needed] in places.
Vasari's ideas about art were enormously influential, and served as a model for many, including in the north of Europe Karel van Mander's Schilder-boeck and Joachim von Sandrart's Teutsche Akademie.[citation needed] Vasari's approach held sway until the 18th century, when criticism was leveled at his biographical account of history.[citation needed]
Scholars such as Johann Joachim Winckelmann (17171768), criticized Vasari's "cult" of artistic personality, and they argued that the real emphasis in the study of art should be the views of the learned beholder and not the unique viewpoint of the charismatic artist. Winckelmann's writings thus were the beginnings of art criticism. His two most notable works that introduced the concept of art criticism were Gedanken ber die Nachahmung der griechischen Werke in der Malerei und Bildhauerkunst, published in 1755, shortly before he left for Rome (Fuseli published an English translation in 1765 under the title Reflections on the Painting and Sculpture of the Greeks), and Geschichte der Kunst des Altertums (History of Art in Antiquity), published in 1764 (this is the first occurrence of the phrase history of art in the title of a book)".[10] Winckelmann critiqued the artistic excesses of Baroque and Rococo forms, and was instrumental in reforming taste in favor of the more sober Neoclassicism. Jacob Burckhardt (18181897), one of the founders of art history, noted that Winckelmann was 'the first to distinguish between the periods of ancient art and to link the history of style with world history'. From Winckelmann until the mid-20th century, the field of art history was dominated by German-speaking academics. Winckelmann's work thus marked the entry of art history into the high-philosophical discourse of German culture.
Winckelmann was read avidly by Johann Wolfgang Goethe and Friedrich Schiller, both of whom began to write on the history of art, and his account of the Laocon group occasioned a response by Lessing. The emergence of art as a major subject of philosophical speculation was solidified by the appearance of Immanuel Kant's Critique of Judgment in 1790, and was furthered by Hegel's Lectures on Aesthetics. Hegel's philosophy served as the direct inspiration for Karl Schnaase's work. Schnaase's Niederlndische Briefe established the theoretical foundations for art history as an autonomous discipline, and his Geschichte der bildenden Knste, one of the first historical surveys of the history of art from antiquity to the Renaissance, facilitated the teaching of art history in German-speaking universities. Schnaase's survey was published contemporaneously with a similar work by Franz Theodor Kugler.
Heinrich Wlfflin (18641945), who studied under Burckhardt in Basel, is the "father" of modern art history. Wlfflin taught at the universities of Berlin, Basel, Munich, and Zurich. A number of students went on to distinguished careers in art history, including Jakob Rosenberg and Frida Schottmuller. He introduced a scientific approach to the history of art, focusing on three concepts. Firstly, he attempted to study art using psychology, particularly by applying the work of Wilhelm Wundt. He argued, among other things, that art and architecture are good if they resemble the human body. For example, houses were good if their faades looked like faces. Secondly, he introduced the idea of studying art through comparison. By comparing individual paintings to each other, he was able to make distinctions of style. His book Renaissance and Baroque developed this idea, and was the first to show how these stylistic periods differed from one another. In contrast to Giorgio Vasari, Wlfflin was uninterested in the biographies of artists. In fact he proposed the creation of an "art history without names." Finally, he studied art based on ideas of nationhood. He was particularly interested in whether there was an inherently "Italian" and an inherently "German" style. This last interest was most fully articulated in his monograph on the German artist Albrecht Drer.
Contemporaneous with Wlfflin's career, a major school of art-historical thought developed at the University of Vienna. The first generation of the Vienna School was dominated by Alois Riegl and Franz Wickhoff, both students of Moritz Thausing, and was characterized by a tendency to reassess neglected or disparaged periods in the history of art. Riegl and Wickhoff both wrote extensively on the art of late antiquity, which before them had been considered as a period of decline from the classical ideal. Riegl also contributed to the revaluation of the Baroque.
The next generation of professors at Vienna included Max Dvok, Julius von Schlosser, Hans Tietze, Karl Maria Swoboda, and Josef Strzygowski. A number of the most important twentieth-century art historians, including Ernst Gombrich, received their degrees at Vienna at this time. The term "Second Vienna School" (or "New Vienna School") usually refers to the following generation of Viennese scholars, including Hans Sedlmayr, Otto Pcht, and Guido Kaschnitz von Weinberg. These scholars began in the 1930s to return to the work of the first generation, particularly to Riegl and his concept of Kunstwollen, and attempted to develop it into a full-blown art-historical methodology. Sedlmayr, in particular, rejected the minute study of iconography, patronage, and other approaches grounded in historical context, preferring instead to concentrate on the aesthetic qualities of a work of art. As a result, the Second Vienna School gained a reputation for unrestrained and irresponsible formalism, and was furthermore colored by Sedlmayr's overt racism and membership in the Nazi party. This latter tendency was, however, by no means shared by all members of the school; Pcht, for example, was himself Jewish, and was forced to leave Vienna in the 1930s.
Our 21st-century understanding of the symbolic content of art comes from a group of scholars who gathered in Hamburg in the 1920s. The most prominent among them were Erwin Panofsky, Aby Warburg, Fritz Saxl and Gertrud Bing. Together they developed much of the vocabulary that continues to be used in the 21st century by art historians. "Iconography"with roots meaning "symbols from writing" refers to subject matter of art derived from written sourcesespecially scripture and mythology. "Iconology" is a broader term that referred to all symbolism, whether derived from a specific text or not. Today art historians sometimes use these terms interchangeably.
Panofsky, in his early work, also developed the theories of Riegl, but became eventually more preoccupied with iconography, and in particular with the transmission of themes related to classical antiquity in the Middle Ages and Renaissance. In this respect his interests coincided with those of Warburg, the son of a wealthy family who had assembled an impressive library in Hamburg devoted to the study of the classical tradition in later art and culture. Under Saxl's auspices, this library was developed into a research institute, affiliated with the University of Hamburg, where Panofsky taught.
Warburg died in 1929, and in the 1930s Saxl and Panofsky, both Jewish, were forced to leave Hamburg. Saxl settled in London, bringing Warburg's library with him and establishing the Warburg Institute. Panofsky settled in Princeton at the Institute for Advanced Study. In this respect they were part of an extraordinary influx of German art historians into the English-speaking academy in the 1930s. These scholars were largely responsible for establishing art history as a legitimate field of study in the English-speaking world, and the influence of Panofsky's methodology, in particular, determined the course of American art history for a generation.
Heinrich Wlfflin was not the only scholar to invoke psychological theories in the study of art. Psychoanalyst Sigmund Freud wrote a book on the artist Leonardo da Vinci, in which he used Leonardo's paintings to interrogate the artist's psyche and sexual orientation. Freud inferred from his analysis that Leonardo was probably homosexual.
Though the use of posthumous material to perform psychoanalysis is controversial among art historians, especially since the sexual mores of Leonardo's time and Freud's are different, it is often attempted. One of the best-known psychoanalytic scholars is Laurie Schneider Adams, who wrote a popular textbook, Art Across Time, and a book Art and Psychoanalysis.
An unsuspecting turn for the history of art criticism came in 1914 when Sigmund Freud published a psychoanalytical interpretation of Michelangelo's Moses titled Der Moses des Michelangelo as one of the first psychology based analyses on a work of art.[11] Freud first published this work shortly after reading Vasari's Lives. For unknown purposes, Freud originally published the article anonymously.
Carl Jung also applied psychoanalytic theory to art. C.G. Jung was a Swiss psychiatrist, an influential thinker, and founder of analytical psychology. Jung's approach to psychology emphasized understanding the psyche through exploring the worlds of dreams, art, mythology, world religion and philosophy. Much of his life's work was spent exploring Eastern and Western philosophy, alchemy, astrology, sociology, as well as literature and the arts. His most notable contributions include his concept of the psychological archetype, the collective unconscious, and his theory of synchronicity. Jung believed that many experiences perceived as coincidence were not merely due to chance but, instead, suggested the manifestation of parallel events or circumstances reflecting this governing dynamic.[12] He argued that a collective unconscious and archetypal imagery were detectable in art. His ideas were particularly popular among American Abstract expressionists in the 1940s and 1950s.[13] His work inspired the surrealist concept of drawing imagery from dreams and the unconscious.
Jung emphasized the importance of balance and harmony. He cautioned that modern humans rely too heavily on science and logic and would benefit from integrating spirituality and appreciation of the unconscious realm. His work not only triggered analytical work by art historians, but it became an integral part of art-making. Jackson Pollock, for example, famously created a series of drawings to accompany his psychoanalytic sessions with his Jungian psychoanalyst, Dr. Joseph Henderson. Henderson who later published the drawings in a text devoted to Pollock's sessions realized how powerful the drawings were as a therapeutic tool.[14]
The legacy of psychoanalysis in art history has been profound, and extends beyond Freud and Jung. The prominent feminist art historian Griselda Pollock, for example, draws upon psychoanalysis both in her reading into contemporary art and in her rereading of modernist art. With Griselda Pollock's reading of French feminist psychoanalysis and in particular the writings of Julia Kristeva and Bracha L. Ettinger, as with Rosalind Krauss readings of Jacques Lacan and Jean-Franois Lyotard and Catherine de Zegher's curatorial rereading of art, Feminist theory written in the fields of French feminism and Psychoanalysis has strongly informed the reframing of both men and women artists in art history.
During the mid-20th century, art historians embraced social history by using critical approaches. The goal was to show how art interacts with power structures in society. One critical approach that art historians[who?] used was Marxism. Marxist art history attempted to show how art was tied to specific classes, how images contain information about the economy, and how images can make the status quo seem natural (ideology).[citation needed]
Marcel Duchamp and Dada Movement jump started the Anti-art style. Various artist did not want to create artwork that everyone was conforming to at the time. These two movements helped other artist to create pieces that were not viewed as traditional art. Some examples of styles that branched off the anti-art movement would be Neo-Dadaism, Surrealism, and Constructivism. These styles and artist did not want to surrender to traditional ways of art. This way of thinking provoked political movements such as the Russian Revolution and the communist ideals.[15]
Artist Isaak Brodsky work of art 'Shock-worker from Dneprstroi' in 1932 shows his political involvement within art. This piece of art can be analysed to show the internal troubles Soviet Russia was experiencing at the time.
Perhaps the best-known Marxist was Clement Greenberg, who came to prominence during the late 1930s with his essay "Avant-Garde and Kitsch".[16] In the essay Greenberg claimed that the avant-garde arose in order to defend aesthetic standards from the decline of taste involved in consumer society, and seeing kitsch and art as opposites. Greenberg further claimed that avant-garde and Modernist art was a means to resist the leveling of culture produced by capitalist propaganda. Greenberg appropriated the German word 'kitsch' to describe this consumerism, although its connotations have since changed to a more affirmative notion of leftover materials of capitalist culture. Greenberg later[when?] became well known for examining the formal properties of modern art.[citation needed]
Meyer Schapiro is one of the best-remembered Marxist art historians of the mid-20th century. Although he wrote about numerous time periods and themes in art, he is best remembered for his commentary on sculpture from the late Middle Ages and early Renaissance, at which time he saw evidence of capitalism emerging and feudalism declining.[citation needed]
Arnold Hauser wrote the first Marxist survey of Western Art, entitled The Social History of Art. He attempted to show how class consciousness was reflected in major art periods. The book was controversial when published during the 1950s since it makes generalizations about entire eras, a strategy now called "vulgar Marxism".[citation needed]
Marxist Art History was refined in the department of Art History at UCLA with scholars such as T.J. Clark, O.K. Werckmeister, David Kunzle, Theodor W. Adorno, and Max Horkheimer. T.J. Clark was the first art historian writing from a Marxist perspective to abandon vulgar Marxism. He wrote Marxist art histories of several impressionist and realist artists, including Gustave Courbet and douard Manet. These books focused closely on the political and economic climates in which the art was created.[17]
Linda Nochlin's essay "Why Have There Been No Great Women Artists?" helped to ignite feminist art history during the 1970s and remains one of the most widely read essays about female artists. This was then followed by a 1972 College Art Association Panel, chaired by Nochlin, entitled "Eroticism and the Image of Woman in Nineteenth-Century Art". Within a decade, scores of papers, articles, and essays sustained a growing momentum, fueled by the Second-wave feminist movement, of critical discourse surrounding women's interactions with the arts as both artists and subjects.  In her pioneering essay, Nochlin applies a feminist critical framework to show systematic exclusion of women from art training, arguing that exclusion from practicing art as well as the canonical history of art was the consequence of cultural conditions which curtailed and restricted women from art producing fields.[18] The few who did succeed were treated as anomalies and did not provide a model for subsequent success. Griselda Pollock is another prominent feminist art historian, whose use of psychoanalytic theory is described above.
While feminist art history can focus on any time period and location, much attention has been given to the Modern era. Some of this scholarship centers on the feminist art movement, which referred specifically to the experience of women. Often, feminist art history offers a critical "re-reading" of the Western art canon, such as Carol Duncan's re-interpretation of Les Demoiselles d'Avignon.  Two pioneers of the field are Mary Garrard and Norma Broude. Their anthologies Feminism and Art History: Questioning the Litany, The Expanding Discourse: Feminism and Art History, and Reclaiming Feminist Agency: Feminist Art History After Postmodernism are substantial efforts to bring feminist perspectives into the discourse of art history.  The pair also co-founded the Feminist Art History Conference.[19]
As opposed to iconography which seeks to identify meaning, semiotics is concerned with how meaning is created. Roland Barthes's connoted and denoted meanings are paramount to this examination. In any particular work of art, an interpretation depends on the identification of denoted meaning[20]the recognition of a visual sign, and the connoted meaning[21]the instant cultural associations that come with recognition. The main concern of the semiotic art historian is to come up with ways to navigate and interpret connoted meaning.[22]
Semiotic art history seeks to uncover the codified meaning or meanings in an aesthetic object by examining its connectedness to a collective consciousness.[23] Art historians do not commonly commit to any one particular brand of semiotics but rather construct an amalgamated version which they incorporate into their collection of analytical tools. For example, Meyer Schapiro borrowed Saussure's differential meaning in effort to read signs as they exist within a system.[24] According to Schapiro, to understand the meaning of frontality in a specific pictorial context, it must be differentiated from, or viewed in relation to, alternate possibilities such as a profile, or a three-quarter view. Schapiro combined this method with the work of Charles Sanders Peirce whose object, sign, and interpretant provided a structure for his approach. Alex Potts demonstrates the application of Peirce's concepts to visual representation by examining them in relation to the Mona Lisa. By seeing the Mona Lisa, for example, as something beyond its materiality is to identify it as a sign. It is then recognized as referring to an object outside of itself, a woman, or Mona Lisa. The image does not seem to denote religious meaning and can therefore be assumed to be a portrait. This interpretation leads to a chain of possible interpretations: who was the sitter in relation to Leonardo da Vinci? What significance did she have to him? Or, maybe she is an icon for all of womankind. This chain of interpretation, or unlimited semiosis is endless; the art historian's job is to place boundaries on possible interpretations as much as it is to reveal new possibilities.[25]
Semiotics operates under the theory that an image can only be understood from the viewer's perspective. The artist is supplanted by the viewer as the purveyor of meaning, even to the extent that an interpretation is still valid regardless of whether the creator had intended it.[25] Rosalind Krauss espoused this concept in her essay In the Name of Picasso. She denounced the artist's monopoly on meaning and insisted that meaning can only be derived after the work has been removed from its historical and social context. Mieke Bal argued similarly that meaning does not even exist until the image is observed by the viewer. It is only after acknowledging this that meaning can become opened up to other possibilities such as feminism or psychoanalysis.[26]
Aspects of the subject which have come to the fore in recent decades include interest in the patronage and consumption of art, including the economics of the art market, the role of collectors, the intentions and aspirations of those commissioning works, and the reactions of contemporary and later viewers and owners. Museum studies, including the history of museum collecting and display, is now a specialized field of study, as is the history of collecting.
Scientific advances have made possible much more accurate investigation of the materials and techniques used to create works, especially infra-red and x-ray photographic techniques which have allowed many underdrawings of paintings to be seen again. Proper analysis of pigments used in paint is now possible, which has upset many attributions. Dendrochronology for panel paintings and radio-carbon dating for old objects in organic materials have allowed scientific methods of dating objects to confirm or upset dates derived from stylistic analysis or documentary evidence. The development of good colour photography, now held digitally and available on the internet or by other means, has transformed the study of many types of art, especially those covering objects existing in large numbers which are widely dispersed among collections, such as illuminated manuscripts and Persian miniatures, and many types of archaeological artworks.
Concurrent to those technological advances, art historians have shown increasing interest in new theoretical approaches to the nature of artworks as objects. Thing theory, actornetwork theory, and object-oriented ontology have played an increasing role in art historical literature.
The making of art, the academic history of art, and the history of art museums are closely intertwined with the rise of nationalism. Art created in the modern era, in fact, has often been an attempt to generate feelings of national superiority or love of one's country. Russian art is an especially good example of this, as the Russian avant-garde and later Soviet art were attempts to define that country's identity.
Most art historians working today identify their specialty as the art of a particular culture and time period, and often such cultures are also nations. For example, someone might specialize in the 19th-century German or contemporary Chinese art history. A focus on nationhood has deep roots in the discipline. Indeed, Vasari's Lives of the Most Excellent Painters, Sculptors, and Architects is an attempt to show the superiority of Florentine artistic culture, and Heinrich Wlfflin's writings (especially his monograph on Albrecht Drer) attempt to distinguish Italian from German styles of art.
Many of the largest and most well-funded art museums of the world, such as the Louvre, the Victoria and Albert Museum, and the National Gallery of Art in Washington are state-owned. Most countries, indeed, have a national gallery, with an explicit mission of preserving the cultural patrimony owned by the governmentregardless of what cultures created the artand an often implicit mission to bolster that country's own cultural heritage. The National Gallery of Art thus showcases art made in the United States, but also owns objects from across the world.
The discipline of art history is traditionally divided into specializations or concentrations based on eras and regions, with further sub-division based on media. Thus, someone might specialize in "19th-century German architecture" or in "16th-century Tuscan sculpture." Sub-fields are often included under a specialization. For example, the Ancient Near East, Greece, Rome, and Egypt are all typically considered special concentrations of Ancient art. In some cases, these specializations may be closely allied (as Greece and Rome, for example), while in others such alliances are far less natural (Indian art versus Korean art, for example).
Non-Western or global perspectives on art have become increasingly predominant in the art historical canon since the 1980s.
"Contemporary art history" refers to research into the period from the 1960s until today reflecting the break from the assumptions of modernism brought by artists of the neo-avant-garde[27] and a continuity in contemporary art in terms of practice based on conceptualist and post-conceptualist practices.
In the United States, the most important art history organization is the College Art Association.[28] It organizes an annual conference and publishes the Art Bulletin and Art Journal. Similar organizations exist in other parts of the world, as well as for specializations, such as architectural history and Renaissance art history. In the UK, for example, the Association of Art Historians is the premiere organization, and it publishes a journal titled Art History.[29]


The history of cotton can be traced to domestication. Cotton played an important role in the history of India, the British Empire, and the United States, and continues to be an important crop and commodity.
The history of the domestication of cotton is very complex and is not known exactly.[1] Several isolated civilizations in both the Old and New World independently domesticated and converted cotton into fabric. All the same tools were invented to work it also, including combs, bows, hand spindles, and primitive looms.[2]:1113
The word "cotton" has Arabic origins, derived from the Arabic word  (qutn or qutun). This was the usual word for cotton in medieval Arabic.[3] The word entered the Romance languages in the mid-12th century,[4] and English a century later. Cotton fabric was known to the ancient Romans as an import but cotton was rare in the Romance-speaking lands until imports from the Arabic-speaking lands in the later medieval era at transformatively lower prices.[5][6]
The oldest cotton textiles were found in graves and city ruins of civilizations from dry climates, where the fabrics did not decay completely.[7]
The oldest cotton fabric has been found in Huaca Prieta in Peru, dated to about 6000 BCE. It is here that Gossypium barbadense is thought to have been domesticated at its earliest.[8][9] Some of the oldest cotton bolls were discovered in a cave in Tehuacn Valley, Mexico, and were dated to approximately 5500 BCE, but some doubt has been cast on these estimates. Seeds and cordage dating to about 2500 BCE have been found in Peru.[1] By 3000 BCE cotton was being grown and processed in Mexico, and Arizona.[9]
Cotton (Gossypium herbaceum Linnaeus) may have been domesticated around 5000 BCE in eastern Sudan near the Middle Nile Basin region, where cotton cloth was being produced.[10]
The cultivation of cotton and the knowledge of its spinning and weaving in Mero reached a high level in the 4th century BC. The export of textiles was one of the sources of wealth for Mero. Aksumite King Ezana boasted in his inscription that he destroyed large cotton plantations in Mero during his conquest of the region.[11]
The latest archaeological discovery in Mehrgarh puts the dating of early cotton cultivation and the use of cotton to 5000 BCE.[12] The Indus Valley civilization started cultivating cotton by 3000 BCE.[13] Cotton was mentioned in Hindu hymns in 1500 BCE.[9]
Herodotus, an ancient Greek historian, mentions Indian cotton in the 5th century BCE as "a wool exceeding in beauty and goodness that of sheep." When Alexander the Great invaded India, his troops started wearing cotton clothes that were more comfortable than their previous woolen ones.[14] Strabo, another Greek historian, mentioned the vividness of Indian fabrics, and Arrian told of IndianArab trade of cotton fabrics in 130 CE.[15]
Handheld roller cotton gins had been used in India since the 6th century, and was then introduced to other countries from there.[16] Between the 12th and 14th centuries, dual-roller gins appeared in India and China. The Indian version of the dual-roller gin was prevalent throughout the Mediterranean cotton trade by the 16th century. This mechanical device was, in some areas, driven by water power.[17]
Egyptians grew and spun cotton from 6700 CE.[9]
Cotton was a common fabric during the Middle Ages, and was hand-woven on a loom. Cotton manufacture was introduced to Europe during the Muslim conquest of the Iberian Peninsula and Sicily. The knowledge of cotton weaving was spread to northern Italy in the 12th century, when Sicily was conquered by the Normans, and consequently to the rest of Europe. The spinning wheel, introduced to Europe circa 1350, improved the speed of cotton spinning.[18] By the 15th century, Venice, Antwerp, and Haarlem were important ports for cotton trade, and the sale and transportation of cotton fabrics had become very profitable.[14]
Christopher Columbus, in his explorations of the Bahamas and Cuba, found natives wearing cotton ("the costliest and handsomest... cotton mantles and sleeveless shirts embroidered and painted in different designs and colours"), a fact that may have contributed to his incorrect belief that he had landed on the coast of India.[2]:1113
India had been an exporter of fine cotton fabrics to other countries since the ancient times. Sources such as Marco Polo, who traveled India in the 13th century, Chinese travelers, who traveled Buddhist pilgrim centers earlier, Vasco Da Gama, who entered Calicut in 1498, and Tavernier, who visited India in the 17th century, have praised the superiority of Indian fabrics.[19]
The worm gear roller cotton gin, which was invented in India during the early Delhi Sultanate era of the 13th14th centuries, came into use in the Mughal Empire some time around the 16th century,[20] and is still used in India through to the present day.[16] Another innovation, the incorporation of the crank handle in the cotton gin, first appeared in India some time during the late Delhi Sultanate or the early Mughal Empire.[21] The production of cotton, which may have largely been spun in the villages and then taken to towns in the form of yarn to be woven into cloth textiles, was advanced by the diffusion of the spinning wheel across India shortly before the Mughal era, lowering the costs of yarn and helping to increase demand for cotton. The diffusion of the spinning wheel, and the incorporation of the worm gear and crank handle into the roller cotton gin, led to greatly expanded Indian cotton textile production during the Mughal era.[22]
It was reported that, with an Indian cotton gin, which is half machine and half tool, one man and one woman could clean 28 pounds of cotton per day. With a modified Forbes version, one man and a boy could produce 250 pounds per day. If oxen were used to power 16 of these machines, and a few people's labour was used to feed them, they could produce as much work as 750 people did formerly.[23]
During the early 16th century to the early 18th century, Indian cotton production increased, in terms of both raw cotton and cotton textiles. The Mughals introduced agrarian reforms such as a new revenue system that was biased in favour of higher value cash crops such as cotton and indigo, providing state incentives to grow cash crops, in addition to rising market demand.[24]
The largest manufacturing industry in the Mughal Empire was cotton textile manufacturing, which included the production of piece goods, calicos, and muslins, available unbleached and in a variety of colours. The cotton textile industry was responsible for a large part of the empire's international trade.[25] India had a 25% share of the global textile trade in the early 18th century.[26] Indian cotton textiles were the most important manufactured goods in world trade in the 18th century, consumed across the world from the Americas to Japan.[27] The most important center of cotton production was the Bengal Subah province, particularly around its capital city of Dhaka.[28]
Bengal accounted for more than 50% of textiles imported by the Dutch from Asia,[29] Bengali cotton textiles were exported in large quantities to Europe, Indonesia, and Japan,[30] and Bengali Muslim textiles from Dhaka were sold in Central Asia, where they were known as "daka" textiles.[28] Indian textiles dominated the Indian Ocean trade for centuries, were sold in the Atlantic Ocean trade, and had a 38% share of the West African trade in the early 18th century, while Indian calicos were a major force in Europe, and Indian textiles accounted for 20% of total English trade with Southern Europe in the early 18th century.[31]
Cotton cloth started to become highly sought-after for the European urban markets during the Renaissance and the Enlightenment.[citation needed] Vasco da Gama (d. 1524), a Portuguese explorer, opened Asian sea trade, which replaced caravans and allowed for heavier cargo. Indian craftspeople had long protected the secret of how to create colourful patterns. However, some converted to Christianity and their secret was revealed by a French Catholic priest, Father Coeurdoux (16911779). He revealed the process of creating the fabrics in France, which assisted the European textile industry.[32]
In early modern Europe, there was significant demand for cotton textiles from Mughal India.[25] European fashion, for example, became increasingly dependent on Mughal Indian textiles.[citation needed] From the late 17th century to the early 18th century, Mughal India accounted for 95% of British imports from Asia, and the Bengal Subah province alone accounted for 40% of Dutch imports from Asia.[29] In contrast, there was very little demand for European goods in Mughal India, which was largely self-sufficient, thus Europeans had very little to offer, except for some woolens, unprocessed metals and a few luxury items. The trade imbalance caused Europeans to export large quantities of gold and silver to Mughal India in order to pay for South Asian imports.[25]
Egypt under Muhammad Ali in the early 19th century had the fifth most productive cotton industry in the world, in terms of the number of spindles per capita.[33] The industry was initially driven by machinery that relied on traditional energy sources, such as animal power, water wheels, and windmills, which were also the principal energy sources in Western Europe up until around 1870.[34] It was under Muhammad Ali of Egypt in the early 19th century that steam engines were introduced to the Egyptian cotton industry.[34]
Cotton's rise to global importance came about as a result of the cultural transformation of Europe and Britain's trading empire.[15] Calico and chintz, types of cotton fabrics, became popular in Europe, and by 1664 the East India Company was importing a quarter of a million pieces into Britain.[32] By the 18th century, the middle class had become more concerned with cleanliness and fashion, and there was a demand for easily washable and colourful fabric. Wool continued to dominate the European markets, but cotton prints were introduced to Britain by the East India Company in the 1690s.[15] Imports of calicoes, cheap cotton fabrics from Kozhikode, then known as Calicut, in India, found a mass market among the poor. By 1721 these calicoes threatened British manufacturers, and Parliament passed the Calico Act that banned calicoes for clothing or domestic purposes. In 1774 the act was repealed with the invention of machines that allowed for British manufacturers to compete with Eastern fabrics.[35]
Indian cotton textiles, particularly those from Bengal, continued to maintain a competitive advantage up until the 19th century. In order to compete with India, Britain invested in labour-saving technical progress, while implementing protectionist policies such as bans and tariffs to restrict Indian imports.[36] At the same time, the East India Company's rule in India opened up a new market for British goods,[36] while the capital amassed from Bengal after its 1757 conquest was used to invest in British industries such as textile manufacturing and greatly increase British wealth.[37][38][39] British colonization also forced open the large Indian market to British goods, which could be sold in India without tariffs or duties, compared to local Indian producers, while raw cotton was imported from India without tariffs to British factories which manufactured textiles from Indian cotton, giving Britain a monopoly over India's large market and cotton resources.[40][36][41] India served as both a significant supplier of raw goods to British manufacturers and a large captive market for British manufactured goods.[42] Britain eventually surpassed India as the world's leading cotton textile manufacturer in the 19th century.[36]
The cotton industry grew under the British commercial empire. British cotton products were successful in European markets, constituting 40.5% of exports in 17841786. Britain's success was also due to its trade with its own colonies, whose settlers maintained British identities, and thus, fashions. With the growth of the cotton industry, manufacturers had to find new sources of raw cotton, and cultivation was expanded to West India.[15] High tariffs against Indian textile workshops, British power in India through the East India Company,[32] and British restrictions on Indian cotton imports[43] transformed India from the source of textiles to a source of raw cotton.[32] Cultivation was also attempted in the Caribbean and West Africa, but these attempts failed due to bad weather and poor soil. The Indian subcontinent was looked to as a possible source of raw cotton, but intra-imperial conflicts and economic rivalries prevented the area from producing the necessary supply.[15]
Cotton's versatility allowed it to be combined with linen and be made into velvet. It was cheaper than silk and could be imprinted more easily than wool, allowing for patterned dresses for women. It became the standard fashion and, because of its price, was accessible to the general public. New inventions in the 1770ssuch as the spinning jenny, the water frame, and the spinning mulemade the British Midlands into a very profitable manufacturing centre. In 17941796, British cotton goods accounted for 15.6% of Britain's exports, and in 18041806 grew to 42.3%.[15]
The Lancashire textile mills were major parts of the British industrial revolution. Their workers had poor working conditions: low wages, child labour, and 18-hour work days. Richard Arkwright created a textile empire by building a factory system powered by water, which was occasionally raided by the Luddites, weavers put out of business by the mechanization of textile production. In the 1790s, James Watt's steam power was applied to textile production, and by 1839 thousands of children worked in Manchester's cotton mills. Karl Marx, who frequently visited Lancashire, may have been influenced by the conditions of workers in these mills in writing Das Kapital.[32] Child labour was banned during the middle of the 19th century.
Anglo-French warfare in the early 1790s restricted access to continental Europe, causing the United States to become an importantand temporarily the largestconsumer for British cotton goods.[15] In 1791, U.S. cotton production was small, at only 900 thousand kilograms (2000 thousand pounds). Several factors contributed to the growth of the cotton industry in the U.S.: the increasing British demand; innovations in spinning, weaving, and steam power; inexpensive land; and a slave labour force.[44] The modern cotton gin, invented in 1793 by Eli Whitney, enormously grew the American cotton industry, which was previously limited by the speed of manual removal of seeds from the fibre,[45] and helped cotton to surpass tobacco as the primary cash crop of the South.[46] By 1801 the annual production of cotton had reached over 22 million kilograms (48.5 million pounds), and by the early 1830s the United States produced the majority of the world's cotton. Cotton also exceeded the value of all other United States exports combined.[44] The need for fertile land conducive to its cultivation led to the expansion of slavery in the United States and an early 19th-century land rush known as Alabama Fever.[47][48]
Cultivation of cotton using black slaves brought huge profits to the owners of large plantations, making them some of the wealthiest men in the U.S. prior to the Civil War. In the non-slave-owning states, farms rarely grew larger than what could be cultivated by one family due to scarcity of farm workers. In the slave states, owners of farms could buy many slaves and thus cultivate large areas of land. By the 1850s, slaves made up 50% of the population of the main cotton states: Georgia, Alabama, Mississippi, and Louisiana. Slaves were the most important asset in cotton cultivation, and their sale brought profits to slaveowners outside of cotton-cultivating areas. Thus, the cotton industry contributed significantly to the Southern upper class's support of slavery. Although the Southern small-farm owners did not grow cotton due to its lack of short-term profitability, they were still supportive of the system in the hopes of one day owning slaves.[44]
Slaves were fobidden to use for themselves commercial cotton, selected to produce fibers as white as possible, but it seems that their  use of cotton with naturally colored fibers was tolerated.[49] Ironically, today, these heirloom varieties are the subject of collectors passions but also renewed interest for high-end niche markets with the hope to produce textiles of lower environmental impact or fibers with sought-after unusual properties (e.g. UV-protection).[50]
Cotton's central place in the national economy and its international importance led Senator James Henry Hammond of South Carolina to make a famous boast in 1858 about King Cotton:
Without firing a gun, without drawing a sword, should they make war on us, we could bring the whole world to our feet... What would happen if no cotton was furnished for three years?... England would topple headlong and carry the whole civilized world with her save the South. No, you dare not to make war on cotton. No power on the earth dares to make war upon it. Cotton is king.[51]Cotton diplomacy, the idea that cotton would cause Britain and France to intervene in the Civil War, was unsuccessful.[52] It was thought that the Civil War caused the Lancashire Cotton Famine, a period between 18611865 of depression in the British cotton industry, by blocking off American raw cotton. Some, however, suggest that the Cotton Famine was mostly due to overproduction and price inflation caused by an expectation of future shortage.[53]
Prior to the Civil War, Lancashire companies issued surveys to find new cotton-growing countries if the Civil War were to occur and reduce American exports. India was deemed to be the country capable of growing the necessary amounts. Indeed, it helped fill the gap during the war, making up only 31% of British cotton imports in 1861, but 90% in 1862 and 67% in 1864.[54]
The main European purchasers, Britain and France, began to turn to Egyptian cotton. After the American Civil War ended in 1865, British and French traders abandoned Egyptian cotton and returned to cheap American exports,[55] sending Egypt into a deficit spiral that led to the country declaring bankruptcy in 1876, a key factor behind Egypt's occupation by the British Empire in 1882.
The South continued to be a one-crop economy until the 20th century, when the boll weevil struck across the South. The New Deal and World War II encouraged diversification.[46] Many ex-slaves as well as poor whites worked in the sharecropping system in serf-like conditions.[56]
The farmer said to the merchant
I need some meat and meal.
Get away from here, you son-of-a-gun,
You got boll weevils in your field.

Going to get your home, going to get your home.Boll weevils, small, cotton eating insects, entered the United States from Mexico in 1892, created 100 years of problems for the U.S. cotton industry. Many consider the boll weevil almost as important as the Civil War as an agent of change in the South, forcing economic and social changes. In total, the boll weevil is estimated to have caused $22 billion in damages. In the late 1950s, the U.S. cotton industry faced economic problems, and eradication of the boll weevil was prioritized. The Agricultural Research Service built the Boll Weevil Research Laboratory, which came up with detection traps and pheromone lures. The program was successful, and pesticide use reduced significantly while the boll weevil was eradicated in some areas.[57]
After the Cotton Famine, the European textile industry looked to new sources of raw cotton. The African colonies of West Africa and Mozambique provided a cheap supply. Taxes and extra-market means again discouraged local textile production. Working conditions were brutal, especially in the Congo, Angola, and Mozambique. Several revolts occurred, and a cotton black market created a local textile industry. In recent history, United States agricultural subsidies have depressed world prices, making it difficult for African farmers to compete.[32]
India's cotton industry struggled in the late 19th century because of unmechanized production and American dominance of raw cotton export. India, ceasing to be a major exporter of cotton goods, became the largest importer of British cotton textiles.[58]
Mohandas Gandhi believed that cotton was closely tied to Indian self-determination. In the 1920s he launched the Khadi Movement, a massive boycott of British cotton goods. He urged Indians to use simple homespun cotton textiles, khadi. Cotton became an important symbol in Indian independence. During World War II, shortages created a high demand for khadi, and 16 million yards of cloth were produced in nine months. The British Raj declared khadi subversive; damaging to the British imperial rule. Confiscation, burning of stocks, and jailing of workers resulted, which intensified resistance.[2]:309311 In the second half of the 20th century, a downturn in the European cotton industry led to a resurgence of the Indian cotton industry. India began to mechanize and was able to compete in the world market.[58]
In 1912, the British cotton industry was at its peak, producing eight billion yards of cloth. In World War I, cotton couldn't be exported to foreign markets, and some countries built their own factories, particularly Japan. By 1933 Japan introduced 24-hour cotton production and became the world's largest cotton manufacturer. Demand for British cotton slumped, and during the interwar period 345,000 workers left the industry and 800 mills closed.
India's boycott of British cotton products devastated Lancashire, and in Blackburn 74 mills closed in under four years.
In World War II, the British cotton industry saw an upturn and an increase in workers, with Lancashire mills being tasked with creating parachutes and uniforms for the war.
In the 1950s and '60s, many workers came from the Indian sub-continent and were encouraged to look for work in Lancashire. An increase in the work force allowed mill owners to introduce third (night) shifts. This resurgence in the textile industry did not last long, and by 1958, Britain had become a net importer of cotton cloth.
Modernization of the industry was attempted in 1959 with the Cotton Industry Act.
Mill closures occurred in Lancashire, and it was failing to compete with foreign industry. During the 1960s and '70s, a mill closed in Lancashire almost once a week. By the 1980s, the textile industry of North West Britain had almost disappeared.[59]
Textile mills have moved from Western Europe to, more recently, lower-wage areas. Industrial production is currently mostly located in countries like India, Bangladesh, China, and in Latin America. In these regions labour is much less expensive than in the first world, and attracts poor workers.[32] Biotechnology plays an important role in cotton agriculture as genetically modified cotton that can resist Roundup, a herbicide made by the company Monsanto, as well as repel insects.[2]:277 Organically grown cotton is becoming less prevalent in favour of synthetic fibres made from petroleum products.[2]:301
The demand for cotton has doubled since the 1980s.[60] The main producer of cotton, as of December 2016, is India, at 26%, past China at 20% and the United States at 16%.[61] The leading cotton exporter is the United States, whose production is subsidized by the government, with subsidies estimated at $14 billion between 1995 and 2003. The value of cotton lint has been decreasing for sixty years, and the value of cotton has decreased by 50% in 19972007. The global textile and clothing industry employs 23.6 million workers, of which 75% are women.[60]
Max Havelaar, a fair trade association, launched a fair trade label for cotton in 2005, the first for a non-food commodity. Working with small producers from Cameroon, Mali, and Senegal, the fair trade agreement increases substantially the price paid for goods and increases adherence to World Labour Organization conventions. A two-year period in Mali has allowed farmers to buy new agricultural supplies and cattle, and enroll their children in school.[62]

The population history of China covers the long-term pattern of population growth in China and its impact on the history of China. For recent  trends see demographics of China and China.
During the Warring States period (403221 BC), the development of private commerce, new trade routes, handicraft industries, and a money economy led to the growth of new urban centers. These centers were markedly different from the older cities, which had merely served as power bases for the nobility.[5] The use of a standardized, nationwide currency during the Qin dynasty (221206 BC) facilitated long-distance trade between cities.[6]  Many Han cities grew large: the Western Han capital, Chang'an, had approximately 250,000 inhabitants, while the Eastern Han capital, Luoyang, had approximately 500,000 inhabitants.[7] The population of the Han Empire, recorded in the tax census of 2 AD, was 57.6 million people in 12,366,470 households.[8] The majority of commoners who populated the cities lived in extended urban and suburban areas outside the city walls and gatehouses.[9]
Demographic historian Angus Maddison uses extensive data to argue that the main base of the Chinese economy shifted southwards between about 750 AD and 1250 AD. In 750 three quarters of the population lived in the  rural north, growing wheat and millet. By about 1250 three quarters lived south of the Yangtze and grew mainly rice. By 1000 AD per capita income in China was higher than the Europe average at the same time. Divergence took place from fifteenth and eighteenth centuries as the European economy grew faster.  From 1250 to 1900 China saw a fourfold increase in population whilst maintaining an average per capita income more or less stable. The main explanation were peace, irrigation and fast ripening seeds that permitted two crops a year. Chinese total GDP grew faster than that of western Europe from 1700 to 1820, even though European per capita income grew faster.[10][11]
Sinologist historians debate the population figures for each era in the Ming dynasty. The historian Timothy Brook notes that the Ming government census figures are dubious since fiscal obligations prompted many families to underreport the number of people in their households and many county officials to underreport the number of households in their jurisdiction.[12] Children were often underreported, especially female children, as shown by skewed population statistics throughout the Ming. Even adult women were underreported; for example, the Daming Prefecture in North Zhili reported a population of 378,167 males and 226,982 females in 1502.[13] The government attempted to revise the census figures using estimates of the expected average number of people in each household, but this did not solve the widespread problem of tax registration. Some part of the gender imbalance may be attributed to the practice of female infanticide. The practice is well documented in China, going back over two thousand years, and it was described as "rampant" and "practiced by almost every family" by contemporary authors.[14] However, the dramatically skewed sex ratios, which many counties reported exceeding 2:1 by 1586, cannot likely be explained by infanticide alone.[15]
The number of people counted in the census of 1381 was 59,873,305; however, this number dropped significantly when the government found that some 3million people were missing from the tax census of 1391.  Even though underreporting figures was made a capital crime in 1381, the need for survival pushed many to abandon the tax registration and wander from their region, where Hongwu had attempted to impose rigid immobility on the populace. The government tried to mitigate this by creating their own conservative estimate of 60,545,812 people in 1393.[17]  In his Studies on the Population of China, Ho Ping-ti suggests revising the 1393 census to 65million people, noting that large areas of North China and frontier areas were not counted in that census.[18] Brook states that the population figures gathered in the official censuses after 1393 ranged between 51 and 62million, while the population was in fact increasing.[19] Even the Hongzhi Emperor (r. 14871505) remarked that the daily increase in subjects coincided with the daily dwindling amount of registered civilians and soldiers.[20] William Atwell states that around 1400 the population of China was perhaps 90million people, citing Heijdra and Mote.[21]
Historians are now turning to local gazetteers of Ming China for clues that would show consistent growth in population.  Using the gazetteers, Brook estimates that the overall population under the Chenghua Emperor (r. 146487) was roughly 75million, despite mid-Ming census figures hovering around 62million. While prefectures across the empire in the mid-Ming period were reporting either a drop in or stagnant population size, local gazetteers reported massive amounts of incoming vagrant workers with not enough good cultivated land for them to till, so that many would become drifters, conmen, or wood-cutters that contributed to deforestation. The Hongzhi and Zhengde emperors lessened the penalties against those who had fled their home region, while the Jiajing Emperor (r. 152167) finally had officials register migrants wherever they had moved or fled in order to bring in more revenues.[22]
Even with the Jiajing reforms to document migrant workers and merchants, by the late Ming era the government census still did not accurately reflect the enormous growth in population. Gazetteers across the empire noted this and made their own estimations of the overall population in the Ming, some guessing that it had doubled, tripled, or even grown fivefold since 1368. Fairbank estimates that the population was perhaps 160million in the late Ming dynasty,[23] while Brook estimates 175million,[24]  and Ebrey states perhaps as large as 200million.[25] However, a great epidemic that entered China through the northwest in 1641 ravaged the densely populated areas along the Grand Canal; a gazetteer in northern Zhejiang noted more than half the population fell ill that year and that 90% of the local populace in one area was dead by 1642.[26]
The most significant facts of early and mid-Qing social history was growth in population, population density, and mobility. The population in 1700, according to widely accepted estimates, was roughly 150million, about what it had been under the late Ming a century before, then doubled over the next century, and reached a height of 450million on the eve of the Taiping Rebellion in 1850.[27] The food supply increased due to better irrigation and especially the introduction of fast-maturing rice seeds, which permitted harvesting two or even three crops a year on the same land. An additional factor was the spread of New World crops like peanuts, potatoes, and especially sweet potatoes. They helped to sustain the people during shortages of harvest for crops such as rice or wheat.  These crops could be grown under harsher conditions, and thus were cheaper as well, which led to them becoming staples for poorer farmers, decreasing the number of deaths from malnutrition. Diseases such as smallpox, widespread in the seventeenth century, were brought under control by an increase in inoculations. In addition, infant deaths were also greatly decreased due to improvements in birthing techniques and childcare performed by midwives and doctors. Government campaigns lowered the incidence of infanticide. Unlike Europe, where numerical growth in this period was greatest in the cities, in China the growth in cities and the lower Yangzi was low. The greatest growth was in the borderlands and the highlands, where farmers could clear large tracts of marshlands and forests.[28]
The population was also remarkably mobile, perhaps more so than at any time in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Millions of Han Chinese migrated to Yunnan and Guizhou in the 18th century, and also to Taiwan. After the conquests of the 1750s and 1760s, the court organized agricultural colonies in Xinjiang. Migration might be permanent, for resettlement, or the migrants (in theory at least) might regard the move as a temporary sojourn. The latter included an increasingly large and mobile workforce. Local-origin-based merchant groups also moved freely. This mobility also included the organized movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.[29]
Chinese scholars had kept count of 1,828 instances of famine from 108BC to 1911 in one province or anotheran average of close to one famine per year. From 1333 to 1337 a famine in the north killed 6 million Chinese. The four famines of 1810, 1811, 1846, and 1849 cost perhaps 45 million lives.[30][31]
The period from 1850 to 1873 saw, as a result of the Taiping Rebellion, drought, and famine, the population of China drop by over 30 million people.[32] China's Qing Dynasty bureaucracy, which devoted extensive attention to minimizing famines, is credited with averting a series of famines following El Nio-Southern Oscillation-linked droughts and floods. These events are comparable, though somewhat smaller in scale, to the ecological trigger events of China's vast 19th-century famines.[33] Qing China carried out its relief efforts, which included vast shipments of food, a requirement that the rich open their storehouses to the poor, and price regulation, as part of a state guarantee of subsistence to the peasantry (known as ming-sheng).
When a stressed monarchy shifted from state management and direct shipments of grain to monetary charity in the mid-19th century, the system broke down. Thus the 186768 famine under the Tongzhi Restoration was successfully relieved but the Great North China Famine of 187778, caused by drought across northern China, was a catastrophe. The province of Shanxi was substantially depopulated as grains ran out, and desperately starving people stripped forests, fields, and their very houses for food. Estimated mortality is 9.5 to 13 million people.[34]
The largest famine of the 20th century, and almost certainly of all time, was the 19581961 famine associated with the Great Leap Forward in China. The immediate causes of this famine lay in Mao Zedong's ill-fated attempt to transform China from an agricultural nation to an industrial power in one huge leap. Communist Party cadres across China insisted that peasants abandon their farms for collective farms, and begin to produce steel in small foundries, often melting down their farm instruments in the process. Collectivisation undermined incentives for the investment of labor and resources in agriculture; unrealistic plans for decentralized metal production sapped needed labor; unfavorable weather conditions; and communal dining halls encouraged overconsumption of available food.[35] Such was the centralized control of information and the intense pressure on party cadres to report only good newssuch as production quotas met or exceededthat information about the escalating disaster was effectively suppressed. When the leadership did become aware of the scale of the famine, it did little to respond, and continued to ban any discussion of the cataclysm. This blanket suppression of news was so effective that very few Chinese citizens were aware of the scale of the famine, and the greatest peacetime demographic disaster of the 20th century only became widely known twenty years later, when the veil of censorship began to lift.
The number of famine deaths during 19581961 range from 18 million[36] to at least 42 million[37] people, with a further 30 million cancelled or delayed births.[38] Agricultural collectivisation policies began to be reversed in 1978.[39]
Chinese emigration first occurred thousands of years ago. The mass emigration that occurred from the 19th century to 1949 was caused mainly by wars and starvation in mainland China, as well as political corruption. Most migrants were illiterate or poorly educated peasants, called by the now-recognized racial slur coolies (Chinese: , literally "hard labor"), who migrated to developing countries in need of labor, such as the Americas, Australia, South Africa, Southeast Asia, Malaya and other places.[40][41]
In 2009, there were 40-45 million overseas Chinese. They lived in 180 countries; 75% lived in Southeast Asia, and 19% in the United States.[42]
From 1980 to 2015, the government of China permitted the great majority of families to have only one child.[43] The ongoing Cultural Revolution and the strain it placed on the nation were large factors. During this time, the birth rate dropped from nearly 6 children per woman to just under 3.[44] (The colloquial term "births per woman" is usually formalized as the Total Fertility Rate (TFR), a technical term in demographic analysis meaning the average number of children that would be born to a woman over her lifetime if she were to experience the exact current age-specific fertility rates through her lifetime.)
As China's youngest generation (born under the one-child policy came of age for formation of the next generation, a single child would be left with having to provide support for their two parents and four grandparents. By 2014 families could have two children if one of the parents is an only child.[45][46]
The policy was supposedly voluntary. It was more strongly enforced in urban areas, where housing was in very short supply. Policies included free contraceptives, financial and employment incentives, economic penalties, and sometimes forced abortions and sterilizations.[47]
After 2000 the policy was steadily relaxed. Han Chinese living in rural areas were often permitted to have two children, as exceptions existed if the first child was a daughter.[48] Because of cases such as these, as well as urban couples who simply paid a fine (or "social maintenance fee") to have more children,[49] the overall fertility rate of mainland China is, in fact, closer to two children per family than to one child per family (1.8). In addition, since 2012, Han Chinese in southern Xinjiang were allowed to have two children. This, along with incentives and restrictions against higher Muslim Uyghur fertility, was seen as attempt to counter the threat of Uyghur separatism.[50]
In 2016 the national policy changed to a two-child policy; in 2018 it changed to a three-policy.[51]   The new policies helped address the aging issue in China.[52]
In 2018, about two years after the new policy reform, China is facing new ramifications from the two-child policy. Since the revision of the one-child policy, 90 million women have become eligible to have a second child.[53] According to The Economist, the new two-child policy may have negative implications on gender roles, with new expectations for women to bear more children and to abandon their careers.[54]
After the reform, China saw a short-lived boost in fertility rate for 2016. Chinese women gave birth to 17.9 million babies in 2016 (a record value in the 21st century), but the number of births declined by 3.5% to 17.2 million in 2017,[54] and to 15.2 million in 2018.[55][56]
In China, men still have greater marital power, which increases fertility pressure on their female partners.[53] The dynamic of relationships (amount of "power" held by each parent), and the amount of resources each parent has contributes to the struggle for dominance.[57] Resources would be items such as income, and health insurance. Dominance would be described as who has the final say in pregnancy, who has to resign in their career for maternal/parental leave. However, women have shown interest in a second child if the first child did not possess the desired gender.[54]
Chinese couples were also polled and stated that they would rather invest in one child opposed to two children.[53] To add, another concern for couples would be the high costs of raising another child; China's childcare system needs to be further developed.[58] The change in cultural norms appears to be having negative consequences and leads to fear of a large aging population with smaller younger generations; thus the lack of workforce to drive the economy.[57]
In May 2018, it was reported that Chinese authorities were in the process of ending their population control policies.[59] In May 2021, the Chinese government announced it would scrap the two child policy in favour of a three child policy, allowing couples to have three children in order to mitigate the country's falling birth rates.[60][61]

This article covers the history and bibliography of Romania and links to specialized articles.
34,950-year-old remains of modern humans with a possible Neanderthalian trait were discovered in present-day Romania when the Petera cu Oase ("Cave with Bones") was uncovered in 2002.[1] In 2011, older modern human remains were identified in the UK (Kents Cavern 41,500 to 44,200 years old) and Italy (Grotta del Cavallo 43,000 to 45,000 years old)[2] but the Romanian fossils are still among the oldest remains of Homo sapiens in Europe, so they may be representative of the first such people to have entered Europe.[3] The remains present a mixture of archaic, early modern human and Neanderthal morphological features.[4][5][6]
[7]
The Neolithic-Age Cucuteni area in northeastern Romania was the western region of the earliest European civilization, which is known as the CucuteniTrypillia culture.[8] The earliest-known salt works is at Poiana Slatinei near the village of Lunca; it was first used in the early Neolithic around 6050BC by the Starevo culture and later by the Cucuteni-Trypillia culture in the pre-Cucuteni period.[9] Evidence from this and other sites indicates the Cucuteni-Trypillia culture extracted salt from salt-laden spring water through the process of briquetage.[citation needed]
The earliest written evidence of people living in the territory of present-day Romania comes from Herodotus in Book IV of his Histories, which was written in c. 440BC; He writes that the tribal union/confederation of the Getae were defeated by the Persian Emperor Darius the Great during his campaign against the Scythians.[10] The Dacians, who are widely accepted as part of the Getae described earlier by the Greeks, were a branch of Thracians who inhabited Dacia, which corresponds with modern Romania, Moldova, northern Bulgaria and surrounding nations.[11]
The Dacian Kingdom reached its maximum expansion during the reign of King Burebista between 82BC and 44BC. Under his leadership, Dacia became a powerful state that threatened the regional interests of the Romans. Julius Caesar intended to start a campaign against the Dacians due to the support that Burebista gave to Pompey but he was assassinated in 44BC.[citation needed] A few months later, Burebista was assassinated by his own noblemen. Another theory suggests he was killed by Caesar's friends. Burebista's powerful state was divided into four and was not reunified until 95AD under the reign of the Dacian king Decebalus.[citation needed]
The Roman Empire conquered Moesia by 29BC, reaching the Danube River. In 87AD, Emperor Domitian sent six legions into Dacia, which were defeated at Tapae. The Dacians were eventually defeated by Emperor Trajan in two campaigns that lasted from 101AD to 106AD,[12] and the core of their kingdom was turned into the province of Roman Dacia.
The Romans exploited the rich ore deposits of Dacia. Gold and silver were especially plentiful,[13] and were found in great quantities in the Western Carpathians. After Trajan's conquest, he took to Rome over 165 tons of gold and 330 tons of silver. The Romans colonized the province extensively,[14] beginning a period of intense romanization, during which the language Vulgar Latin morphed into the Proto-Romanian language.[15][16]
Dacia's geographical position made it difficult to defend against the barbarians and during 240256AD, Dacia was lost under attacks of the Carpi and the Goths. The Roman Empire withdrew from Dacia Romana around 271AD, making it the first province to be abandoned.[17][18]
Between 271 and 275, the Roman army and administration left Dacia, which was invaded later by the Goths.[19] The Goths mixed with the local people until the 4th century, when the Huns, a nomadic people, arrived.[20] The Gepids,[21][22] the Avars, the Bulgars and their Slavic subjects[23] ruled Transylvania until the 8th century. The territories of Wallachia and Moldavia were under the control of the First Bulgarian Empire from its establishment in 681 until around the time of the Hungarian conquest of Transylvania at the end of the 10th century.[21]
After the disintegration of Great Bulgaria following Khan Kubrat's death in 668, a large group of Bulgars followed Asparukh, the third son of the great Khan, who headed westwards. In the 670's they settled in the area known as the Ongal to the north of the Danube delta.[citation needed] From there, Asparukh's cavalry in alliance with local Slavs annually attacked the Byzantine territories in the south. In 680, the Byzantine Emperor Constantine IV led a large army to fight the Bulgars but was defeated in the battle of Ongal and the Byzantines were forced to acknowledge the formation of a new country, the First Bulgarian Empire. The northern border of the country followed the southern slopes of the Carpathian mountains from the Iron Gates and reached the Dneper river or possibly just the Dniester river to the east.[citation needed]
The Bulgarians' main rivals in the area were the Avars to the west and the Khazars to the east. The Khazars were a serious threat; they marched westwards after they crushed the resistance of Kubrat's eldest son Bayan and waged a war against Asparukh, who perished in battle in 700.[citation needed] To protect their northern borders, the Bulgarians built several enormous ditches that ran the whole length of the border from the Timok river to the Black Sea.[citation needed]
In 803, Krum of Bulgaria became Khan. The new, energetic ruler focused on the north-west where Bulgaria's old enemies the Avars experienced difficulties and setbacks against the Franks under Charlemagne.[citation needed] Between 804 and 806, the Bulgarian armies annihilated the Avars and destroyed their state. Krum took the eastern parts of the former Avar Khaganate and took over rule of the local Slavic tribes. Bulgaria's territory twice extended twice from the middle Danube to the north of Budapest to the Dnester, though its possession of Transylvania is debatable.[citation needed] In 813 Khan Krum seized Odrin and plundered the whole of Eastern Thrace. He took 50,000 captives who were settled in Bulgaria across the Danube.[citation needed]
During the Middle Ages the Bulgarian Empire controlled vast areas to the north of the river Danube (with interruptions) from its establishment in 681 to its fragmentation in 13711422. These lands were called by contemporary Byzantine historians Bulgaria across the Danube, or Transdanubian Bulgaria.[24] Original information for the centuries-old Bulgarian rule there is scarce as the archives of the Bulgarian rulers were destroyed and little is mentioned for this area in Byzantine or Hungarian manuscripts. During the First Bulgarian Empire, the Dridu culture developed in the beginning of the 8thcentury and flourished until the 11th century.[25][26] It represents an early medieval archaeological culture which emerged in the region of the Lower Danube. [25][26] In Bulgaria it is usually referred to as Pliska-Preslav culture.[27]
The Pechenegs,[28] the Cumans[29] and Uzes are also mentioned by historic chronicles on the territory of Romania until the founding of the Romanian principalities of Wallachia in the south by Basarab I around 1310 in the High Middle Ages,[30] and Moldavia in the east, by Drago around 1352.[31]
The Pechenegs, a semi-nomadic Turkic people of the Central Asian steppes, occupied the steppes north of the Black Sea from the 8th to the 11th centuries, and by the 10th century they were in control of all of the territory between the Don and the lower Danube rivers.[32] During the 11th and 12th centuries, the nomadic confederacy of the Cumans and Eastern Kipchaks dominated the territories between present-day Kazakhstan, southern Russia, Ukraine, southern Moldavia and western Wallachia.[33][34][35]
It is a subject of dispute whether elements of the mixed DacoRoman population survived in Transylvania through the Dark Ages to become the ancestors of modern Romanians or whether the first Vlachs and Romanians appeared in the area in the 13th century after a northward migration from the Balkan Peninsula.[36][37] There is also debate over the ethnicity of Transylvania's population before the Hungarian conquest.[citation needed]
There is evidence the Second Bulgarian Empire, at least nominally, ruled the Wallachian lands up to the RucrBran corridor until the late 14th century. In a charter by Radu I, the Wallachian voivode requests tsar Ivan Alexander of Bulgaria to order his customs officers at Rucr and the Dmbovia River bridge to collect taxes following the law. The presence of Bulgarian customs officers at the Carpathians indicates Bulgarian suzerainty over those lands, though Radu's imperative tone implies a strong and increasing Wallachian autonomy.[38] Under Radu I and his successor Dan I, the realms in Transylvania and Severin continued to be disputed with Hungary.[39] Basarab was succeeded by Nicholas Alexander and Vladislav I. Vladislav attacked Transylvania after Louis I occupied lands south of the Danube, conceded to recognize him as overlord in 1368 but rebelled again in the same year. Vladislav's rule also witnessed the first confrontation between Wallachia and the Ottoman Empire, a battle in which Vladislav was allied with Ivan Shishman.[40] After the Magyar conquest of the 10th and 11th centuries, Transylvania became an autonomous and multi-ethnic voivodeship that was led by a voivode who was appointed by the King of Hungary until the 16th century.[41]
Several Kings of Hungary invited settlers from Central and Western Europe, such as the Saxons, to occupy Transylvania. The Szkelys were brought to southeastern Transylvania as border guards. Romanians are mentioned by the Hungarian documents of a township called Olahteluk in 1283 in Bihar County.[42][43] The "land of Romanians" (Terram Blacorum)[44][45][46][43]  appeared in Fgra and this area was mentioned under the name "Olachi" in 1285.[43] After the collapse of the Hungarian Kingdom following the disastrous Battle of Mohcs in 1526, the region became the independent Principality of Transylvania[47] until 1711.[48]
Many other small states with varying degrees of independence developed on the territory of today's Romania.[citation needed] In the 14th century, the larger principalities Moldavia and Wallachia emerged to fight the Ottoman Turks, who conquered Constantinople in 1453.[citation needed]
Independent Wallachia had been near the border of the Ottoman Empire since the 14th century until it had gradually succumbed to the Ottomans' influence during the next centuries with brief periods of independence. Vlad III the Impaler, also known as Vlad Dracula Romanian: Vlad epe, was a Prince of Wallachia in 1448, 145662, and 1476.[49][50] Vlad III is remembered for his raids against the Ottoman Empire and his initial success of keeping his small country free for a short time. In the Western world, Vlad is best known for being the inspiration for the main character in Bram Stoker's 1897 novel Dracula. The Romanian historiography[ro] evaluates him as a ferocious but just ruler.[51] the defender of the Wallachian independence and of the European Christianity against Ottoman expansionism.
The Principality of Moldavia reached its most glorious period under the rule of Stephen the Great between 1457 and 1504.[52] Stephen (Romanian: tefan) ruled for 47 years, an unusually long period for that time. He was a successful military leader and statesman, losing only two out of fifty battles;[53] he built a shrine to commemorate each victory, founding 48 churches and monasteries,[54] many of which have a unique architectural style and are listed in UNESCO's list of World Heritage Sites. Stefan's most prestigious victory was over the Ottoman Empire in 1475 at the Battle of Vaslui, for which he raised the Vorone Monastery. For this victory, Pope Sixtus IV nominated him as verus christianae fidei athleta (a true Champion of the Christian Faith). After Stephen's death, Moldavia also came under the suzerainty of the Ottoman Empire during the 16th century.[citation needed]
Although the core religious vocabulary of the Romanian language originated from Latin,[55] many terms were adopted from the Slavic Orthodoxy,[56] showing a significant influence dating from the Bulgarian Empire (681-1396).[57]
By 1541, the entire Balkan peninsula and northern Hungary became Ottoman provinces. Moldavia, Wallachia, and Transylvania came under Ottoman suzerainty but remained fully autonomous and until the 18th century, had some external independence.[citation needed] During this period, the Romanian lands experienced a slow disappearance of the feudalism and the distinguishing of some rulers like Vasile Lupu and Dimitrie Cantemir in Moldavia, Matei Basarab and Constantin Brncoveanu in Wallachia, and Gabriel Bethlen in Transylvania. At that time, the Russian Empire appeared to become the political and military power the threatened the Romanian principalities.[citation needed]
John II, the non-Habsburg King of Hungary, moved his royal court to Alba Iulia in Transylvania and after his abdication from the Hungarian throne, he became the first Prince of Transylvania.[58] His 1568 Edict of Turda was the first decree of religious freedom in the modern European history.[citation needed] In the aftermath, Transylvania was ruled by mostly Calvinist Hungarian princes until the end of the 17th century, and Protestantism flourished in the region.[citation needed]
Michael the Brave (Romanian: Mihai Viteazul) was the Prince of Wallachia from 1593 to 1601, of Transylvania from 1599 to 1600, and of Moldavia in 1600. For a short time during his reign, Transylvania was ruled together with Moldavia and Wallachia in a personal union.[59] After his death the union dissolved and as vassal tributary states, Moldavia and Wallachia still had internal autonomy and some external independence, which was finally lost in the 18th century.[citation needed]
The Principality of Transylvania reached its golden age under the absolutist rule of Gbor Bethlen from 1613 to 1629. In 1699, Transylvania became a part of the Habsburg Monarchy following the Austrian victory over the Turks.[60] The Habsburgs rapidly expanded their empire; in 1718 Oltenia, a major part of Wallachia, was annexed to the Habsburg monarchy and was only returned in 1739. In 1775, the Habsburgs later occupied the north-western part of Moldavia, which was later called Bukovina and was incorporated to the Austrian Empire in 1804. The eastern half of the principality, which was called Bessarabia, was occupied in 1812 by Russia.[citation needed]
During the Austro-Hungarian rule of Transylvania, Romanians formed the majority of the population.[61][62] Nationality issues occurred between Hungarians and Romanians due to the Magyarization policy.[63]
After their defeat to the Russians, the Ottoman Empire restored the Danube ports of Turnu, Giurgiu and Braila to Wallachia, and agreed to give up their commercial monopoly and recognize freedom of navigation on the Danube as specified in the Treaty of Adrianople, which was signed in 1829.[citation needed] The political autonomy of the Romanian principalities grew as their rulers were elected for life by a Community Assembly consisting of boyars, a method used to reduce political instability and Ottoman interventions.[citation needed] Following the war, Romanian lands came under Russian occupation under the governance of General Pavel Kiselyov until 1844. During his rule, the local boyars enacted the first Romanian constitution.[citation needed]
In 1848, there was a revolution in Moldavia, Wallachia and Transylvania perpetrated by Tudor Vladimirescu and his Pandurs in the Wallachian uprising of 1821.[citation needed] The goals of the revolutionaries were full independence for Moldavia and Wallachia, and national emancipation in Transylvania; these were not fulfilled but were the basis of the subsequent revolutions.[citation needed] The uprising helped the population of all three principalities recognize their unity of language and interests; all three Romanian principalities were very close in language and geography.[citation needed]
After the unsuccessful 1848 revolution, the Great Powers rejected the Romanians' desire to officially unite in a single state, forcing the Romanians to proceed alone their struggle against the Turks. Heavily taxed and badly administered under the Ottoman Empire, in 1859, people's representatives in both Moldavia and Wallachia elected the same Domnitor (ruling Prince of the Romanians); Alexandru Ioan Cuza, resulting in the unification of both principalities.[64]
Romania was created as a personal union that did not include Transylvania, where the upper class and the aristocracy remained mainly Hungarian, although Romanian nationalism clashed with Hungarian nationalism at the end of the 19th century.[citation needed] Austria-Hungary, especially under the Dual Monarchy of 1867, kept the territory firmly in control even in parts of Transylvania where Romanians constituted a vast majority.[citation needed]
In Romania between the 1750s and the 1830s, the exclusion of dowered women from the family inheritance led to increased cohesion within the nuclear family.  The wife's male relatives controlled the dowry but she retained sole ownership of the dowry and wedding gifts. Her relatives could prosecute the husband for squandering a dowry; wives gained some ability to leave an abusive marriage. The long-term result was a greater legal empowerment of women while providing economic security to divorced women, widows, and children.[65]
In an 1866 coup d'tat, Cuza was exiled and replaced with Prince Karl of Hohenzollern-Sigmaringen. He was appointed Domnitor, Ruling Prince of the United Principality of Romania, as  Prince Carol of Romania. Romania declared its independence from the Ottoman Empire after the Russo-Turkish War (18771878), in which the Ottomans fought against the Russian empire.[66]
In the 1878 Treaty of Berlin,[67] Romania was officially recognized as an independent state by the Great Powers.[68] In return, Romania ceded the district Bessarabia to Russia in exchange for access to the Black Sea ports and acquired Dobruja.[citation needed] In 1881, the Romania's principality status was raised to that of a kingdom and on 26 March that year, Prince Carol became King Carol I of Romania.[69][citation needed]
The period between 1878 and 1914 was one of stability and progress for Romania. During the Second Balkan War, Romania joined Greece, Serbia and Montenegro against Bulgaria.[citation needed] In the Treaty of Bucharest of 1913, Romania gained Southern Dobruja and established the Durostor and Caliacra counties.[70]
The governments of Britain and the United States repeatedly protested the brutal treatment of Romanians Jews, who were regarded as aliens who had no civil or political rights. The Romanian government tolerated their frequent humiliation and exclusion from many professions and government services. Romania engaged in arbitrary expulsions of Jews as vagabonds and tolerated violent pogroms against Jews, many of whom fled to the United States.[71][72]
The new state, which was located between the Ottoman, Austro-Hungarian, and Russian empires, looked to the Westparticularly to Francefor its cultural, educational, military and administrative models.[citation needed]
In August 1914, when World War I broke out, Romania declared its neutrality. Two years later, under the pressure of the Alliesespecially France, which was desperate to open a new front. Between 14 and 27 August 1916, Romania joined the Allies, for which it was promised support for the accomplishment of national unity, including recognition of Romanian rights over Transylvania, which was part of Austria-Hungary. Romania declared war on Austria-Hungary.[73]
The Romanian military campaign ended in disaster for Romania as the Central Powers conquered two-thirds of the country and captured or killed the majority of its army within four months.[citation needed] Moldavia remained in Romanian hands after the invading forces were stopped in 1917.[citation needed] In May 1918, Romania could not continue the war and negotiated a peace treaty with Germany.[citation needed] In November 1918, Romania rejoined the war after the Austro-Hungarian and Russian empires had disintegrated.[citation needed]
In 1918, at the end of World War I, the union of Romania with Bukovina was ratified in 1919 in the Treaty of Saint Germain,[74] and some of the Allies recognized the union with Bessarabia in 1920 through the never ratified Treaty of Paris.[75] On 1 December, the Deputies of the Romanians from Transylvania voted to unite Transylvania, Banat, Criana and Maramure with Romania by the Proclamation of Union of Alba Iulia. Romanians today celebrate this as the Great Union Day, that is a national holiday.
The Romanian expression Romnia Mare (Great or Greater Romania) refers to the Romanian state in the interwar period and to the territory Romania covered at the time. At that time, Romania achieved its greatest territorial extent, almost 300,000km2 or 120,000sqmi[76]), including all of the historic Romanian lands.[77]
Most of the claimed territories were granted to the Old Kingdom of Romania, which was ratified in 1920 by the Treaty of Trianon that defined the new border between Hungary and Romania.[78] The union of Bucovina and Bessarabia with Romania was ratified in 1920 by the Treaty of Versailles. Romania also acquired Southern Dobruja territory called "The Quadrilateral" from Bulgaria as a result of its participation in the Second Balkan War in 1913.[citation needed]
As a result of the peace treaties, most regions with clear Romanian majorities were merged into a single state.[citation needed] It also led to the inclusion of sizable minorities, including Magyars (ethnic Hungarians), Germans, Jews, Ukrainians and Bulgariansabout 28% of the country's population.[citation needed] National minorities were recognized by the 1923 Constitution of Romania and supported by laws; they were represented in Parliament and several of them created political parties, although a unique standing of minorities with autonomy on a wide basis, provided for at the assembly of Transylvanian Romanians on 1 December 1918, was not fulfilled.[citation needed]
Two periods can be identified in Romania between the two World Wars. From 1918 to 1938, Romania was a monarchy whose liberal Constitution was seldom respected in practice, but one facing the rise of the nationalist, anti-semitic parties, particularly Iron Guard, which won about 15% of the votes in the general elections of 1937. From 1938 to 1944, Romania was a dictatorship. The first dictator was King Carol II, who abolished the parliamentary regime and ruled with his camarilla.
In 1939, Germany and the Soviet Union signed the MolotovRibbentrop Pact, which stipulated, among other things, the Soviet "interest" in Bessarabia. Following the severe territorial losses of 1940 (see next section), Carol was forced to abdicate, replaced as king by his son Mihai, but the power was taken by the military dictator Ion Antonescu (initially in conjunction with the Iron Guard). In August 1944, Antonescu was arrested by Mihai.
During the Second World War, Romania tried to remain neutral but on 28 June 1940, it received a Soviet ultimatum with an implied threat of invasion in the event of non-compliance.[79] Under pressure from Moscow and Berlin, the Romanian administration and the army were forced to retreat from Bessarabia and Northern Bukovina to avoid war.[80] This and other factors prompted the Romanian government to join the Axis powers. Southern Dobruja was awarded to Bulgaria while Hungary received Northern Transylvania as result of an Axis arbitration.[81]
In 1940, Romania lost territory in both its east and west: In June 1940, after receiving an ultimatum from the Soviet Union, Romania ceded Bessarabia and northern Bukovina[82][83][84][85] Two-thirds of Bessarabia was combined with a small part of the USSR to form the Moldavian SSR. Northern Bukovina and Budjak were apportioned to the Ukrainian SSR.[citation needed] In August 1940, Northern Transylvania was awarded to Hungary by Germany and Italy through the Second Vienna Award.[86] Southern Dobruja was ceded to Bulgaria shortly after Carol's abdication.[citation needed]
Because Carol II lost so much territory through failed diplomacy, the army supported seizure of power by General Ion Antonescu.[citation needed] For four monthsthe period of the National Legionary Statehe shared power with the Iron Guard but the latter overplayed its hand in January 1941 and was suppressed.[citation needed] Romania entered World War II under the command of the German Wehrmacht in June 1941, declaring war on the Soviet Union [87] to recover Bessarabia and northern Bukovina.[citation needed] Romania continued to participate in the invasion after recovering the territories and was also awarded the territory between Dniester and the Southern Bug by Germany to administer under the name of Transnistria, where Romanians built a concentration camp[88][89] for the extermination of Jews.[citation needed]
During the war, Romania was the most important source of oil for Nazi Germany,[90] prompting multiple Allied bombing raids.[citation needed] By means of the Axis invasion of the Soviet Union, Romania recovered Bessarabia and northern Bukovina from Soviet Union under the leadership of general Ion Antonescu.[citation needed]
The Antonescu government played a major role in the Holocaust,[91] following to a lesser extent the Nazi policy of oppression and massacre of the Jews and Romas, primarily in the Eastern territories Transnistria and Moldavia, which Romania recovered from the Soviet Union.[92] According to an international commission report released by the Romanian government in 2004, Antonescu's dictatorial government was responsible for the murder in various forms including deportations to concentration camps and executions by the Romanian Army and Gendarmerie, and the German Einsatzgruppen of between 280,000 and 380,000 Jews on Romanian territories and in the war zones Bessarabia, Bukovina and Transnistria.[93][94]
On 20 August 1944, the Soviet Red Army crossed the border into Romania. On 23 August 1944, Antonescu was toppled and arrested by King Michael I of Romania, who joined the Allies and declared war on Germany. On 31 August 1944, the Red Army entered Bucharest. Despite Romania's change of sides, its role in the defeat of Nazi Germany was not recognized by the Paris Peace Conference of 1947.[95]
With the Red Army forces still stationed in the country and exerting de facto control, Communists and their allied parties claimed 80% of the vote through a combination of vote manipulation,[96] elimination and forced mergers of competing parties, thus establishing themselves as the dominant force. Romania suffered heavy casualties fighting the Nazis in Hungary and Czechoslovakia. By the end of the war, the Romanian army had suffered almost 300,000 casualties.[97]
A the end of World War II, the Paris Peace Treaty rendered the Vienna Awards void: Northern Transylvania was returned to Romania but Bessarabia, northern Bukovina and southern Dobruja were not recovered.[citation needed] The Moldavian-SSR became independent of the Soviet Union after the latter's 1991 demise and turned into the Republic of Moldova.[citation needed]
Soviet occupation following World War II strengthened the position of Communists, who became dominant in the left-wing coalition government that was appointed in March 1945. King Michael I was forced to abdicate and went into exile. Romania was proclaimed a people's republic[98][99] and remained under military and economic control of the Soviet Union until the late 1950s. During this period, Romania's resources were drained by the "SovRom" agreements; mixed Soviet-Romanian companies were established to mask the Soviet Union's looting of Romania.[100][101][102]
Romania's leader from 1948 to his death in 1965 was Gheorghe Gheorghiu-Dej, the First Secretary of the Romanian Workers' Party. Between 1947 and 1962, people were detained in prisons and camps, deported and put under house arrest and administrative detention. According to writer Cicerone Ionioiu, there were hundreds of thousands of cases of abuse, death and torture against a large range of people from political opponents to ordinary citizens.[103] Between 60,000[104] and 80,000 political prisoners were detained.[105] Ionioiu estimated two million people were victims of Communist repression in Romania.[106][107] According to Benjamin Valentino, probably tens or hundreds of thousands of deaths occurred as part of political repression and agricultural collectivization in Communist Romania, though he said documentation is insufficient for an accurate estimate to be made.[108][109]
Gheorghiu-Dej attained greater independence for Romania from the Soviet Union by persuading Soviet First Secretary Nikita Khrushchev to withdraw troops from Romania in April 1958.[110] After the negotiated withdrawal of Soviet troops, Romania under the new leadership of Nicolae Ceauescu started to pursue independent policies, including the condemnation of the Soviet-led 1968 invasion of CzechoslovakiaRomania being the only Warsaw Pact country not to take part in the invasionthe continuation of diplomatic relations with Israel after the Six-Day War of 1967 (again, the only Warsaw Pact country to do so), and the establishment of economic (1963) and diplomatic (1967) relations with West Germany.[111] Romania's close ties with Arab countries and the Palestine Liberation Organisation (PLO) allowed to play a key role in the Israel-Egypt and Israel-PLO peace processes by intermediating the visit of Egyptian president Sadat to Israel.[112]
Between 1977 and 1981, Romania's foreign debt sharply increased from US$3 to US$10billion[113] and the influence of international financial organizations such as the IMF and the World Bank grew, in conflict with Ceauescu's autarchic policies.[citation needed] Ceauescu's independent foreign policy meant leaders of Western nations leaders were slow to criticize Romania's government which, by the late 1970s, had become arbitrary, capricious and harsh.[citation needed] The Romanian economy grew quickly through foreign credit but this was replaced with austerity and political repression, which became more draconian through the 1980s.[citation needed]
Ceauescu eventually initiated a project of full reimbursement of the foreign debt; to achieve this, he imposed austerity policies that impoverished Romanians and exhausted the nation's economy. The project was completed in 1989, shortly before his overthrow. He greatly extended the authority of the Securitate (secret police) and imposed a cult of personality, leading to a dramatic decrease in Ceauescu's popularity and culminating in his overthrow and execution in the bloody Romanian Revolution in December 1989.[citation needed]
The Romanian Revolution resulted in more than 1,000 deaths in Timioara and Bucharest, and brought the fall of Ceauescu and the end of the Communist regime in Romania.[citation needed] After a week of unrest in Timioara, a mass rally summoned in Bucharest in support of Ceauescu on 21 December 1989 turned hostile. The Ceauescu couple fled Bucharest by helicopter but ended up in the custody of the army.[citation needed]
After being tried and convicted by a kangaroo court for genocide and other crimes, they were executed on 25 December 1989.[citation needed]
Ion Iliescu, a former Communist Party official marginalized by Ceauescu, attained national recognition as the leader of an impromptu governing coalition, the National Salvation Front (FSN) that proclaimed the establishment of democracy and civil liberties on 22 December 1989.[citation needed] The Communist Party was initially outlawed by Ion Iliescu, but he soon revoked that decision; as a consequence, Communism is not outlawed in Romania today. However, Ceauescu's most controversial measures, such as bans on abortion and contraception, were among the first laws to be changed after the Revolution.[citation needed]
After the fall of Ceauescu, the National Salvation Front (FSN) led by Ion Iliescu introduced partial multi-party democratic and free market measures.[114][115] A university professor with family roots in the Communist Party, Petre Roman, was named prime minister of the new government, which mostly consisted of former communist officials. The government initiated modest free market reforms. Several major political parties of the pre-war era, the National Christian Democrat Peasant's Party (PN-CD), the National Liberal Party (PNL), and the Romanian Social Democratic Party (PSDR), were reconstituted.[116]
In April 1990, after several major political rallies that January), a sit-in protest questioning the legitimacy of the government began in University Square, Bucharest, organized by the main opposition parties. The protest became ongoing mass demonstration known as the Golaniad.[117] The protesters accused the FSN of being made up of former Communists and members of the Securitate. Presidential and parliamentary elections were held on 20 May 1990. Taking advantage of FSN's tight control of the national radio and television, Iliescu won 85% of the vote. The FSN secured two-thirds of the seats in Parliament. Though most protesters left University Square after the government gained a large parliamentary majority, a minority deemed the results undemocratic and demanded the exclusion from political life of the former high-ranking Communist Party members. The peaceful demonstrations degenerated into violence; some of the protesters attacked the police headquarters, national television station, and the Foreign Ministry. After the police failed to bring the demonstrators to order, Ion Iliescu called on the "men of good will" to defend the state institutions in Bucharest.[118][119]
Various worker groups from Romania's industrial platforms responded, some of them engaged in altercations with the protesters. The coal miners of the Jiu Valley, thousands of whom arrived in Bucharest on 14 June, were the most visible and politically influential. According to the miners, most of the violence was perpetrated by government agents who were agitating the crowds.[117][120] Some of the counter-protesters attacked the headquarters and private residences of opposition leaders. Later parliamentary inquiries showed members of the government intelligence services were involved in the instigation and manipulation of both the protesters and the miners, and in June 1994, a Bucharest court found two former Securitate officers guilty of ransacking and stealing $100,000 from the house of a leading opposition politician.[117][120] Petre Roman's government fell in late September 1991, when the miners returned to Bucharest to demand higher salaries.[citation needed] A technocrat, Theodor Stolojan, was appointed to head an interim government until new elections could be held.[citation needed]
In December 1991, a new constitution was drafted and subsequently adopted, after a popular referendum, which, however, attracted criticism from international observers. The constitution was most recently revised by a national referendum on 1819 October 2003, and took effect on 29 October 2003.[citation needed]
In March 1992, the FSN split into two groups: the Democratic National Front (FDSN), led by Ion Iliescu and the Democratic Party (PD), led by Petre Roman. Iliescu won the presidential elections in September 1992 and his FDSN won the general elections held at the same time. With parliamentary support from the nationalist Romanian National Unity Party (PUNR), Greater Romania Party (PRM), and the ex-communist Socialist Workers' Party (PSM), a new government was formed in November 1992 under Prime Minister Nicolae Vcroiu. The FDSN changed its name to Party of Social Democracy in Romania (PDSR) in July 1993.[citation needed]
The subsequent disintegration of the FSN produced the Romanian Democrat Social Party (PDSR) (later Social Democratic Party, PSD), the Democratic Party (PD), and the ApR (Alliance for Romania). The PDSR party governed Romania from 1990 until 1996 through several coalitions and governments with Ion Iliescu as head of state.[citation needed]
Emil Constantinescu of the Democratic Convention (CDR) won the second round of the 1996 presidential election and replaced Iliescu as head of state.[121] The PDSR won the largest number of seats in the Parliament, but was unable to form a viable coalition. Constituent parties of the CDR joined the Democratic Party (PD) and the Democratic Alliance of Hungarians in Romania (UDMR) to form a centrist coalition government, holding 60% of the seats in Parliament.[citation needed]
This coalition implemented several critical reforms. The new coalition government, under prime minister Victor Ciorbea remained in office until March 1998, when Radu Vasile (PN-CD) took over as prime minister. The former governor of the National Bank, Mugur Isrescu, eventually replaced Radu Vasile as head of the government.[citation needed]
The 2000 election brought Iliescu's PDSR, known as Social Democratic Party (PSD) after the merger with the PSDR, back to power.[citation needed] Iliescu won a third term as the country's president. Adrian Nstase became the prime minister of the newly formed government.[citation needed]
In 2004, Traian Bsescu was elected president with an electoral coalition called Justice and Truth Alliance (DA).[122] The government was formed by a larger coalition which also included the Conservative Party (PC) and the Democratic Alliance of Hungarians in Romania (UDMR).[citation needed]
PostCold War Romania developed closer ties with Western Europe, eventually joining NATO in 2004.[123]
Presidential and parliamentary elections took place again on 28 November 2004. No political party secured a viable parliamentary majority and opposition parties alleged the PSD had committed large-scale electoral fraud.[124] There was no winner in the first round of the presidential elections. The joint PNL-PD candidate Traian Bsescu won the second round on 12 December 2004 with 51% of the vote and became the third post-revolutionary president of Romania.[125][126]
The then PNL leader, Clin Popescu-Triceanu was assigned the task of building a coalition government without the PSD. In December 2004, the new coalition government (PD, PNL, PUR Romanian Humanist Partywhich eventually changed its name to Romanian Conservative Party and UDMRwas sworn in under Prime Minister Triceanu.[127]
In June 1993, the country applied for membership in the European Union (EU). It became an Associated State of the EU in 1995, an Acceding Country in 2004, and a full member on 1 January 2007.[128]
Following the free travel agreement and politic of the postCold War period, as well as hardship of the life in the post 1990s economic depression, Romania has an increasingly large diaspora. The main emigration targets are Spain, Italy, Germany, Austria, Canada and the USA.[citation needed]
In April 2008, Bucharest hosted the NATO summit.[129]
In 2009, President Traian Basescu was re-elected for a second five-year term as the President of Romania.[130]
In January 2012, Romania experienced its first national protests since 1989, motivated by the global economical crisis and as an answer to the crisis situations and unrest in Europe of 2000s.[citation needed]
In January 2014, Romania's supreme court sentenced former Prime Minister Adrian Nastase, who held office between 2000 and 2004, to four years in prison for taking bribe.[131]
In 2014, Klaus Iohannis was elected as the President of Romania,[132] and he was re-elected by a landslide victory in 2019.[133]
In December 2020, the parliamentary election was won by the oppositional Social Democrats (PSD). Prime minister Ludovic Orban resigned because of the defeat of the National Liberal Party (PNL).[134] However, Florin Cu, a member of the National Liberal Party (PNL), became the new Prime Minister, forming a three party, center-right coalition of the PNL, the USR PLUS  and the Democratic Alliance of Hungarians in Romania (UDMR).[135]
General:


The Natural History Museum in London is a natural history museum that exhibits a vast range of specimens from various segments of natural history. It is one of three major museums on Exhibition Road in South Kensington, the others being the Science Museum and the Victoria and Albert Museum. The Natural History Museum's main frontage, however, is on Cromwell Road.
The museum is home to life and earth science specimens comprising some 80million items within five main collections: botany, entomology, mineralogy, palaeontology and zoology. The museum is a centre of research specialising in taxonomy, identification and conservation. Given the age of the institution, many of the collections have great historical as well as scientific value, such as specimens collected by Charles Darwin. The museum is particularly famous for its exhibition of dinosaur skeletons and ornate architecturesometimes dubbed a cathedral of natureboth exemplified by the large Diplodocus cast that dominated the vaulted central hall before it was replaced in 2017 with the skeleton of a blue whale hanging from the ceiling. The Natural History Museum Library contains extensive books, journals, manuscripts, and artwork collections linked to the work and research of the scientific departments; access to the library is by appointment only. The museum is recognised as the pre-eminent centre of natural history and research of related fields in the world.
Although commonly referred to as the Natural History Museum, it was officially known as British Museum (Natural History) until 1992, despite legal separation from the British Museum itself in 1963. Originating from collections within the British Museum, the landmark Alfred Waterhouse building was built and opened by 1881 and later incorporated the Geological Museum. The Darwin Centre is a more recent addition, partly designed as a modern facility for storing the valuable collections.
Like other publicly funded national museums in the United Kingdom, the Natural History Museum does not charge an admission fee.[2]
The museum is an exempt charity and a non-departmental public body sponsored by the Department for Digital, Culture, Media and Sport.[3] Catherine, Duchess of Cambridge, is a patron of the museum.[4] There are approximately 850 staff at the museum. The two largest strategic groups are the Public Engagement Group and Science Group.[5]
The foundation of the collection was that of the Ulster doctor Sir Hans Sloane (16601753), who allowed his significant collections to be purchased by the British Government at a price well below their market value at the time. This purchase was funded by a lottery. Sloane's collection, which included dried plants, and animal and human skeletons, was initially housed in Montagu House, Bloomsbury, in 1756, which was the home of the British Museum.
Most of the Sloane collection had disappeared by the early decades of the nineteenth century. Dr George Shaw (Keeper of Natural History 18061813) sold many specimens to the Royal College of Surgeons and had periodic cremations of material in the grounds of the museum. His successors also applied to the trustees for permission to destroy decayed specimens.[6] In 1833, the Annual Report states that, of the 5,500 insects listed in the Sloane catalogue, none remained. The inability of the natural history departments to conserve its specimens became notorious: the Treasury refused to entrust it with specimens collected at the government's expense. Appointments of staff were bedevilled by gentlemanly favouritism; in 1862 a nephew of the mistress of a Trustee was appointed Entomological Assistant despite not knowing the difference between a butterfly and a moth.[7][8][verification needed]
J. E. Gray (Keeper of Zoology 18401874) complained of the incidence of mental illness amongst staff: George Shaw threatened to put his foot on any shell not in the 12th edition of Linnaeus' Systema Naturae; another had removed all the labels and registration numbers from entomological cases arranged by a rival. The huge collection of the conchologist Hugh Cuming was acquired by the museum, and Gray's own wife had carried the open trays across the courtyard in a gale: all the labels blew away. That collection is said never to have recovered.[9]
The Principal Librarian at the time was Antonio Panizzi; his contempt for the natural history departments and for science in general was total. The general public was not encouraged to visit the museum's natural history exhibits. In 1835 to a Select Committee of Parliament, Sir Henry Ellis said this policy was fully approved by the Principal Librarian and his senior colleagues.
Many of these faults were corrected by the palaeontologist Richard Owen, appointed Superintendent of the natural history departments of the British Museum in 1856. His changes led Bill Bryson to write that "by making the Natural History Museum an institution for everyone, Owen transformed our expectations of what museums are for".[10]
Owen saw that the natural history departments needed more space, and that implied a separate building as the British Museum site was limited. Land in South Kensington was purchased, and in 1864 a competition was held to design the new museum. The winning entry was submitted by the civil engineer Captain Francis Fowke, who died shortly afterwards. The scheme was taken over by Alfred Waterhouse who substantially revised the agreed plans, and designed the faades in his own idiosyncratic Romanesque style which was inspired by his frequent visits to the Continent.[11] The original plans included wings on either side of the main building, but these plans were soon abandoned for budgetary reasons. The space these would have occupied are now taken by the Earth Galleries and Darwin Centre.
Work began in 1873 and was completed in 1880. The new museum opened in 1881, although the move from the old museum was not fully completed until 1883.
Both the interiors and exteriors of the Waterhouse building make extensive use of architectural terracotta tiles to resist the sooty atmosphere of Victorian London, manufactured by the Tamworth-based company of Gibbs and Canning Limited. The tiles and bricks feature many relief sculptures of flora and fauna, with living and extinct species featured within the west and east wings respectively. This explicit separation was at the request of Owen, and has been seen as a statement of his contemporary rebuttal of Darwin's attempt to link present species with past through the theory of natural selection.[12] Though Waterhouse slipped in a few anomalies, such as bats amongst the extinct animals and a fossil ammonite with the living species. The sculptures were produced from clay models by a French sculptor based in London, M Dujardin, working to drawings prepared by the architect.[13]
The central axis of the museum is aligned with the tower of Imperial College London (formerly the Imperial Institute) and the Royal Albert Hall and Albert Memorial further north. These all form part of the complex known colloquially as Albertopolis.
Even after the opening, the Natural History Museum legally remained a department of the British Museum with the formal name British Museum (Natural History), usually abbreviated in the scientific literature as B.M.(N.H.). A petition to the Chancellor of the Exchequer was made in 1866, signed by the heads of the Royal, Linnean and Zoological societies as well as naturalists including Darwin, Wallace and Huxley, asking that the museum gain independence from the board of the British Museum, and heated discussions on the matter continued for nearly one hundred years. Finally, with the passing of the British Museum Act 1963, the British Museum (Natural History) became an independent museum with its own board of trustees, although  despite a proposed amendment to the act in the House of Lords  the former name was retained. In 1989 the museum publicly re-branded itself as the Natural History Museum and stopped using the title British Museum (Natural History) on its advertising and its books for general readers. Only with the Museums and Galleries Act 1992 did the museum's formal title finally change to the Natural History Museum.
In 1976, the museum absorbed the adjacent Geological Museum of the British Geological Survey, which had long competed for the limited space available in the area. The Geological Museum became world-famous for exhibitions including an active volcano model and an earthquake machine (designed by James Gardner), and housed the world's first computer-enhanced exhibition (Treasures of the Earth). The museum's galleries were completely rebuilt and relaunched in 1996 as The Earth Galleries, with the other exhibitions in the Waterhouse building retitled The Life Galleries. The Natural History Museum's own mineralogy displays remain largely unchanged as an example of the 19th-century display techniques of the Waterhouse building.
The central atrium design by Neal Potter overcame visitors' reluctance to visit the upper galleries by "pulling" them through a model of the Earth made up of random plates on an escalator. The new design covered the walls in recycled slate and sandblasted the major stars and planets onto the wall. The museum's 'star' geological exhibits are displayed within the walls. Six iconic figures were the backdrop to discussing how previous generations have viewed Earth. These were later removed to make place for a Stegosaurus skeleton that was put on display in late 2015.
The Darwin Centre (named after Charles Darwin) was designed as a new home for the museum's collection of tens of millions of preserved specimens, as well as new work spaces for the museum's scientific staff and new educational visitor experiences. Built in two distinct phases, with two new buildings adjacent to the main Waterhouse building, it is the most significant new development project in the museum's history.
Phase one of the Darwin Centre opened to the public in 2002, and it houses the zoological department's 'spirit collections'organisms preserved in alcohol. Phase Two was unveiled in September 2008 and opened to the general public in September 2009. It was designed by the Danish architecture practice C. F. Mller Architects in the shape of a giant, eight-story cocoon and houses the entomology and botanical collectionsthe 'dry collections'.[14] It is possible for members of the public to visit and view non-exhibited items for a fee by booking onto one of the several Spirit Collection Tours offered daily.[15]
Arguably the most famous creature in the centre is the 8.62-metre-long giant squid, affectionately named Archie.[16]
As part of the museum's remit to communicate science education and conservation work, a new multimedia studio forms an important part of Darwin Centre Phase 2. In collaboration with the BBC's Natural History Unit (holder of the largest archive of natural history footage) the Attenborough Studionamed after the broadcaster Sir David Attenboroughprovides a multimedia environment for educational events. The studio holds regular lectures and demonstrations, including free Nature Live talks on Fridays, Saturdays and Sundays.

One of the most famous and certainly most prominent of the exhibitsnicknamed "Dippy"is a 105-foot (32m)-long replica of a Diplodocus carnegii skeleton which was on display for many years within the central hall. The cast was given as a gift by the Scottish-American industrialist Andrew Carnegie, after a discussion with King Edward VII, then a keen trustee of the British Museum. Carnegie paid 2,000 for the casting, copying the original held at the Carnegie Museum of Natural History. The pieces were sent to London in 36 crates, and on 12 May 1905, the exhibit was unveiled to great public and media interest. The real fossil had yet to be mounted, as the Carnegie Museum in Pittsburgh was still being constructed to house it. As word of Dippy spread, Mr Carnegie paid to have additional copies made for display in most major European capitals and in Central and South America, making Dippy the most-viewed dinosaur skeleton in the world. The dinosaur quickly became an iconic representation of the museum, and has featured in many cartoons and other media, including the 1975 Disney comedy One of Our Dinosaurs Is Missing.  After 112 years on display at the museum, the dinosaur replica was removed in early 2017 to be replaced by the actual skeleton of a young blue whale, a 128-year-old skeleton nicknamed "Hope".[17] Dippy went on a tour of various British museums starting in 2018 and concluding in 2020 at Norwich Cathedral.[18][19][20]
The blue whale skeleton, Hope, that has replaced Dippy, is another prominent display in the museum. The display of the skeleton, some 82 feet (25m) long and weighing 4.5 tonnes, was only made possible in 1934 with the building of the New Whale Hall (now the Mammals (blue whale model) gallery). The whale had been in storage for 42 years since its stranding on sandbanks at the mouth of Wexford Harbour, Ireland in March 1891 after being injured by whalers.[19] At this time, it was first displayed in the Mammals (blue whale model) gallery, but now takes pride of place in the museum's Hintze Hall. Discussion of the idea of a life-sized model also began around 1934, and work was undertaken within the Whale Hall itself. Since taking a cast of such a large animal was deemed prohibitively expensive, scale models were used to meticulously piece the structure together. During construction, workmen left a trapdoor within the whale's stomach, which they would use for surreptitious cigarette breaks. Before the door was closed and sealed forever, some coins and a telephone directory were placed insidethis soon growing to an urban myth that a time capsule was left inside. The work was completedentirely within the hall and in view of the publicin 1938. At the time it was the largest such model in the world, at 92 feet (28m) in length. The construction details were later borrowed by several American museums, who scaled the plans further. The work involved in removing Dippy and replacing it with Hope was documented in a BBC Television special, Horizon: Dippy and the Whale, narrated by David Attenborough, which was first broadcast on BBC Two on 13 July 2017, the day before Hope was unveiled for public display.[21]
The Darwin Centre is host to Archie, an 8.62-metre-long giant squid taken alive in a fishing net near the Falkland Islands in 2004. The squid is not on general display, but stored in the large tank room in the basement of the Phase 1 building. It is possible for members of the public to visit and view non-exhibited items behind the scenes for a fee by booking onto one of the several Spirit Collection Tours offered daily.[15] On arrival at the museum, the specimen was immediately frozen while preparations commenced for its permanent storage. Since few complete and reasonably fresh examples of the species exist, "wet storage" was chosen, leaving the squid undissected. A 9.45-metre acrylic tank was constructed (by the same team that provide tanks to Damien Hirst), and the body preserved using a mixture of formalin and saline solution.
The museum holds the remains and bones of the "River Thames whale", a northern bottlenose whale that lost its way on 20 January 2006 and swam into the Thames. Although primarily used for research purposes, and held at the museum's storage site at Wandsworth.
Dinocochlea, one of the longer-standing mysteries of paleontology (originally thought to be a giant gastropod shell, then a coprolite, and now a concretion of a worm's tunnel), has been part of the collection since its discovery in 1921.
The museum keeps a wildlife garden on its west lawn, on which a potentially new species of insect resembling Arocatus roeselii was discovered in 2007.[22]
The museum is divided into four sets of galleries, or zones, each colour coded to follow a broad theme.
This is the zone that can be entered from Exhibition Road, on the East side of the building. It is a gallery themed around the changing history of the Earth.
Earth's Treasury shows specimens of rocks, minerals and gemstones behind glass in a dimly lit gallery. Lasting Impressions is a small gallery containing specimens of rocks, plants and minerals, of which most can be touched.
This zone is accessed from the Cromwell Road entrance via the Hintze Hall and follows the theme of the evolution of the planet.
To the left of the Hintze Hall, this zone explores the diversity of life on the planet.
Enables the public to see science at work and also provides spaces for relaxation and contemplation. Accessible from Queens Gate.
The museum runs a series of educational and public engagement programmes. These include for example a highly praised "How Science Works" hands on workshop for school students demonstrating the use of microfossils in geological research. The museum also played a major role in securing designation of the Jurassic Coast of Devon and Dorset as a UNESCO World Heritage Site and has subsequently been a lead partner in the Lyme Regis Fossil Festivals.
In 2005, the museum launched a project to develop notable gallery characters to patrol display cases, including 'facsimiles' of Carl Linnaeus, Mary Anning, Dorothea Bate and William Smith. They tell stories and anecdotes of their lives and discoveries and aim to surprise visitors.[23]
In 2010, a six-part BBC documentary series was filmed at the museum entitled Museum of Life exploring the history and behind the scenes aspects of the museum.[24]
Since May 2001, the Natural History Museum admission has been free for some events and permanent exhibitions. However, there are certain temporary exhibits and shows that require a fee.
The Natural History museum combines the museum's life and earth science collections with specialist expertise in "taxonomy, systematics, biodiversity, natural resources, planetary science, evolution and informatics" to tackle scientific questions.[25]
In 2011, the museum led the setting up of an International Union for Conservation of Nature Bumblebee Specialist Group, chaired by Dr. Paul H. Williams,[26] to assess the threat status of bumblebee species worldwide using Red List criteria.[27][28]
The closest London Underground station is South Kensington  there is a tunnel from the station that emerges close to the entrances of all three museums. Admission is free, though there are donation boxes in the foyer.
Museum Lane immediately to the north provides disabled access to the museum.[29]
A connecting bridge between the Natural History and Science museums closed to the public in the late 1990s.
The museum is a prominent setting in Charlie Fletcher's children's book trilogy about "unLondon" called Stoneheart. George Chapman, the hero, sneaks outside when punished on a school trip; he breaks off a small dragon's stone head from a relief and is chased by a pterodactyl, which comes to life from a statue on the roof.
The museum is the primary setting for Rattle His Bones, the eighth Daisy Dalrymple Mystery by Carola Dunn. The story revolves around a murder and jewel theft occurring during the time Daisy Dalrymple is writing a story about the museum for an American publisher.
The museum plays an important role in the 1975 London-based Disney live-action feature One of Our Dinosaurs Is Missing; the eponymous skeleton is stolen from the museum, and a group of intrepid nannies hide inside the mouth of the museum's blue whale model (in fact a specially created prop  the nannies peer out from behind the whale's teeth, but a blue whale is a baleen whale and has no teeth). Additionally, the film is set in the 1920s, before the blue whale model was built.
The museum features on 'School Trip', an episode of The Sooty Show. 
The museum appears on The Lost World when Professor Challenger leads a scientific expedition to the Amazon River to find a hidden plateau where dinosaurs, pterosaurs, and apemen survive.
The museum features as a base for Prodigium, a secret society which studies and fights monsters]], first appearing on The Mummy. 
In the 2014 film Paddington, Millicent Clyde is a devious and trecherous taxidermist at the museum. She kidnaps Paddington, intending to kill and stuff him, but is thwarted by the Brown family after scenes involving chases inside and on the roof of the building.[30]
In the first episode of the third season of the TV series Penny Dreadful (20142016), the main character, Vanessa Ives (Eva Green), visits the museum after her psychotherapist tells her to "go somewhere different". There, she meets Dr. Alexander Sweet (Christian Camargo), who is a zoologist and the Director of Zoological Studies. The museum is frequently seen in the following episodes as Vanessa and Dr. Sweet's relationship flourishes.
Andy Day's CBeebies shows, Andy's Dinosaur Adventures and Andy's Prehistoric Adventures are filmed in the Natural History Museum.
The faade and front steps of the museum appear in the first part of the Marvel Studios 2021 movie The Eternals. We see Gemma Chans character Sersi enter the museum followed by internal shots of the whale and statue of Charles Darwin as she passes through the main hall.
The NHM also has an outpost in Tring, Hertfordshire, built by local eccentric Lionel Walter Rothschild. The NHM took ownership in 1938. In 2007, the museum announced that the name would be changed to the Natural History Museum at Tring, though the older name, the Walter Rothschild Zoological Museum, is still in widespread use.

Alternate history (also alternative history, althist, AH) is a genre of speculative fiction of stories in which one or more historical events occur and are resolved differently than in real life.[1][2][3][4] As conjecture based upon historical fact, alternative history stories propose What if? scenarios about crucial events in human history, and present outcomes very different from the historical record. Alternate history also is a subgenre of literary fiction, science fiction, and historical fiction; as literature, alternate history uses the tropes of the genre to answer the What if? speculations of the story.
Since the 1950s, as a subgenre of science fiction, alternative history stories feature the tropes of time travel between histories, and the psychic awareness of the existence of an alternative universe, by the inhabitants of a given universe; and time travel that divides history into various timestreams.[5] In the Spanish, French, German, and Portuguese, Italian,  Catalan, and Galician languages, the terms Uchronie, ucronia,  ucrona, and Uchronie identify the alternate history genre, from which derives the English term Uchronia, composed of the Greek prefix - ("not", "not any", and "no") and the Greek word  (chronos) "time", to describe a story that occurs "[in] no time"; analogous to a story that occurs in utopia, "[in] no place". The term Uchronia also is the name of the list of alternate-history books, uchronia.net.[6] Moreover, Allohistory (other history) is another term for the genre of alternative history.[5]
Alternative history is a genre of fiction wherein the author speculates upon how the course of history might have been altered if a particular historical event had an outcome different from the real life outcome.[1] An alternate history requires three conditions: (i) A point of divergence from the historical record, before the time in which the author is writing; (ii) A change that would alter known history; and (iii) An examination of the ramifications of that alteration to history.[7] Occasionally, some types of genre fiction are misidentified as alternative history, specifically science fiction stories set in a time that was the future for the writer, but now is the past for the reader, such as the novels 2001: A Space Odyssey (1968), by Arthur C. Clarke and Nineteen Eighty-Four (1949), by George Orwell, because the authors did not alter the history of the past when they wrote the stories.[7]
Moreover, the genre of the Secret History of an event, which can be either fictional or non-fictional, documents events that might have occurred in history, but which had no effect upon the recorded historical outcome.[7][8] Alternative history also is thematically related to, but distinct from, Counterfactual History, which is a form of historiography that attempts to answer the What if? speculations that arise from counterfactual conditions in order to understand what did happen.[9] As a method of historical research, counterfactual history explores historical events with an extrapolated timeline in which key historical events either did not occur or had an outcome different from the historical record.[10]
The earliest example of alternate (or counterfactual) history is found in Livy's Ab Urbe Condita Libri (book IX, sections 1719). Livy contemplated an alternative 4th century BC in which Alexander the Great had survived to attack Europe as he had planned; asking, "What would have been the results for Rome if she had been engaged in a war with Alexander?"[11][12][13] Livy concluded that the Romans would likely have defeated Alexander.[11][14][15] An even earlier possibility is Herodotus's Histories, which contains speculative material.[16]

Another example of counterfactual history was posited by cardinal and Doctor of the Church Peter Damian in the 11th century. In his famous work De Divina Omnipotentia, a long letter in which he discusses God's omnipotence, he treats questions related to the limits of divine power, including the question of whether God can change the past,[17] for example, bringing about that Rome was never founded:[18][19][20]I see I must respond finally to what  many people, on the basis of your holinesss [own] judgment, raise as an objection on the topic of this dispute. For they say: If, as you assert, God is omnipotent in all things, can he manage this, that things that have been made were not  made? He can certainly destroy all things that have been made, so that they do not exist now. But it cannot be seen how he can bring it about that things that have been made were not made. To be sure, it can come about that from now on and hereafter Rome does not exist; for it can be destroyed. But no opinion can grasp how it can come about that it was not founded long ago...[21]One early work of fiction detailing an alternate history is Joanot Martorell's 1490 epic romance Tirant lo Blanch, which was written when the loss of Constantinople to the Turks was still a recent and traumatic memory for Christian Europe. It tells the story of the knight Tirant the White from Brittany who travels to the embattled remnants of the Byzantine Empire. He becomes a Megaduke and commander of its armies and manages to fight off the invading Ottoman armies of Mehmet II. He saves the city from Islamic conquest, and even chases the Turks deeper into lands they had previously conquered.
One of the earliest works of alternate history published in large quantities for the reception of a large audience may be Louis Geoffroy's Histoire de la Monarchie universelle: Napolon et la conqute du monde (18121832) (History of the Universal Monarchy: Napoleon and the Conquest of the World) (1836), which imagines Napoleon's First French Empire emerging victorious in the French invasion of Russia in 1811 and in an invasion of England in 1814, later unifying the world under Bonaparte's rule.[12]
In the English language, the first known complete alternate history is Nathaniel Hawthorne's short story "P.'s Correspondence", published in 1845. It recounts the tale of a man who is considered "a madman" due to his perceptions of a different 1845, a reality in which long-dead famous people, such as the poets Robert Burns, Lord Byron, Percy Bysshe Shelley and John Keats, the actor Edmund Kean, the British politician George Canning, and Napoleon Bonaparte, are still alive.
The first novel-length alternate history in English would seem to be Castello Holford's Aristopia (1895). While not as nationalistic as Louis Geoffroy's Napolon et la conqute du monde, 18121823, Aristopia is another attempt to portray a Utopian society. In Aristopia, the earliest settlers in Virginia discover a reef made of solid gold and are able to build a Utopian society in North America.
In 1905, H. G. Wells published A Modern Utopia. As explicitly noted in the book itself, Wells's main aim in writing it was to set out his social and political ideas, the plot serving mainly as a vehicle to expound them. This book introduced the idea of a person being transported from a point in our familiar world to the precise geographical equivalent point in an alternate world in which history had gone differently. The protagonists undergo various adventures in the alternate world, and then are finally transported back to our world, again to the precise geographical equivalent point. Since then, that has become a staple of the alternate history genre.    
A number of alternate history stories and novels appeared in the late 19th and early 20th centuries (see, for example, Joseph Edgar Chamberlin's The Ifs of History [1907] and Charles Petrie's If: A Jacobite Fantasy [1926]).[22] In 1931, British historian Sir John Squire collected a series of essays from some of the leading historians of the period for his anthology If It Had Happened Otherwise. In that work, scholars from major universities, as well as important non-academic authors, turned their attention to such questions as "If the Moors in Spain Had Won" and "If Louis XVI Had Had an Atom of Firmness". The essays range from serious scholarly efforts to Hendrik Willem van Loon's fanciful and satiric portrayal of an independent 20th-century New Amsterdam, a Dutch city-state on the island of Manhattan. Among the authors included were Hilaire Belloc, Andr Maurois, and Winston Churchill.
One of the entries in Squire's volume was Churchill's "If Lee Had Not Won the Battle of Gettysburg", written from the viewpoint of a historian in a world in which the Confederacy had won the American Civil War. The entry considers what would have happened if the North had been victorious (in other words, a character from an alternate world imagines a world more like the real one we live in, although it is not identical in every detail). Speculative work that narrates from the point of view of an alternate history is variously known as "recursive alternate history", a "double-blind what-if", or an "alternate-alternate history".[23] Churchill's essay was one of the influences behind Ward Moore's alternate history novel Bring the Jubilee[citation needed] in which General Robert E. Lee won the Battle of Gettysburg and paved the way for the eventual victory of the Confederacy in the American Civil War (named the "War of Southron Independence" in this timeline). The protagonist, the autodidact Hodgins Backmaker, travels back to the aforementioned battle and inadvertently changes history, which results in the emergence of our own timeline and the consequent victory of the Union instead.
The American humorist author James Thurber parodied alternate history stories about the American Civil War in his 1930 story "If Grant Had Been Drinking at Appomattox", which he accompanied with this very brief introduction: "Scribner's magazine is publishing a series of three articles: 'If Booth Had Missed Lincoln', 'If Lee Had Won the Battle of Gettysburg', and 'If Napoleon Had Escaped to America'. This is the fourth".
Another example of alternate history from this period (and arguably[24] the first that explicitly posited cross-time travel from one universe to another as anything more than a visionary experience) is H.G. Wells' Men Like Gods (1923) in which the London-based journalist Mr. Barnstable, along with two cars and their passengers, is mysteriously teleported into "another world", which the "Earthlings" call Utopia. Being far more advanced than Earth, Utopia is some 3000 years ahead of humanity in its development. Wells describes a multiverse of alternative worlds, complete with the paratime travel machines that would later become popular with American pulp writers. However, since his hero experiences only a single alternate world, the story is not very different from conventional alternate history.[25]
In the 1930s, alternate history moved into a new arena. The December 1933 issue of Astounding published Nat Schachner's "Ancestral Voices", which was quickly followed by Murray Leinster's "Sidewise in Time". While earlier alternate histories examined reasonably-straightforward divergences, Leinster attempted something completely different. In his "World gone mad", pieces of Earth traded places with their analogs from different timelines. The story follows Professor Minott and his students from a fictitious Robinson College as they wander through analogues of worlds that followed a different history.
A somewhat similar approach was taken by Robert A. Heinlein in his 1941 novelette Elsewhen in which a professor trains his mind to move his body across timelines. He then hypnotizes his students so that they can explore more of them. Eventually, each settles into the reality that is most suitable for him or her. Some of the worlds they visit are mundane, some are very odd, and others follow science fiction or fantasy conventions.
World War II produced alternate history for propaganda: both British and American[26] authors wrote works depicting Nazi invasions of their respective countries as cautionary tales.
The period around World War II also saw the publication of the time travel novel Lest Darkness Fall by L. Sprague de Camp in which an American academic travels to Italy at the time of the Byzantine invasion of the Ostrogoths. De Camp's time traveler, Martin Padway, is depicted as making permanent historical changes and implicitly forming a new time branch, thereby making the work an alternate history.
In William Tenn's short story Brooklyn Project (1948), a tyrannical US Government brushes aside the warnings of scientists about the dangers of time travel and goes on with a planned experiment - with the result that minor changes to the prehistoric past cause Humanity to never have existed, its place taken by tentacled underwater intelligent creatures - who also have a tyrannical government which also insists on experimenting with time-travel.[27]
Time travel as the cause of a point of divergence (POD), which can denote either the bifurcation of a historical timeline or a simple replacement of the future that existed before the time-travelling event, has continued to be a popular theme. In Ward Moore's Bring the Jubilee, the protagonist lives in an alternate history in which the Confederacy has won the American Civil War. He travels backward through time and brings about a Union victory at the Battle of Gettysburg.
When a story's assumptions about the nature of time travel lead to the complete replacement of the visited time's future, rather than just the creation of an additional time line, the device of a "time patrol" is often used where guardians move through time to preserve the "correct" history.
A more recent example is Making History by Stephen Fry in which a time machine is used to alter history so that Adolf Hitler was never born. That ironically results in a more competent leader of Nazi Germany and results in the country's ascendancy and longevity in the altered timeline.
H.G. Wells' "cross-time" or "many universes" variant (see above) was fully developed by Murray Leinster in his 1934 short story "Sidewise in Time", in which sections of the Earth's surface begin changing places with their counterparts in alternate timelines.
Fredric Brown employed this subgenre to satirize the science fiction pulps and their adolescent readersand fears of foreign invasionin the classic What Mad Universe (1949). In Clifford D. Simak's Ring Around the Sun (1953), the hero ends up in an alternate earth of thick forests in which humanity never developed but a band of mutants is establishing a colony; the story line appears to frame the author's anxieties regarding McCarthyism and the Cold War.[citation needed]
While many justifications for alternate histories involve a multiverse, the "many world" theory would naturally involve many worlds, in fact a continually exploding array of universes. In quantum theory, new worlds would proliferate with every quantum event, and even if the writer uses human decisions, every decision that could be made differently would result in a different timeline. A writer's fictional multiverse may, in fact, preclude some decisions as humanly impossible, as when, in Night Watch, Terry Pratchett depicts a character informing Vimes that while anything that can happen, has happened, nevertheless there is no history whatsoever in which Vimes has ever murdered his wife. When the writer explicitly maintains that all possible decisions are made in all possible ways, one possible conclusion is that the characters were neither brave, nor clever, nor skilled, but simply lucky enough to happen on the universe in which they did not choose the cowardly route, take the stupid action, fumble the crucial activity, etc.; few writers focus on this idea, although it has been explored in stories such as Larry Niven's story All the Myriad Ways, where the reality of all possible universes leads to an epidemic of suicide and crime because people conclude their choices have no moral import.
In any case, even if it is true that every possible outcome occurs in some world, it can still be argued that traits such as bravery and intelligence might still affect the relative frequency of worlds in which better or worse outcomes occurred (even if the total number of worlds with each type of outcome is infinite, it is still possible to assign a different measure to different infinite sets). The physicist David Deutsch, a strong advocate of the many-worlds interpretation of quantum mechanics, has argued along these lines, saying that "By making good choices, doing the right thing, we thicken the stack of universes in which versions of us live reasonable lives. When you succeed, all the copies of you who made the same decision succeed too. What you do for the better increases the portion of the multiverse where good things happen."[28] This view is perhaps somewhat too abstract to be explored directly in science fiction stories, but a few writers have tried, such as Greg Egan in his short story The Infinite Assassin, where an agent is trying to contain reality-scrambling "whirlpools" that form around users of a certain drug, and the agent is constantly trying to maximize the consistency of behavior among his alternate selves, attempting to compensate for events and thoughts he experiences, he guesses are of low measure relative to those experienced by most of his other selves.
Many writersperhaps the majorityavoid the discussion entirely. In one novel of this type, H. Beam Piper's Lord Kalvan of Otherwhen, a Pennsylvania State Police officer, who knows how to make gunpowder, is transported from our world to an alternate universe where the recipe for gunpowder is a tightly held secret and saves a country that is about to be conquered by its neighbors. The paratime patrol members are warned against going into the timelines immediately surrounding it, where the country will be overrun, but the book never depicts the slaughter of the innocent thus entailed, remaining solely in the timeline where the country is saved.
The cross-time theme was further developed in the 1960s by Keith Laumer in the first three volumes of his Imperium sequence, which would be completed in Zone Yellow (1990). Piper's politically more sophisticated variant was adopted and adapted by Michael Kurland and Jack Chalker in the 1980s; Chalker's G.O.D. Inc trilogy (198789), featuring paratime detectives Sam and Brandy Horowitz, marks the first attempt at merging the paratime thriller with the police procedural.[citation needed] Kurland's Perchance (1988), the first volume of the never-completed "Chronicles of Elsewhen", presents a multiverse of secretive cross-time societies that utilize a variety of means for cross-time travel, ranging from high-tech capsules to mutant powers. Harry Turtledove has launched the Crosstime Traffic series for teenagers featuring a variant of H. Beam Piper's paratime trading empire.
The concept of a cross-time version of a world war, involving rival paratime empires, was developed in Fritz Leiber's Change War series, starting with the Hugo Award winning The Big Time (1958); followed by Richard C. Meredith's Timeliner trilogy in the 1970s, Michael McCollum's A Greater Infinity (1982) and John Barnes' Timeline Wars trilogy in the 1990s.
Such "paratime" stories may include speculation that the laws of nature can vary from one universe to the next, providing a science fictional explanationor veneerfor what is normally fantasy. Aaron Allston's Doc Sidhe and Sidhe Devil take place between our world, the "grim world" and an alternate "fair world" where the Sidhe retreated to. Although technology is clearly present in both worlds, and the "fair world" parallels our history, about fifty years out of step, there is functional magic in the fair world. Even with such explanation, the more explicitly the alternate world resembles a normal fantasy world, the more likely the story is to be labelled fantasy, as in Poul Anderson's "House Rule" and "Loser's Night". In both science fiction and fantasy, whether a given parallel universe is an alternate history may not be clear. The writer might allude to a POD only to explain the existence and make no use of the concept, or may present the universe without explanation of its existence.
Isaac Asimov's short story "What If" (1952) is about a couple who can explore alternate realities by means of a television-like device. This idea can also be found in Asimov's novel The End of Eternity (1955), in which the "Eternals" can change the realities of the world, without people being aware of it.  Poul Anderson's Time Patrol stories feature conflicts between forces intent on changing history and the Patrol who work to preserve it.  One story, Delenda Est, describes a world in which Carthage triumphed over the Roman Republic.  The Big Time, by Fritz Leiber, describes a Change War ranging across all of history.
Keith Laumer's Worlds of the Imperium is one of the earliest alternate history novels; it was published by Fantastic Stories of the Imagination in 1961, in magazine form, and reprinted by Ace Books in 1962 as one half of an Ace Double. Besides our world, Laumer describes a world ruled by an Imperial aristocracy formed by the merger of European empires, in which the American Revolution never happened, and a third world in post-war chaos ruled by the protagonist's doppelganger.
Philip K. Dick's novel, The Man in the High Castle (1962), is an alternate history in which Nazi Germany and Imperial Japan won World War II. This book contains an example of "alternate-alternate" history, in that one of its characters authored a book depicting a reality in which the Allies won the war, itself divergent from real-world history in several aspects. The several characters live within a divided United States, in which the Empire of Japan takes the Pacific states, governing them as a puppet, Nazi Germany takes the East Coast of the United States and parts of the Midwest, with the remnants of the old United States' government as the Neutral Zone, a buffer state between the two superpowers. The book has inspired an Amazon series of the same name.
Vladimir Nabokov's novel, Ada or Ardor: A Family Chronicle (1969), is a story of incest that takes place within an alternate North America settled in part by Czarist Russia and that borrows from Dick's idea of "alternate-alternate" history (the world of Nabokov's hero is wracked by rumors of a "counter-earth" that apparently is ours). Some critics[who?] believe that the references to a counter-earth suggest that the world portrayed in Ada is a delusion in the mind of the hero (another favorite theme of Dick's novels[citation needed]). Strikingly, the characters in Ada seem to acknowledge their own world as the copy or negative version, calling it "Anti-Terra", while its mythical twin is the real "Terra". Like history, science has followed a divergent path on Anti-Terra: it boasts all the same technology as our world, but all based on water instead of electricity; e.g., when a character in Ada makes a long-distance call, all the toilets in the house flush at once to provide hydraulic power.
Guido Morselli described the defeat of Italy (and subsequently France) in World War I in his novel, Past Conditional (1975; Contro-passato prossimo), wherein the static Alpine front line which divided Italy from Austria during that war collapses when the Germans and the Austrians forsake trench warfare and adopt blitzkrieg twenty years in advance.
Kingsley Amis set his novel, The Alteration (1976), in the 20th century, but major events in the Reformation did not take place, and Protestantism is limited to the breakaway Republic of New England. Martin Luther was reconciled to the Roman Catholic Church and later became Pope Germanian I.
In Nick Hancock and Chris England's 1997 book What Didn't Happen Next: An Alternative History of Football it is suggested that, had Gordon Banks been fit to play in the 1970 FIFA World Cup quarter-final, there would have been no Thatcherism and the post-war consensus would have continued indefinitely.[29][pageneeded]
Kim Stanley Robinson's novel, The Years of Rice and Salt (2002), starts at the point of divergence with Timur turning his army away from Europe, and the Black Death has killed 99% of Europe's population, instead of only a third. Robinson explores world history from that point in AD 1405 (807 AH) to about AD 2045 (1467 AH). Rather than following the great man theory of history, focusing on leaders, wars, and major events, Robinson writes more about social history, similar to the Annales School of history theory and Marxist historiography, focusing on the lives of ordinary people living in their time and place.
Philip Roth's novel, The Plot Against America (2004), looks at an America where Franklin D. Roosevelt is defeated in 1940 in his bid for a third term as President of the United States, and Charles Lindbergh is elected, leading to a US that features increasing fascism and anti-Semitism.
Michael Chabon, occasionally an author of speculative fiction, contributed to the genre with his novel The Yiddish Policemen's Union (2007), which explores a world in which the State of Israel was destroyed in its infancy and many of the world's Jews instead live in a small strip of Alaska set aside by the US government for Jewish settlement. The story follows a Jewish detective solving a murder case in the Yiddish-speaking semi-autonomous city state of Sitka. Stylistically, Chabon borrows heavily from the noir and detective fiction genres, while exploring social issues related to Jewish history and culture. Apart from the alternate history of the Jews and Israel, Chabon also plays with other common tropes of alternate history fiction; in the book, Germany actually loses the war even harder than they did in reality, getting hit with a nuclear bomb instead of just simply losing a ground war (subverting the common "what if Germany won WWII?" trope).
The late 1980s and the 1990s saw a boom in popular-fiction versions of alternate history, fueled by the emergence of the prolific alternate history author Harry Turtledove, as well as the development of the steampunk genre and two series of anthologiesthe What Might Have Been series edited by Gregory Benford and the Alternate ... series edited by Mike Resnick. This period also saw alternate history works by S. M. Stirling, Kim Stanley Robinson, Harry Harrison, Howard Waldrop, Peter Tieryas,[30] and others.
In 1986, a sixteen-part epic comic book series called Captain Confederacy began examining a world where the Confederate States of America won the American Civil War. In the series, the Captain and others heroes are staged government propaganda events featuring the feats of these superheroes.[31]
Since the late 1990s, Harry Turtledove has been the most prolific practitioner of alternate history and has been given the title "Master of Alternate History" by some.[32] His books include those of Timeline 191 (a.k.a. Southern Victory, also known as TL-191), in which, while the Confederate States of America won the American Civil War, the Union and Imperial Germany defeat the Entente Powers in the two "Great War"s of the 1910s and 1940s (with a Nazi-esque Confederate government attempting to exterminate its Black population), and the Worldwar series, in which aliens invaded Earth during World War II. Other stories by Turtledove include A Different Flesh, in which America was not colonized from Asia during the last ice age; In the Presence of Mine Enemies, in which the Nazis won World War II; and Ruled Britannia, in which the Spanish Armada succeeded in conquering England in the Elizabethan era, with William Shakespeare being given the task of writing the play that will motivate the Britons to rise up against their Spanish conquerors. He also co-authored a book with actor Richard Dreyfuss, The Two Georges, in which the United Kingdom retained the American colonies, with George Washington and King George III making peace. He did a two-volume series in which the Japanese not only bombed Pearl Harbor but also invaded and occupied the Hawaiian Islands.
Perhaps the most incessantly explored theme in popular alternate history focuses on worlds in which the Nazis won World War Two. In some versions, the Nazis and/or Axis Powers conquer the entire world; in others, they conquer most of the world but a "Fortress America" exists under siege; while in others, there is a Nazi/Japanese Cold War comparable to the US/Soviet equivalent in 'our' timeline. Fatherland (1992), by Robert Harris, is set in Europe following the Nazi victory. The novel Dominion by C.J. Sansom (2012) is similar in concept but is set in England, with Churchill the leader of an anti-German Resistance and other historic persons in various fictional roles.[33] In the Mecha Samurai Empire series (2016), Peter Tieryas focuses on the Asian-American side of the alternate history, exploring an America ruled by the Japanese Empire while integrating elements of Asian pop culture like mechas and videogames.[34]
Several writers have posited points of departure for such a world but then have injected time splitters from the future or paratime travel, for instance James P. Hogan's The Proteus Operation. Norman Spinrad wrote The Iron Dream in 1972, which is intended to be a science fiction novel written by Adolf Hitler after fleeing from Europe to North America in the 1920s.
In Jo Walton's "Small Change" series, the United Kingdom made peace with Hitler before the involvement of the United States in World War II, and slowly collapses due to severe economic depression. Former House Speaker Newt Gingrich and William R. Forstchen have written a novel, 1945, in which the US defeated Japan but not Germany in World War II, resulting in a Cold War with Germany rather than the Soviet Union. Gingrich and Forstchen neglected to write the promised sequel; instead, they wrote a trilogy about the American Civil War, starting with Gettysburg: A Novel of the Civil War, in which the Confederates win a victory at the Battle of Gettysburg - however, after Lincoln responds by bringing Grant and his forces to the eastern theater, the Army of Northern Virginia is soon trapped and destroyed in Maryland, and the war ends within weeks. Also from that general era, Martin Cruz Smith, in his first novel, posited an independent American Indian nation following the defeat of Custer in The Indians Won (1970).[35]
Beginning with The Probability Broach in 1980, L. Neil Smith wrote several novels that postulated the disintegration of the US Federal Government after Albert Gallatin joins the Whiskey Rebellion in 1794 and eventually leads to the creation of a libertarian utopia.[36]
A recent time traveling splitter variant involves entire communities being shifted elsewhere to become the unwitting creators of new time branches. These communities are transported from the present (or the near-future) to the past or to another time-line via a natural disaster, the action of technologically advanced aliens, or a human experiment gone wrong. S. M. Stirling wrote the Island in the Sea of Time trilogy, in which Nantucket Island and all its modern inhabitants are transported to Bronze Age times to become the world's first superpower. In Eric Flint's 1632 series, a small town in West Virginia is transported to 17th century central Europe and drastically changes the course of the Thirty Years' War, which was then underway. John Birmingham's Axis of Time trilogy deals with the culture shock when a United Nations naval task force from 2021 finds itself back in 1942 helping the Allies against the Empire of Japan and the Germans (and doing almost as much harm as good in spite of its advanced weapons). Similarly, Robert Charles Wilson's Mysterium depicts a failed US government experiment which transports a small American town into an alternative version of the US run by believers in a form of Christianity known as Gnosticism, who are engaged in a bitter war with the "Spanish" in Mexico (the chief scientist at the laboratory where the experiment occurred is described as a Gnostic, and references to Christian Gnosticism appear repeatedly in the book).[37] In Time for Patriots by retired astronomer Thomas Wm. Hamilton (4897 Tomhamilton) a town and military academy on Long Island are transported back to 1770, where they shorten the American Revolution, rewrite the Constitution, prolong Mozart's life, battle Barbary pirates, and have other adventures.
Although not dealing in physical time travel, in his alt-history novel Marx Returns, Jason Barker introduces anachronisms into the life and times of Karl Marx, such as when his wife Jenny sings a verse from the Sex Pistols's song "Anarchy in the U.K.", or in the games of chess she plays with the Marxes' housekeeper Helene Demuth, which on one occasion involves a CaroKann Defence.[38] In her review of the novel, Nina Power writes of "Jennys 'utopian' desire for an end to time", an attitude which, according to Power, is inspired by her husband's co-authored book The German Ideology. However, in keeping with the novel's anachronisms, the latter was not published until 1932.[39] By contrast, the novel's timeline ends in 1871.
Many works of straight fantasy and science fantasy take place in historical settings, though with the addition of, for example, magic or mythological beasts. Some present a secret history in which the modern day world no longer believes that these elements ever existed. Many ambiguous alternate/secret histories are set in Renaissance or pre-Renaissance times, and may explicitly include a "retreat" from the world, which would explain the current absence of such phenomena. Other stories make plan a divergence of some kind. 
In Poul Anderson's Three Hearts and Three Lions in which the Matter of France is history and the fairy folk are real and powerful. The same author's A Midsummer Tempest, occurs in a world in which the plays of William Shakespeare (called here "the Great Historian"), presented the literal truth in every instance. The novel itself takes place in the era of Oliver Cromwell and Charles I. Here, the English Civil War had a different outcome, and the Industrial Revolution has occurred early. 
Randall Garrett's "Lord Darcy" series presents a point of divergence: a monk systemizes magic rather than science, so the use of foxglove to treat heart disease is regarded as superstition. Another point of divergence occurs in 1199, when Richard the Lionheart survives the Siege of Chaluz and returns to England and makes the Angevin Empire so strong that it survives into the 20th century.
Jonathan Strange & Mr Norrell by Susanna Clarke takes place in an England where a separate Kingdom ruled by the Raven King and founded on magic existed in Northumbria for over 300 years. In Patricia Wrede's Regency fantasies, Great Britain has a Royal Society of Wizards.
The Tales of Alvin Maker series by Orson Scott Card (a parallel to the life of Joseph Smith, founder of the Latter Day Saint movement) takes place in an alternate America, beginning in the early 19th century. Prior to that time, a POD occurred: England, under the rule of Oliver Cromwell, had banished "makers", or anyone else demonstrating "knacks" (an ability to perform seemingly supernatural feats) to the North American continent. Thus the early American colonists embraced as perfectly ordinary these gifts, and counted on them as a part of their daily lives. The political division of the continent is considerably altered, with two large English colonies bookending a smaller "American" nation, one aligned with England, and the other governed by exiled Cavaliers. Actual historical figures are seen in a much different light: Ben Franklin is revered as the continent's finest "maker", George Washington was executed after being captured, and "Tom" Jefferson is the first president of "Appalachia", the result of a compromise between the Continentals and the Crown.[citation needed]
On the other hand, when the "Old Ones" (fairies) still manifest themselves in England in Keith Roberts's Pavane, which takes place in a technologically backward world after a Spanish assassination of Elizabeth I allowed the Spanish Armada to conquer England, the possibility that the fairies were real but retreated from modern advances makes the POD possible: the fairies really were present all along, in a secret history. 
Again, in the English Renaissance fantasy Armor of Light by Melissa Scott and Lisa A. Barnett, the magic used in the book, by Dr. John Dee and others, actually was practiced in the Renaissance; positing a secret history of effective magic makes this an alternate history with a point of departure. Sir Philip Sidney survives the Battle of Zutphen in 1586, and shortly thereafter saving the life of Christopher Marlowe.
When the magical version of our world's history is set in contemporary times, the distinction becomes clear between alternate history on the one hand and contemporary fantasy, using in effect a form of secret history (as when Josepha Sherman's Son of Darkness has an elf living in New York City, in disguise) on the other. In works such as Robert A. Heinlein's Magic, Incorporated where a construction company can use magic to rig up stands at a sporting event and Poul Anderson's Operation Chaos and its sequel Operation Luna, where djinns are serious weapons of warwith atomic bombsthe use of magic throughout the United States and other modern countries makes it clear that this is not secret historyalthough references in Operation Chaos to degaussing the effects of cold iron make it possible that it is the result of a POD. The sequel clarifies this as the result of a collaboration of Einstein and Planck in 1901, resulting in the theory of "rhea tics". Henry Moseley applies this theory to "degauss the effects of cold iron and release the goetic forces." This results in the suppression of ferromagnetism and the re-emergence of magic and magical creatures.
Alternate history shades off into other fantasy subgenres when the use of actual, though altered, history and geography decreases, although a culture may still be clearly the original source; Barry Hughart's Bridge of Birds and its sequels take place in a fantasy world, albeit one clearly based on China, and with allusions to actual Chinese history, such as the Empress Wu. Richard Garfinkle's Celestial Matters incorporates ancient Chinese physics and Greek Aristotelian physics, using them as if factual.
Alternate history has long been a staple of Japanese speculative fiction with such authors as Futaro Yamada and Ry Hanmura writing novels set in recognizable historical settings withaddded  supernatural or science fiction elements. Ry Hanmura's 1973 Musubi no Yama Hiroku which recreated 400 years of Japan's history from the perspective of a secret magical family with psychic abilities. The novel has since come to be recognized as a masterpiece of Japanese speculative fiction.[40] Twelve years later, author Hiroshi Aramata wrote the groundbreaking Teito Monogatari which reimagined the history of Tokyo across the 20th century in a world heavily influenced by the supernatural.[41]
The TV show Sliders explores different possible alternate realities by having the protagonist "slide" into different parallel dimensions of the same planet Earth. Another TV show Motherland: Fort Salem explores a female-dominated world in which witchcraft is real. Its world diverged from our timeline when the Salem witch trials are resolved by an agreement between witches and ungifted humans.
The anime Fena: Pirate Princess featured an alternate 18th century.[42]
The TV show The Man in the High Castle is an adaptation of the novel with the same name that ran for four seasons.
For the same reasons that this genre is explored by role-playing games, alternate history is also an intriguing backdrop for the storylines of many video games. A famous example of an alternate history game is Command & Conquer: Red Alert. Released in 1996, the game presents a point of divergence in 1946 in which Albert Einstein goes back in time to prevent World War II from ever taking place by erasing Adolf Hitler from time after he is released from Landsberg Prison in 1924. Einstein is successful in his mission, but in the process, he allows Joseph Stalin and the Soviet Union to become powerful enough to launch a massive campaign to conquer Europe.
In the Civilization series, the player guides a civilization from prehistory to the present and creates radically altered versions of history on a long time scale. Several scenarios recreate a particular period, which becomes the "point of divergence" in an alternate history shaped by the player's actions. Popular examples in Sid Meier's Civilization IV include Desert War, set in the Mediterranean theatre of World War II and featuring scripted events tied to possible outcomes of battles; Broken Star, set in a hypothetical Russian civil war in 2010; and Rhye's and Fall of Civilization, an 'Earth simulator' designed to mirror a history as closely as possible but incorporating unpredictable elements to provide realistic alternate settings.
In some games such as the Metal Gear and Resident Evil series, events that were originally intended to represent the near future when the games were originally released later ended up becoming alternate histories in later entries in those franchises. For example, Metal Gear 2: Solid Snake (1990), set in 1999, depicted a near future that ended up becoming an alternate history in Metal Gear Solid (1998). Likewise, Resident Evil (1996) and Resident Evil 2 (1998), both set in 1998, depicted near-future events that had later become an alternative history by the time Resident Evil 4 (2005) was released.
In the 2009 steampunk shooter, Damnation is set on an alternate version of planet Earth, in the early 20th century after the American Civil War, which had spanned over several decades, and steam engines replace combustion engines. The game sees the protagonists fighting off a rich industrialist who wants to do away with both the Union and the Confederacy in one swift movement and turn the United States of America into a country called the "American Empire" with a totalitarian dictatorship.
Crimson Skies is one example of an alternate history spawning multiple interpretations in multiple genres. The stories and games in Crimson Skies take place in an alternate 1930s United States in which the nation crumbled into many hostile states following the effects of the Great Depression, the Great War, and Prohibition. With the road and railway system destroyed, commerce took to the skies, which led to the emergence of air pirate gangs who plunder the aerial commerce.
The game Freedom Fighters portrays a situation similar to that of the movie Red Dawn and Red Alert 2 but less comically than the latter. The point of divergence is during World War II in which the Soviet Union develops an atomic bomb first and uses it on Berlin. With the balance of power and influence tipped in Russia's favor, history diverges. Brief summaries at the beginning of the game inform the player of the Communist bloc's complete takeover of Europe by 1953, a different ending to the Cuban Missile Crisis, and the spread of Soviet influence into South America and Mexico.
Similarly, the 2007 video game World in Conflict is set in 1989, with the Soviet Union on the verge of collapse. The point of divergence is several months before the opening of the game, when Warsaw Pact forces staged a desperate invasion of Western Europe. As the game begins, a Soviet invasion force lands in Seattle and takes advantage of the fact that most of the US military is in Europe.
The game Battlestations: Pacific, released in 2008, offered in alternate history campaign for the Imperial Japanese Navy in which Japan destroys all three carriers in the Battle of Midway, which is followed by a successful invasion of the island. That causes the United States to lack any sort of aerial power to fight the Japanese and to be continuously forced onto the defense.
Turning Point: Fall of Liberty, released in February 2008, is an alternate history first-person shooter in which Winston Churchill died in 1931 from being struck by a taxi cab. Therefore, Great Britain lacks the charismatic leader needed to keep the country together and allows it to be successfully conquered by Nazi Germany during Operation Sea Lion in 1940. Germany later conquers the rest of Europe, as well as North Africa and the Middle East, and produces a massive number of Wunderwaffe. The Axis powers launch a surprise invasion of the isolationist United States in the Eastern Seaboard in 1953, which forces the country to surrender and submit to a puppet government.
Another alternate history game involving Nazis is War Front: Turning Point in which Hitler died during the early days of World War II and so a much more effective leadership rose to power. Under the command of a new Fhrer (who is referred to as "Chancellor", with his real name never being revealed), Operation Sealion succeeds, and the Nazis successfully conquer Britain and spark a cold war between them and the Allied Powers.
The Fallout series of computer role-playing games is set in a divergent US, whose history after World War II diverges from the real world to follow a retro-futuristic timeline. For example, fusion power was invented quite soon after the end of the war, but the transistor was never developed. The result was a future that has a 1950s "World of Tomorrow" feel to it, with extremely high technology like artificial intelligence implemented with thermionic valves and other technologies that are now considered obsolete.
Many game series by the Swedish developer Paradox Interactive start at a concise point in history and allow the player to immerse in the role of a contemporary leader and alter the course of in-game history. The most prominent game with that setting is Crusader Kings II.[citation needed]
S.T.A.L.K.E.R. games have an alternate history at the Chernobyl Exclusion Zone in which a special area called "The Zone" is formed.
Wolfenstein: The New Order is set in an alternate 1960 in which the Nazis won World War II and do so also by acquiring high technology. The sequel Wolfenstein II: The New Colossus continues that but is set in the conquered United States of America.
Fans of alternate history have made use of the internet from a very early point to showcase their own works and provide useful tools for those fans searching for anything alternate history, first in mailing lists and usenet groups, later in web databases and forums. The "Usenet Alternate History List" was first posted on April 11, 1991, to the Usenet newsgroup rec.arts.sf-lovers. In May 1995, the dedicated newsgroup soc.history.what-if was created for showcasing and discussing alternate histories.[43] Its prominence declined with the general migration from unmoderated usenet to moderated web forums, most prominently AlternateHistory.com, the self-described "largest gathering of alternate history fans on the internet" with over 10,000 active members.[44][45]
In addition to these discussion forums, in 1997 Uchronia: The Alternate History List was created as an online repository, now containing over 2,900 alternate history novels, stories, essays, and other printed materials in several different languages. Uchronia was selected as the Sci Fi Channel's "Sci Fi Site of the Week" twice.[46][47]

The Oakland Athletics, a current Major League Baseball franchise, originated in Philadelphia.  This article details the history of the Philadelphia Athletics, from 1901 to 1954, when they moved to Kansas City.
The Philadelphia Athletics were among the first teams in professional baseball. The Athletics turned professional in the late 1860s and helped establish the first league, National Association of Professional Base Ball Players (NA), which began play in 1871. The Athletics continued to play in the National League until 1901, when the American League was formed. The Western League had been renamed the American League in 1900 by league president Bancroft (Ban) Johnson, and declared itself the second major league in 1901. Johnson created new franchises in the east and eliminated some franchises in the west.[1] Philadelphia had a new franchise created to compete with the National League's Philadelphia Phillies. Former catcher Connie Mack was recruited to manage the club. Mack in turn persuaded Phillies minority owner Ben Shibe as well as others to invest in the team, which would be called the Philadelphia Athletics. Mack himself bought a 25% interest, while the remaining 25% was sold to Philadelphia sportswriters Sam Jones and Frank Hough.[2]
The new league recruited many of its players from the existing National League, persuading them to "jump" to the American League in defiance of their National League contracts. One of the players who jumped to the new league was second baseman Nap Lajoie, formerly of the crosstown Phillies. He won the A.L.'s first batting title with a .426 batting average, still a league record. The Athletics and the American League received a setback when, on April 21, 1902, the Pennsylvania Supreme Court invalidated Lajoie's contract with the Athletics, and ordered him back to the Phillies. This order, though, was only enforceable in the Commonwealth of Pennsylvania. Lajoie was sold to Cleveland, but was kept out of road games in Philadelphia until the National Agreement was signed between the two leagues in 1903.
Columbia Park was the first home of the Athletics. They played there from their founding in 1901 through the 1908 season, and it was the venue of their two home games in the 1905 World Series.
In the early years, the A's established themselves as one of the dominant teams in the new league, winning the A.L. pennant six times (1902, 1905, 1910, 1911, 1913, and 1914), and winning the World Series in 1910, 1911, and 1913.[3] They won over 100 games in 1910 and 1911, and 99 games in 1914. The team was known for its "$100,000 Infield", consisting of Stuffy McInnis (first base), Eddie Collins (second base), Jack Barry (shortstop), and Frank "Home Run" Baker (third base) as well as pitchers Eddie Plank and Chief Bender. Rube Waddell was also a major pitching star for the A's in the early 1900s. According to Lamont Buchanan in The World Series and Highlights of Baseball, the A's fans were fond of chanting, "If Eddie Plank doesn't make you lose / We have Waddell and Bender all ready to use!" Plank holds the franchise record for career victories, with 284.
In 1909, the A's moved into the major leagues' first concrete-and-steel ballpark, Shibe Park. This remains the second and last time in franchise history where a new ballpark was built specifically for the A's. Later in the decade, Mack bought the 25% of the team's stock owned by Jones and Hough to become a full partner with Shibe. Shibe ceded Mack full control over the baseball side while retaining control over the business side.[2] However, Mack had already enjoyed a nearly free hand in baseball matters since the franchise's inception.
In 1914, the Athletics lost the 1914 World Series to the "Miracle Braves" in a four-game sweep. Mack traded, sold or released most of the team's star players soon after. In his book To Every Thing a Season, Bruce Kuklick points out that there were suspicions that the A's had thrown the Series, or at least "laid down", perhaps in protest of Mack's frugal ways. Mack himself alluded to that rumor years later, but debunked it.  He claimed that the team was torn by numerous internal factions, and was also distracted by the allure of a third major league, the Federal League.
The Federal League had been formed to begin play in 1914. As the AL had done 13 years before, the new league raided existing AL and NL teams for players. Several of his best players, including Bender, had already decided to jump before the World Series. Mack refused to match the upstart league's offers, preferring to rebuild with younger (and less expensive) players. The result was a swift and near-total collapse. The Athletics went from a 9953 (.651) record and a pennant in 1914 to a record of 43109 (.283) and last place in 1915, and then to 36117 (.235, still a modern major-league low) in 1916.[4] The team would finish in last place every year through 1922 and would not contend again until 1925. Shibe died in 1922, and his sons Tom and John took over the business side, leaving the baseball side to Mack. Although Mack only held the titles of vice president and secretary-treasurer, for all intents and purposes he was now the head of the franchise, and would remain so for the next three decades. 
By this time Mack had cemented his famous image of the tall, gaunt and well-dressed man waving his players into position with a scorecard. Unlike most managers, he chose to wear a high-collar shirt, tie, ascot scarf, and a straw boater hat instead of a uniform, a look that he never changed for the rest of his life, even decades after it went out of fashion. This came at the price of Mack not being allowed on-field during games per league regulations.
By the latter half of the 1920s, Mack had assembled one of the most feared batting orders in the history of baseball featuring three future Baseball Hall of Fame members.[5] At its heart were Al Simmons, who batted .334 and hit 307 home runs over his major league career, Jimmie Foxx, who hit 30 or more home runs in 12 consecutive seasons and drove in more than 100 runs in 13 consecutive years, and Mickey Cochrane, one of the best-hitting catchers in baseball history.[5] A fourth future Hall of Fame member was pitcher Lefty Grove, who led the American League in strikeouts seven years in a row, and had the league's lowest earned run average a record nine times.[6]
In 1927 and 1928, the Athletics finished second to the New York Yankees, then won pennants in 1929, 1930 and 1931, winning the World Series in 1929 and 1930.[3] In each of the three years, the Athletics won over 100 games. While the 1927 New York Yankees, whose batting order was known as the Murderers' Row, are remembered as one of the best teams in baseball history, the Athletics teams of the late 1920s and early 1930s are largely forgotten.[5] Opponents who faced both teams considered them to be generally equal.[5] Both teams won three consecutive pennants and two of three World Series.[5]
Statistically the New York and Philadelphia dynasties were remarkably even: The Athletics had a record of 313-143 (.686) between 1929 and 1931; the Yankees, 302-160 (.654) between 1926 and 1928.[5] And while the Athletics scored six fewer runs than the Yankees2,710 -2,716, the Athletics had five fewer runs scored against them: 1,992-1,997, a difference of only one run.[5] The Yankees had the best single season at the plate, hitting for a combined .307 batting average and scoring 975 runs in 1927.[5] The Athletics' strongest offensive performance came in 1929, when they batted .296. On defense the Athletics were clearly superior; over their three-year American League reign they committed only 432 errors, 167 fewer than the Yankees.[5] Cochrane was also especially adept at telling his pitchers how to pitch to opposing batters.[5] Many veteran baseball observers believe that the Yankees' far more exalted status in history is due largely to the fact that they played in New York, where most of the national media is located.[5]
As it turned out, this would be the Athletics' last hurrah in Philadelphia.[5] The Great Depression was well under way, and declining attendance had drastically reduced the team's revenues.[5] Mack again sold or traded his best players in order to reduce expenses.[5] In September 1932, he sold Simmons, Jimmy Dykes and Mule Haas to the Chicago White Sox for $100,000.[5] In December 1933, Mack sent Grove, Rube Walberg and Max Bishop to the Boston Red Sox for Bob Kline, Rabbit Warstler and $125,000.[5] Also in 1933, he sold Cochrane to the Detroit Tigers for $100,000.[5] The construction of a spite fence at Shibe Park, blocking the view from nearby buildings, only served to irritate potential paying fans. However, the consequences did not become apparent for a few more years, as the team finished second in 1932 and third in 1933.
The Athletics finished fifth in 1934, then last in 1935. Mack was already 68 years old when the A's won the pennant in 1931, and many felt that the game had long since passed him by. Although he had every intention of building another winner, he did not have the extra money to get big stars. He also did not (or could not) invest in a farm system.  Unlike most other owners, Mack had no source of income apart from the A's, so the dwindling attendance figures of the early 1930s hit him especially hard.
As a result, the A's went into a decline that lasted for over 30 years, through three cities. Except for a fifth-place finish in 1944, they finished in last or next-to-last place every year from 1935 to 1946. Tom Shibe died in 1936 and John succeeded him as club president. However, John resigned due to illness a few months later, leaving the presidency to Mack. When John died on July 11, 1937, Mack bought enough shares from the Shibe estate to become majority owner.[7]  However, Mack had been the franchise's number-one man since Ben Shibe's death.  Even as bad as the A's got during this time, Mack remained a one-man band with complete control over both baseball and business matters. Long after most teams hired a general manager, Mack continued making all personnel decisions and leading the team on the field.  One of the few times that he even considered ceding some of his duties came in the 1934-35 offseason, when he entertained hiring Babe Ruth to succeed him as manager.  However, he backed off from this idea, saying that the Babe's wife, Claire, would be running the team within a month.[8]  Even when the Phillies moved to Shibe Park as tenants of the A's midway through the 1938 season, not enough revenue came in for Mack to build another winner.
By the mid-1940s, as Mack passed his 80th birthday, his mental state was becoming increasingly questionable. He would frequently sleep through innings, make bad calls that his coaches would simply ignore, have inexplicable fits of anger, or call players from decades earlier to pinch-hit. Mack also never installed a telephone in the dugout and instead would use a series of obtuse hand signs to signal his coaches on the field. According to infielder Ferris Fain, "He'd fall asleep for much of the game waving his score card, but he still had a few working nerve endings left in his big ol' neck waddle. Anyone who dared wake him up was subjected to a hasty trial by the team's kangaroo court." Nonetheless, despite calls inside and outside the organization to step down, Mack would not even consider firing himself.  Also during this time, Mack gave a minority stake in the team to his sons, Roy, Earle and Connie Jr.  Although Connie Jr. was nearly 20 years younger than Roy and Earle (he was the son of Connie Sr.'s second marriage), Mack intended to have all three of them inherit the team upon his death.  He also intended for Earle, who had been assistant manager since 1924, to succeed him as manager. This decision would have dire consequences for the A's later on.[7]
During this time, Shibe Park was also becoming an increasing liability. While the facility had been state of the art when it opened in 1909, by the late 1940s, it had not been well maintained in some time. It was also not suited to automobile traffic, having been designed before the Model T Ford was introduced.
To the surprise of most people in baseball, Mack managed not only to get out of the cellar in 1947, but actually finished with a winning record for the first time in 14 years. They contended for much of 1948, even managing to spend 49 days in first place.  However, the turning point came on June 13, when pitcher Nels Potter, who had been a solid middle reliever for most of the season, blew a three-run lead in the first game of a doubleheader against the St. Louis Browns.  An enraged Mack ordered him off the team in front of a shocked clubhouse after the game.[9] The A's spent most of the summer in either first or second place. Mack had previously released pitcher Bill Dietrich and his dismissal of Potter left the second place A's with only five healthy pitchers at that point.[10] By the end of the year the team faded to fourth place.  The franchise would not be a factor in a pennant race again at that late date until 1969their second year in Oakland.
Another winning record in 1949 sparked hopes that 1950the 50th season for both the American League and Mack's tenure as manager of the A's--would bring a pennant at last.  During that year, the team wore uniforms trimmed in blue and gold, in honor of the Golden Jubilee of "The Grand Old Man of Baseball."  However, the 1950 season was an unmitigated disaster.  They were only above .500 once all season (at 3-2), and a 517 May ended any hope of contention.  Before May was out, Mack's sons had agreed to ease their father out as manager. On May 26, it was announced that Mack would resign at the end of the season. On the same day, former A's star Jimmy Dykes, who had returned to the A's as a coach a year earlier, was named assistant manager and manager-in-waiting; he would succeed Mack as manager for the 1951 season. However, for all practical purposes, Dykes took over as manager immediately; he was given control over the A's day-to-day operations and became the team's main game-day operator. Cochrane, who had been brought back as a coach earlier in the year, was named general manager, stripping Connie Sr. of his last direct authority over baseball matters.[7]  Ultimately, the A's finished with the worst record in the majors at 52-102, 46 games out of first.  Mack's 50-year tenure as manager is a North American professional sports record that has never been threatened.
In the late 1940s, a power struggle developed between Roy and Earle on one side and Connie Jr. on the other. Connie Jr., like many A's fans, had become disenchanted with his brothers' bargain-basement approach to running the team.  However, Roy and Earle were not willing to modernize and refused to listen to their younger brother, whom they considered a mere child with no relevant opinion (Connie Jr. was almost 20 years younger than Roy and Earle). Compounding their disagreements was the fact that they had different mothers. When it was apparent that Roy and Earle would not consider making what he considered to be critical reforms, Connie Jr. and his mother (who was angered at Connie Sr.'s refusal to give Connie Jr.'s sisters any role in the team) made an alliance with the Shibe heirs. Connie Jr. began taking steps to upgrade the team and the park.  One of the few things on which the two sides agreed was that it was time for Connie Sr. to step down as manager.[7]
Matters came to a head in July 1950, when Connie Jr. and the Shibes decided to sell the team. However, Roy and Earle insisted that they have a 30-day option to buy out Connie Jr. and the Shibes before the team was put on the market. Connie Jr. didn't think Roy and Earle could get the $1.74 million required to buy him out, but Roy and Earle called their bluff by mortgaging the team to Connecticut General Life Insurance Company (now part of CIGNA) and pledging Shibe Park as collateral. The mortgage deal closed on August 26.  The shares of Connie Jr. and the Shibes were retired, ending the Shibes' half-century involvement with the A's and making Connie Sr., Roy and Earle the team's only shareholders. Although his father remained nominal owner and team president, Roy, who had been vice president since 1936, now became operating head of the franchise, sharing day-to-day control with Earle. However, under the terms of the mortgage, the A's were now saddled with payments of $200,000 over the first five years, depriving them of badly needed capital that could have been used on improving the team and the park.[7]  Unfortunately for the A's, the team continued to slide on the field. Although the 1949 team set a major league record for double plays which still stands, this was more a reflection of the team's poor pitching staff allowing too many base runners.[11] They would have only one winning record from 1951 to 1954a fourth-place finish in 1952.  The nadir came in 1954, when the A's finished with a ghastly 51-103 record, easily the worst record in baseball and 60 games out of first.  Attendance plummeted and revenues continued to dwindle.
At the same time, the Phillies, who had been the definition of baseball futility for over 30 years, began a surprisingly quick climb to respectability. The A's had always been the more popular team in Philadelphia for most of the first half of the century, even though for much of the last decade they had been as bad or worse than the Phillies. However, unlike the A's, the Phillies began spending lavishly on young prospects in the 1940s. The impact was immediate.  In 1947, the A's finished fourth in the American League while the Phillies tied for the worst record in the National League.  Only three years later, while the A's finished dead last in the majors, the Phillies went all the way to the 1950 World Series.  It soon became obvious that the Phillies had passed the A's as Philadelphia's number-one team.
In response, Roy and Earle began cutting costs even further. They turned over the rent from the Phillies to Connecticut General and took cash advances from their concessions contractor.  The cost-cutting ramped up even further in the 195354 offseason, when they slashed over $100,000 off the player payroll, fired general manager Arthur Ehlers and replaced Dykes as manager with shortstop Eddie Joost. They also pared down the minor-league system to only six clubs. However, even with these measures, there wasn't nearly enough money coming in to service the mortgage debt, and Roy and Earle began feuding with each other.[7]
Despite the turmoil, some Athletics players shone on the field. In 1951, Gus Zernial led the American League with 33 home runs, 129 runs scored, 68 extra-base hits, and 17 outfield assists; in 1952 he swatted 29 homers and bagged 100 RBI; in 1953 he hit 42 homers and drove in 108 runs. In 1952, left-handed pitcher Bobby Shantz won 24 games and was named the league's Most Valuable Player, and Ferris Fain won AL batting championships in 1951 (with a .344 average) and 1952 (with a .320 average). His 1952 batting crown remains the last time an Athletic has led the league in hitting. Joost was a solid fielder who had a good eye at the plate for generating walks and had an above-average on-base percentage as a result. All four players represented the American League in the All-Star Game. Shantz might have won 30 games his best year 1952 but was hurt by a pitched ball on the wrist and was finished for the season.
By the summer of 1954, it was obvious that the A's were on an irreversible slide into bankruptcy. Earle and Roy decided that there was no choice but to sell their father's beloved team, and it was with great sorrow that the old man gave his approval for the sale. Although several offers were put forward by Philadelphia interests, American League president Will Harridge was convinced that the team could never be viable in Philadelphia. The sparse crowds at Shibe had been a source of frustration for some time to the other AL owners, as they could not even begin to meet their expenses for trips to Philadelphia. As a result, Harridge had come to believe that the only way to resolve the "Philadelphia problem" was to move the Athletics elsewhere. For this reason, when Chicago businessman Arnold Johnson offered to buy the team, the other owners pressured Roy Mack to agree to the sale.  Johnson had very close ties to the Yankees; he not only owned Yankee Stadium but also owned Blues Stadium in Kansas City, home to the Yankees' top farm team. Johnson intended to move the A's to a renovated Blues Stadium if he was cleared to buy them. The Yankees made no secret that they favored Johnson, and their backing gave him the upper hand with the other owners. After an October 12 owners meeting at which several offers from Philadelphia interests were rejected as inadequate (Harridge later said that while several of them "talked about millions", they didn't have any money behind them), Mack agreed in principle to sell the A's to Johnson no later than October 18.[7]
However, on October 17, Roy Mack suddenly announced that the A's had been sold to a Philadelphia-based group headed by auto dealer John Crisconi. The deal was to be approved at an American League owners' meeting on October 28. It looked headed for approval when rumors (reportedly planted by the Yankees) cropped up that the Crisconi group was underfinanced, and Johnson collared Roy Mack at Roy's home to persuade him that his original deal was better in the long run. On October 28, the sale to the Crisconi group came up one vote short of the five needed for approval, with Roy Mack voting against the deal he'd just negotiated. A day later, Connie Mack released an open letter to A's fans (one that was likely written by his wife) blasting the owners for sinking the deal to the Crisconi group. However, he conceded that he didn't have enough money to run the A's in 1955, and the Johnson deal was the only one that had any prospect of winning approval. A few days later, the Macks sold the A's to Johnson for $3.5 million--$1.5 million for their shares plus $2 million in debt. Selling Shibe Parkwhich had been renamed Connie Mack Stadium a year earlierproved more difficult, but the Phillies reluctantly bought it. The American League owners met again on November 8, and duly approved Johnson's bid to buy the A's.  Johnson's first act was to request permission to move to Kansas City. This proved more difficult, since it required a three-fourths majority. However, Detroit owner Spike Briggs was persuaded to change his vote, ending the A's 54-year stay in Philadelphia.[7]
The Athletics played the Phillies for the first time in interleague play in June 2003 at Veterans Stadium. The Phillies invited former A's Eddie Joost and Gus Zernial to the games. Connie Mack's daughter Ruth Mack Clark attended the first game. Former Florida U.S. Senator Connie Mack III, Mack's grandson, threw out the first ball.[12]
In turn, the Phillies played the Athletics in Oakland in June 2005. The A's invited Eddie Joost to throw out the first pitch before the series opening game on June 17, 2005.[13]  In 2011 the Athletics visited the Phillies at Citizens Bank Park for an interleague series in which the Phillies took two out of three games.
There remains a level of nostalgia for the Athletics in the Philadelphia region.  A Philadelphia Athletics Historical Society exists with an active website,[14] and a local company called Shibe Vintage Sports sells retro Philadelphia Athletics gear.[15]

